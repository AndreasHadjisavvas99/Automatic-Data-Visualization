{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEl2kKnCYdbW"
      },
      "source": [
        "### Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH1jU8rpbb7H"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input, Concatenate, Softmax, Dot, Reshape, Dropout\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6Qi9fQlbvRB",
        "outputId": "69ea4e83-9ea9-4023-95f7-74d8b3e737ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnds2rGRMenj"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "4d52Erw8Sdk-",
        "outputId": "96ddf458-ffce-4c8a-b897-6f20d6cee41d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preprocessing' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0f4e227d4c0b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_lsb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 0.25 * 0.8 = 0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_train_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocessing' is not defined"
          ]
        }
      ],
      "source": [
        "X,Y = preprocessing(df_lsb)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, Y, test_size=0.2, random_state=100, stratify=Y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "y_train_onehot, y_test_onehot, y_val_onehot, output_shape = one_hot(y_train,y_test,y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8LuH4HZbcbb",
        "outputId": "736aacd4-e460-416f-bd17-b73f3d74b167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-8a849ce0bb88>:3: DtypeWarning: Columns (83) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_bp = pd.read_csv('/content/drive/MyDrive/thesis/automl/features1/c2_bp.csv')\n",
            "<ipython-input-3-8a849ce0bb88>:4: DtypeWarning: Columns (83) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_lb = pd.read_csv('/content/drive/MyDrive/thesis/automl/features1/c2_lb.csv')\n",
            "<ipython-input-3-8a849ce0bb88>:5: DtypeWarning: Columns (83) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_lp = pd.read_csv('/content/drive/MyDrive/thesis/automl/features1/c2_lp.csv')\n",
            "<ipython-input-3-8a849ce0bb88>:6: DtypeWarning: Columns (83) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_ls = pd.read_csv('/content/drive/MyDrive/thesis/automl/features1/c2_ls.csv')\n",
            "<ipython-input-3-8a849ce0bb88>:8: DtypeWarning: Columns (83) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_sp = pd.read_csv('/content/drive/MyDrive/thesis/automl/features1/c2_sp.csv')\n"
          ]
        }
      ],
      "source": [
        "# import data\n",
        "# 2 classes\n",
        "df_bp = pd.read_csv('/content/drive/MyDrive/thesis/automl/features1/c2_bp.csv')\n",
        "df_lb = pd.read_csv('/content/drive/MyDrive/thesis/automl/features1/c2_lb.csv')\n",
        "df_lp = pd.read_csv('/content/drive/MyDrive/thesis/automl/features1/c2_lp.csv')\n",
        "df_ls = pd.read_csv('/content/drive/MyDrive/thesis/automl/features1/c2_ls.csv')\n",
        "df_sb = pd.read_csv('/content/drive/MyDrive/thesis/automl/features1/c2_sb.csv')\n",
        "df_sp = pd.read_csv('/content/drive/MyDrive/thesis/automl/features1/c2_sp.csv')\n",
        "# 3 classes\n",
        "df_lbp = pd.read_csv('/content/drive/MyDrive/thesis/automl/features1/c3_lbp.csv')\n",
        "df_lsb = pd.read_csv('/content/drive/MyDrive/thesis/automl/features1/c3_lsb.csv')\n",
        "df_lsp = pd.read_csv('/content/drive/MyDrive/thesis/automl/features1/c3_lsp.csv')\n",
        "df_sbp = pd.read_csv('/content/drive/MyDrive/thesis/automl/features1/c3_sbp.csv')\n",
        "# 4 classes\n",
        "df_lsbp = pd.read_csv('/content/drive/MyDrive/thesis/automl/features1/c4_lsbp.csv')\n",
        "\n",
        "df_c2 = pd.read_csv('/content/drive/MyDrive/thesis/automl/features1/df_c2.csv') #bar and pie\n",
        "\n",
        "\n",
        "validation_accuracies = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqI41w8EfcJB"
      },
      "source": [
        "## Data preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lw8fuZUcfbJ_"
      },
      "outputs": [],
      "source": [
        "def preprocessing(df):\n",
        "  y = df['Plot_Type']  # Labels\n",
        "  df = df.rename(columns={'Plot_Type':'Label'})\n",
        "\n",
        "  X_categorical = pd.DataFrame()\n",
        "  X_numerical = pd.DataFrame()\n",
        "  X_boolean = pd.DataFrame()\n",
        "  Y = df[\"Label\"].values\n",
        "  df = df.drop(labels = ['Label'], axis=1)\n",
        "\n",
        "  # splitting data to categorical and numerical for later purposes\n",
        "  for column in df.columns:\n",
        "      if df[column].dtype == 'object':\n",
        "          X_categorical[column] = df[column]\n",
        "      elif df[column].dtype == 'bool':\n",
        "          X_boolean[column] = df[column].astype(int)\n",
        "      else:\n",
        "          X_numerical[column] = df[column]\n",
        "\n",
        "  # 1) Set numerical features above the 99th percentile or below the 1st percentile to those respective cut-offs\n",
        "  percentile_1 = X_numerical.quantile(0.01)\n",
        "  percentile_99 = X_numerical.quantile(0.99)\n",
        "  for column in X_numerical.columns:\n",
        "      X_numerical[column] = np.where(X_numerical[column] > percentile_99[column], percentile_99[column], X_numerical[column])\n",
        "      X_numerical[column] = np.where(X_numerical[column] < percentile_1[column], percentile_1[column], X_numerical[column])\n",
        "\n",
        "  # 2) Remove the mean of numeric fields and scale to unit variance\n",
        "  scaler = StandardScaler()\n",
        "  numeric_data = X_numerical.select_dtypes(include=['float64', 'int64'])\n",
        "  scaler.fit(numeric_data)\n",
        "  scaled_numeric_data = scaler.transform(numeric_data)\n",
        "  X_numerical[numeric_data.columns] = scaled_numeric_data\n",
        "\n",
        "  #################### FLAGGING REPLACE WITH INF ########################\n",
        "  # 3a) Impute missing categorical values using the mode of non missing values\n",
        "  for column in X_categorical.columns:\n",
        "      #if X_categorical[column].dtype == 'object':\n",
        "      mode_val = X_categorical[column].mode()[0]\n",
        "      X_categorical[column].fillna(mode_val, inplace=True)\n",
        "\n",
        "  # 3b) Impute missing numerical values with mean of non-missing values\n",
        "  for column in X_numerical.columns:\n",
        "      #if pd.api.types.is_numeric_dtype(X_numerical[column].dtype):\n",
        "      mean_val = X_numerical[column].mean()\n",
        "      median_val = X_numerical[column].median()\n",
        "      item = -9999999999\n",
        "      #X_numerical[column].fillna(np.inf, inplace=True)\n",
        "      X_numerical[column].fillna(mean_val, inplace=True)\n",
        "\n",
        "\n",
        "  # 4) One-hot encoding to categorical features\n",
        "  one_hot_encoder = OneHotEncoder()\n",
        "  X_categorical_encoded = one_hot_encoder.fit_transform(X_categorical).toarray()\n",
        "  encoded_feature_names = one_hot_encoder.get_feature_names_out(X_categorical.columns)\n",
        "  X_categorical_encoded_df = pd.DataFrame(X_categorical_encoded, columns=encoded_feature_names)\n",
        "  X = pd.concat([X_numerical, X_categorical_encoded_df, X_boolean], axis=1)\n",
        "  return X,Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elahrYyTf0MJ"
      },
      "source": [
        "### Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rr4I1YOSGZuR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "db5f65fb-0ee5-43f7-f2d7-09e4a96805e7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_lsbp' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5718ad4faaa3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_lsbp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_lsbp' is not defined"
          ]
        }
      ],
      "source": [
        "X,Y = preprocessing(df_lsbp)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=100, stratify=Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv5pzhM6nzMy",
        "outputId": "5d18aad4-92a2-4f98-b3b6-5df654c36eb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Num_Entries_X', 'Num_Entries_Y', 'Normalized Range (X)', 'Normalized Range (Y)', 'Num_Unique_X', 'Num_Unique_Y', 'Num_None_X', 'Percentage_None_X', 'Num_None_Y', 'Percentage_None_Y', 'Normalized Mean (X)', 'Normalized Median (X)', 'Sample Variance (X)', 'Sample Standard Deviation (X)', 'Coefficient of Variation (X)', 'Minimum Value (X)', 'Maximum Value (X)', 'Normalized Mean (Y)', 'Normalized Median (Y)', 'Sample Variance (Y)', 'Sample Standard Deviation (Y)', 'Coefficient of Variation (Y)', 'Minimum Value (Y)', 'Maximum Value (Y)', 'Mean C (X)', 'Median C (X)', 'Min C (X)', 'Max C (X)', 'Std C (X)', 'Unique C (X)', 'Mean C (Y)', 'Median C (Y)', 'Min C (Y)', 'Max C (Y)', 'Std C (Y)', 'Unique C (Y)', 'Num Identical Elements', 'Percent Shared Elements', 'Identical', 'Num Shared Unique Elements', 'Percent Shared Unique Elements', 'Correlation Value', 'Correlation P', 'Ks Statistic', 'Ks P', 'Q Entropy (X)', 'Q Entropy (Y)', 'C Entropy (X)', 'C Entropy (Y)', 'Perc Mode (X)', 'Perc Mode (Y)', 'Kurtosis (X)', 'Skewness (X)', 'Kurtosis (Y)', 'Skewness (Y)', '% Outliers (1.5 IQR) (X)', '% Outliers (3 IQR) (X)', '% Outliers (1-99 Percentile) (X)', '% Outliers (3 Std Dev) (X)', '% Outliers (1.5 IQR) (Y)', '% Outliers (3 IQR) (Y)', '% Outliers (1-99 Percentile) (Y)', '% Outliers (3 Std Dev) (Y)', 'Sortedness (X)', 'Sortedness (Y)', 'X_Type_c', 'X_Type_d', 'X_Type_q', 'Y_Type_c', 'Y_Type_d', 'Y_Type_q', 'Has Outliers (1.5 IQR) (X)_False', 'Has Outliers (1.5 IQR) (X)_True', 'Has Outliers (3 IQR) (X)_False', 'Has Outliers (3 IQR) (X)_True', 'Has Outliers (1-99 Percentile) (X)_True', 'Has Outliers (3 Std Dev) (X)_False', 'Has Outliers (3 Std Dev) (X)_True', 'Has Outliers (1.5 IQR) (Y)_False', 'Has Outliers (1.5 IQR) (Y)_True', 'Has Outliers (3 IQR) (Y)_False', 'Has Outliers (3 IQR) (Y)_True', 'Has Outliers (1-99 Percentile) (Y)_False', 'Has Outliers (1-99 Percentile) (Y)_True', 'Has Outliers (3 Std Dev) (Y)_False', 'Has Outliers (3 Std Dev) (Y)_True', 'Is Sorted (X)_False', 'Is Sorted (X)_True', 'Is Sorted (Y)_False', 'Is Sorted (Y)_True', 'Has_None_X', 'Has_None_Y', 'Has Shared Elements', 'Has Shared Unique Elements', 'Identical Unique', 'Correlation Significant 005', 'Ks Significant 005']\n"
          ]
        }
      ],
      "source": [
        "print(X.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs3zcjX45lec"
      },
      "source": [
        "### Label One-Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IedJjgLY-gPE"
      },
      "outputs": [],
      "source": [
        "# One-hot encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "def one_hot(y_train,y_test,y_val):\n",
        "  train_integer_encoded = label_encoder.fit_transform(y_train)\n",
        "  test_integer_encoded = label_encoder.fit_transform(y_test)\n",
        "  val_integer_encoded = label_encoder.fit_transform(y_val)\n",
        "  y_train_onehot = to_categorical(train_integer_encoded)\n",
        "  y_test_onehot = to_categorical(test_integer_encoded)\n",
        "  y_val_onehot = to_categorical(val_integer_encoded)\n",
        "\n",
        "  output_shape = y_test_onehot.shape[1]\n",
        "\n",
        "  label_mapping = {index: label for index, label in enumerate(label_encoder.classes_)}\n",
        "  print(label_mapping)\n",
        "  return y_train_onehot,y_test_onehot,y_val_onehot, output_shape\n",
        "\n",
        "#y_train_onehot,y_test_onehot,_,output_shape = one_hot(y_train,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcbvCoTDgBDf"
      },
      "source": [
        "## Build and Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnmnXwGtawlQ"
      },
      "source": [
        "#### Callbacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt4OyWUZcVWn"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=60, min_delta=1e-3, verbose=1, restore_best_weights = True)\n",
        "\n",
        "lr_schedule = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=15, min_delta=10e-3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'best_model.keras',  # File path to save the model\n",
        "    monitor='val_accuracy',  # Metric to monitor\n",
        "    save_best_only=True,  # Save only the best model\n",
        "    mode='max',  # We are looking for the maximum validation accuracy\n",
        "    verbose=0  # Print messages when saving model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_tY_vs3noZi"
      },
      "source": [
        "### C=2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhBHn-f5rDAi"
      },
      "source": [
        "#### BAR - PIE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWGDN2cXUBoW",
        "outputId": "6d1099ac-c75d-4830-829f-3625a0bd78dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'bar', 1: 'pie'}\n"
          ]
        }
      ],
      "source": [
        "X,Y = preprocessing(df_bp)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, Y, test_size=0.2, random_state=100, stratify=Y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "y_train_onehot, y_test_onehot, y_val_onehot, output_shape = one_hot(y_train,y_test,y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW4h68qDrFuV",
        "outputId": "00ff43f7-78d1-461f-b3b0-6de5cfb24fe8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5896 - loss: 0.6671 - val_accuracy: 0.7184 - val_loss: 0.6015 - learning_rate: 5.0000e-05\n",
            "Epoch 2/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7317 - loss: 0.5766 - val_accuracy: 0.7578 - val_loss: 0.5499 - learning_rate: 5.0000e-05\n",
            "Epoch 3/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7673 - loss: 0.5310 - val_accuracy: 0.7724 - val_loss: 0.5210 - learning_rate: 5.0000e-05\n",
            "Epoch 4/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7808 - loss: 0.5089 - val_accuracy: 0.7772 - val_loss: 0.5051 - learning_rate: 5.0000e-05\n",
            "Epoch 5/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7915 - loss: 0.4836 - val_accuracy: 0.7816 - val_loss: 0.4956 - learning_rate: 5.0000e-05\n",
            "Epoch 6/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7783 - loss: 0.4842 - val_accuracy: 0.7821 - val_loss: 0.4883 - learning_rate: 5.0000e-05\n",
            "Epoch 7/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7868 - loss: 0.4741 - val_accuracy: 0.7789 - val_loss: 0.4835 - learning_rate: 5.0000e-05\n",
            "Epoch 8/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7812 - loss: 0.4706 - val_accuracy: 0.7799 - val_loss: 0.4796 - learning_rate: 5.0000e-05\n",
            "Epoch 9/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7905 - loss: 0.4588 - val_accuracy: 0.7826 - val_loss: 0.4750 - learning_rate: 5.0000e-05\n",
            "Epoch 10/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7896 - loss: 0.4600 - val_accuracy: 0.7816 - val_loss: 0.4721 - learning_rate: 5.0000e-05\n",
            "Epoch 11/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7942 - loss: 0.4568 - val_accuracy: 0.7837 - val_loss: 0.4692 - learning_rate: 5.0000e-05\n",
            "Epoch 12/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.7864 - loss: 0.4547 - val_accuracy: 0.7859 - val_loss: 0.4673 - learning_rate: 5.0000e-05\n",
            "Epoch 13/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7912 - loss: 0.4570 - val_accuracy: 0.7853 - val_loss: 0.4649 - learning_rate: 5.0000e-05\n",
            "Epoch 14/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7952 - loss: 0.4475 - val_accuracy: 0.7848 - val_loss: 0.4631 - learning_rate: 5.0000e-05\n",
            "Epoch 15/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7952 - loss: 0.4453 - val_accuracy: 0.7848 - val_loss: 0.4608 - learning_rate: 5.0000e-05\n",
            "Epoch 16/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8024 - loss: 0.4439 - val_accuracy: 0.7875 - val_loss: 0.4593 - learning_rate: 5.0000e-05\n",
            "Epoch 17/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7990 - loss: 0.4447 - val_accuracy: 0.7896 - val_loss: 0.4581 - learning_rate: 5.0000e-05\n",
            "Epoch 18/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8098 - loss: 0.4305 - val_accuracy: 0.7891 - val_loss: 0.4560 - learning_rate: 5.0000e-05\n",
            "Epoch 19/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8099 - loss: 0.4321 - val_accuracy: 0.7907 - val_loss: 0.4546 - learning_rate: 5.0000e-05\n",
            "Epoch 20/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8020 - loss: 0.4367 - val_accuracy: 0.7934 - val_loss: 0.4529 - learning_rate: 5.0000e-05\n",
            "Epoch 21/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8084 - loss: 0.4217 - val_accuracy: 0.7934 - val_loss: 0.4512 - learning_rate: 5.0000e-05\n",
            "Epoch 22/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8135 - loss: 0.4226 - val_accuracy: 0.7988 - val_loss: 0.4497 - learning_rate: 5.0000e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8179 - loss: 0.4172 - val_accuracy: 0.7967 - val_loss: 0.4495 - learning_rate: 5.0000e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8137 - loss: 0.4205 - val_accuracy: 0.7999 - val_loss: 0.4480 - learning_rate: 5.0000e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8062 - loss: 0.4231 - val_accuracy: 0.7999 - val_loss: 0.4468 - learning_rate: 5.0000e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8150 - loss: 0.4200 - val_accuracy: 0.7999 - val_loss: 0.4459 - learning_rate: 5.0000e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8219 - loss: 0.4072 - val_accuracy: 0.8026 - val_loss: 0.4455 - learning_rate: 5.0000e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8094 - loss: 0.4296 - val_accuracy: 0.8026 - val_loss: 0.4435 - learning_rate: 5.0000e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8100 - loss: 0.4151 - val_accuracy: 0.8058 - val_loss: 0.4434 - learning_rate: 5.0000e-05\n",
            "Epoch 30/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8219 - loss: 0.4065 - val_accuracy: 0.8058 - val_loss: 0.4415 - learning_rate: 5.0000e-05\n",
            "Epoch 31/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8196 - loss: 0.4149 - val_accuracy: 0.8031 - val_loss: 0.4415 - learning_rate: 5.0000e-05\n",
            "Epoch 32/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8253 - loss: 0.4005 - val_accuracy: 0.8042 - val_loss: 0.4408 - learning_rate: 5.0000e-05\n",
            "Epoch 33/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8220 - loss: 0.4099 - val_accuracy: 0.8064 - val_loss: 0.4400 - learning_rate: 5.0000e-05\n",
            "Epoch 34/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8170 - loss: 0.4128 - val_accuracy: 0.8047 - val_loss: 0.4403 - learning_rate: 5.0000e-05\n",
            "Epoch 35/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8230 - loss: 0.4068 - val_accuracy: 0.8037 - val_loss: 0.4394 - learning_rate: 5.0000e-05\n",
            "Epoch 36/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8221 - loss: 0.4038 - val_accuracy: 0.8042 - val_loss: 0.4383 - learning_rate: 5.0000e-05\n",
            "Epoch 37/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8304 - loss: 0.3978 - val_accuracy: 0.8042 - val_loss: 0.4378 - learning_rate: 5.0000e-05\n",
            "Epoch 38/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8251 - loss: 0.3937 - val_accuracy: 0.8042 - val_loss: 0.4370 - learning_rate: 5.0000e-05\n",
            "Epoch 39/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8285 - loss: 0.3917 - val_accuracy: 0.8069 - val_loss: 0.4375 - learning_rate: 5.0000e-05\n",
            "Epoch 40/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8157 - loss: 0.4127 - val_accuracy: 0.8069 - val_loss: 0.4363 - learning_rate: 5.0000e-05\n",
            "Epoch 41/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.4009 - val_accuracy: 0.8074 - val_loss: 0.4362 - learning_rate: 5.0000e-05\n",
            "Epoch 42/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8254 - loss: 0.3942 - val_accuracy: 0.8069 - val_loss: 0.4354 - learning_rate: 5.0000e-05\n",
            "Epoch 43/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8265 - loss: 0.3935 - val_accuracy: 0.8069 - val_loss: 0.4352 - learning_rate: 5.0000e-05\n",
            "Epoch 44/50\n",
            "\u001b[1m158/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8311 - loss: 0.3858\n",
            "Epoch 44: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8309 - loss: 0.3865 - val_accuracy: 0.8069 - val_loss: 0.4352 - learning_rate: 5.0000e-05\n",
            "Epoch 45/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8277 - loss: 0.3915 - val_accuracy: 0.8069 - val_loss: 0.4350 - learning_rate: 5.0000e-06\n",
            "Epoch 46/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8324 - loss: 0.3884 - val_accuracy: 0.8069 - val_loss: 0.4350 - learning_rate: 5.0000e-06\n",
            "Epoch 47/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8311 - loss: 0.3860 - val_accuracy: 0.8069 - val_loss: 0.4348 - learning_rate: 5.0000e-06\n",
            "Epoch 48/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8282 - loss: 0.3908 - val_accuracy: 0.8064 - val_loss: 0.4347 - learning_rate: 5.0000e-06\n",
            "Epoch 49/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8264 - loss: 0.3934 - val_accuracy: 0.8074 - val_loss: 0.4347 - learning_rate: 5.0000e-06\n",
            "Epoch 50/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8357 - loss: 0.3820 - val_accuracy: 0.8074 - val_loss: 0.4347 - learning_rate: 5.0000e-06\n"
          ]
        }
      ],
      "source": [
        "input_shape = X_train.shape[1:]\n",
        "init_neurons = X_train.shape[1]\n",
        "custom_optimizer = Adam(learning_rate=0.00005)\n",
        "\n",
        "# Define the model\n",
        "model_c2_bp = tf.keras.models.Sequential([\n",
        "    Dense(init_neurons, activation='relu', input_shape=input_shape),\n",
        "    Dense(45, activation='relu'),\n",
        "    Dense(45, activation='relu'),\n",
        "    Dense(output_shape, activation='softmax')\n",
        "])\n",
        "\n",
        "model_c2_bp.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_c2_bp = model_c2_bp.fit(X_train, y_train_onehot, epochs=50, validation_data=(X_val, y_val_onehot), callbacks=[lr_schedule, checkpoint])\n",
        "best_val_accuracy_c2_bp = max(history_c2_bp.history['val_accuracy'])\n",
        "validation_accuracies['model_c2_bp'] = best_val_accuracy_c2_bp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BudBHXknrNYi",
        "outputId": "8fea6052-1094-4cd1-cf9c-38a957e66072"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5507 - loss: 0.7635 - val_accuracy: 0.7044 - val_loss: 0.6268 - learning_rate: 5.0000e-05\n",
            "Epoch 2/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7248 - loss: 0.6068 - val_accuracy: 0.7562 - val_loss: 0.5552 - learning_rate: 5.0000e-05\n",
            "Epoch 3/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7498 - loss: 0.5434 - val_accuracy: 0.7691 - val_loss: 0.5213 - learning_rate: 5.0000e-05\n",
            "Epoch 4/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7735 - loss: 0.4991 - val_accuracy: 0.7783 - val_loss: 0.5048 - learning_rate: 5.0000e-05\n",
            "Epoch 5/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7785 - loss: 0.4903 - val_accuracy: 0.7783 - val_loss: 0.4957 - learning_rate: 5.0000e-05\n",
            "Epoch 6/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7756 - loss: 0.4798 - val_accuracy: 0.7837 - val_loss: 0.4887 - learning_rate: 5.0000e-05\n",
            "Epoch 7/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7771 - loss: 0.4786 - val_accuracy: 0.7821 - val_loss: 0.4831 - learning_rate: 5.0000e-05\n",
            "Epoch 8/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7867 - loss: 0.4634 - val_accuracy: 0.7891 - val_loss: 0.4793 - learning_rate: 5.0000e-05\n",
            "Epoch 9/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7828 - loss: 0.4713 - val_accuracy: 0.7886 - val_loss: 0.4754 - learning_rate: 5.0000e-05\n",
            "Epoch 10/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7900 - loss: 0.4584 - val_accuracy: 0.7848 - val_loss: 0.4723 - learning_rate: 5.0000e-05\n",
            "Epoch 11/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7913 - loss: 0.4562 - val_accuracy: 0.7907 - val_loss: 0.4699 - learning_rate: 5.0000e-05\n",
            "Epoch 12/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7928 - loss: 0.4502 - val_accuracy: 0.7859 - val_loss: 0.4665 - learning_rate: 5.0000e-05\n",
            "Epoch 13/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8023 - loss: 0.4391 - val_accuracy: 0.7907 - val_loss: 0.4643 - learning_rate: 5.0000e-05\n",
            "Epoch 14/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7909 - loss: 0.4486 - val_accuracy: 0.7907 - val_loss: 0.4617 - learning_rate: 5.0000e-05\n",
            "Epoch 15/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8120 - loss: 0.4263 - val_accuracy: 0.7956 - val_loss: 0.4610 - learning_rate: 5.0000e-05\n",
            "Epoch 16/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7968 - loss: 0.4408 - val_accuracy: 0.7934 - val_loss: 0.4581 - learning_rate: 5.0000e-05\n",
            "Epoch 17/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8032 - loss: 0.4291 - val_accuracy: 0.7967 - val_loss: 0.4564 - learning_rate: 5.0000e-05\n",
            "Epoch 18/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8058 - loss: 0.4336 - val_accuracy: 0.7961 - val_loss: 0.4548 - learning_rate: 5.0000e-05\n",
            "Epoch 19/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8127 - loss: 0.4259 - val_accuracy: 0.7961 - val_loss: 0.4532 - learning_rate: 5.0000e-05\n",
            "Epoch 20/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8086 - loss: 0.4275 - val_accuracy: 0.7956 - val_loss: 0.4519 - learning_rate: 5.0000e-05\n",
            "Epoch 21/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8099 - loss: 0.4275 - val_accuracy: 0.7929 - val_loss: 0.4516 - learning_rate: 5.0000e-05\n",
            "Epoch 22/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8177 - loss: 0.4108 - val_accuracy: 0.7940 - val_loss: 0.4501 - learning_rate: 5.0000e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8040 - loss: 0.4254 - val_accuracy: 0.7934 - val_loss: 0.4486 - learning_rate: 5.0000e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8095 - loss: 0.4192 - val_accuracy: 0.7929 - val_loss: 0.4486 - learning_rate: 5.0000e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8131 - loss: 0.4136 - val_accuracy: 0.7929 - val_loss: 0.4472 - learning_rate: 5.0000e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8041 - loss: 0.4248 - val_accuracy: 0.7956 - val_loss: 0.4462 - learning_rate: 5.0000e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8111 - loss: 0.4206 - val_accuracy: 0.7961 - val_loss: 0.4455 - learning_rate: 5.0000e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8126 - loss: 0.4163 - val_accuracy: 0.7972 - val_loss: 0.4435 - learning_rate: 5.0000e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8211 - loss: 0.4039 - val_accuracy: 0.7972 - val_loss: 0.4433 - learning_rate: 5.0000e-05\n",
            "Epoch 30/50\n",
            "\u001b[1m171/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8245 - loss: 0.4015\n",
            "Epoch 30: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8243 - loss: 0.4018 - val_accuracy: 0.7945 - val_loss: 0.4428 - learning_rate: 5.0000e-05\n",
            "Epoch 31/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8181 - loss: 0.4082 - val_accuracy: 0.7961 - val_loss: 0.4427 - learning_rate: 5.0000e-06\n",
            "Epoch 32/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8058 - loss: 0.4209 - val_accuracy: 0.7967 - val_loss: 0.4426 - learning_rate: 5.0000e-06\n",
            "Epoch 33/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8137 - loss: 0.4122 - val_accuracy: 0.7977 - val_loss: 0.4426 - learning_rate: 5.0000e-06\n",
            "Epoch 34/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8174 - loss: 0.4058 - val_accuracy: 0.7977 - val_loss: 0.4425 - learning_rate: 5.0000e-06\n",
            "Epoch 35/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8192 - loss: 0.4120 - val_accuracy: 0.7983 - val_loss: 0.4425 - learning_rate: 5.0000e-06\n",
            "Epoch 36/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8264 - loss: 0.3924 - val_accuracy: 0.7972 - val_loss: 0.4424 - learning_rate: 5.0000e-06\n",
            "Epoch 37/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8359 - loss: 0.3944 - val_accuracy: 0.7977 - val_loss: 0.4423 - learning_rate: 5.0000e-06\n",
            "Epoch 38/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8194 - loss: 0.4137 - val_accuracy: 0.7972 - val_loss: 0.4423 - learning_rate: 5.0000e-06\n",
            "Epoch 39/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8161 - loss: 0.4137 - val_accuracy: 0.7967 - val_loss: 0.4423 - learning_rate: 5.0000e-06\n",
            "Epoch 40/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8253 - loss: 0.3999 - val_accuracy: 0.7972 - val_loss: 0.4421 - learning_rate: 5.0000e-06\n",
            "Epoch 41/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8151 - loss: 0.4164 - val_accuracy: 0.7967 - val_loss: 0.4421 - learning_rate: 5.0000e-06\n",
            "Epoch 42/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8247 - loss: 0.3993 - val_accuracy: 0.7972 - val_loss: 0.4420 - learning_rate: 5.0000e-06\n",
            "Epoch 43/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8255 - loss: 0.4022 - val_accuracy: 0.7972 - val_loss: 0.4420 - learning_rate: 5.0000e-06\n",
            "Epoch 44/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8263 - loss: 0.4013 - val_accuracy: 0.7972 - val_loss: 0.4419 - learning_rate: 5.0000e-06\n",
            "Epoch 45/50\n",
            "\u001b[1m161/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8227 - loss: 0.4020\n",
            "Epoch 45: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8226 - loss: 0.4023 - val_accuracy: 0.7972 - val_loss: 0.4419 - learning_rate: 5.0000e-06\n",
            "Epoch 46/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8200 - loss: 0.4082 - val_accuracy: 0.7972 - val_loss: 0.4419 - learning_rate: 1.0000e-06\n",
            "Epoch 47/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8200 - loss: 0.4088 - val_accuracy: 0.7972 - val_loss: 0.4418 - learning_rate: 1.0000e-06\n",
            "Epoch 48/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8123 - loss: 0.4151 - val_accuracy: 0.7972 - val_loss: 0.4418 - learning_rate: 1.0000e-06\n",
            "Epoch 49/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8140 - loss: 0.4204 - val_accuracy: 0.7972 - val_loss: 0.4418 - learning_rate: 1.0000e-06\n",
            "Epoch 50/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8167 - loss: 0.4140 - val_accuracy: 0.7972 - val_loss: 0.4418 - learning_rate: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "history_c2_bp = model_c2_bp.fit(X_train, y_train_onehot, epochs=50, validation_data=(X_val, y_val_onehot), callbacks=[lr_schedule, checkpoint])\n",
        "best_val_accuracy_c2_bp = max(history_c2_bp.history['val_accuracy'])\n",
        "validation_accuracies['model_c2_bp'] = best_val_accuracy_c2_bp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPNLfd-ONV4_"
      },
      "source": [
        "#### LINE - BAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXNz2Xe8TMAX",
        "outputId": "69a578df-ce37-4734-9863-55aeb37aef5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'bar', 1: 'line'}\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5965 - loss: 0.6520 - val_accuracy: 0.7778 - val_loss: 0.5531 - learning_rate: 5.0000e-05\n",
            "Epoch 2/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7931 - loss: 0.5238 - val_accuracy: 0.8242 - val_loss: 0.4500 - learning_rate: 5.0000e-05\n",
            "Epoch 3/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8143 - loss: 0.4431 - val_accuracy: 0.8360 - val_loss: 0.3966 - learning_rate: 5.0000e-05\n",
            "Epoch 4/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8394 - loss: 0.3821 - val_accuracy: 0.8430 - val_loss: 0.3684 - learning_rate: 5.0000e-05\n",
            "Epoch 5/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8482 - loss: 0.3568 - val_accuracy: 0.8533 - val_loss: 0.3518 - learning_rate: 5.0000e-05\n",
            "Epoch 6/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8529 - loss: 0.3492 - val_accuracy: 0.8549 - val_loss: 0.3423 - learning_rate: 5.0000e-05\n",
            "Epoch 7/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8611 - loss: 0.3401 - val_accuracy: 0.8619 - val_loss: 0.3359 - learning_rate: 5.0000e-05\n",
            "Epoch 8/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8517 - loss: 0.3344 - val_accuracy: 0.8614 - val_loss: 0.3308 - learning_rate: 5.0000e-05\n",
            "Epoch 9/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8634 - loss: 0.3175 - val_accuracy: 0.8668 - val_loss: 0.3277 - learning_rate: 5.0000e-05\n",
            "Epoch 10/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8594 - loss: 0.3243 - val_accuracy: 0.8608 - val_loss: 0.3239 - learning_rate: 5.0000e-05\n",
            "Epoch 11/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8648 - loss: 0.3161 - val_accuracy: 0.8630 - val_loss: 0.3214 - learning_rate: 5.0000e-05\n",
            "Epoch 12/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8610 - loss: 0.3273 - val_accuracy: 0.8668 - val_loss: 0.3193 - learning_rate: 5.0000e-05\n",
            "Epoch 13/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8659 - loss: 0.3135 - val_accuracy: 0.8641 - val_loss: 0.3174 - learning_rate: 5.0000e-05\n",
            "Epoch 14/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8707 - loss: 0.3092 - val_accuracy: 0.8641 - val_loss: 0.3157 - learning_rate: 5.0000e-05\n",
            "Epoch 15/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8681 - loss: 0.3105 - val_accuracy: 0.8646 - val_loss: 0.3141 - learning_rate: 5.0000e-05\n",
            "Epoch 16/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8728 - loss: 0.2989 - val_accuracy: 0.8684 - val_loss: 0.3131 - learning_rate: 5.0000e-05\n",
            "Epoch 17/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8673 - loss: 0.3040 - val_accuracy: 0.8673 - val_loss: 0.3119 - learning_rate: 5.0000e-05\n",
            "Epoch 18/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8718 - loss: 0.3052 - val_accuracy: 0.8673 - val_loss: 0.3107 - learning_rate: 5.0000e-05\n",
            "Epoch 19/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8719 - loss: 0.3033 - val_accuracy: 0.8668 - val_loss: 0.3100 - learning_rate: 5.0000e-05\n",
            "Epoch 20/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8773 - loss: 0.2912 - val_accuracy: 0.8657 - val_loss: 0.3092 - learning_rate: 5.0000e-05\n",
            "Epoch 21/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8789 - loss: 0.2897 - val_accuracy: 0.8657 - val_loss: 0.3088 - learning_rate: 5.0000e-05\n",
            "Epoch 22/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8752 - loss: 0.2967 - val_accuracy: 0.8673 - val_loss: 0.3078 - learning_rate: 5.0000e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8736 - loss: 0.3023 - val_accuracy: 0.8673 - val_loss: 0.3072 - learning_rate: 5.0000e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m162/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8760 - loss: 0.2960\n",
            "Epoch 24: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8760 - loss: 0.2958 - val_accuracy: 0.8668 - val_loss: 0.3068 - learning_rate: 5.0000e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8699 - loss: 0.3043 - val_accuracy: 0.8668 - val_loss: 0.3068 - learning_rate: 5.0000e-06\n",
            "Epoch 26/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8768 - loss: 0.2864 - val_accuracy: 0.8668 - val_loss: 0.3068 - learning_rate: 5.0000e-06\n",
            "Epoch 27/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8839 - loss: 0.2795 - val_accuracy: 0.8673 - val_loss: 0.3068 - learning_rate: 5.0000e-06\n",
            "Epoch 28/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8731 - loss: 0.2931 - val_accuracy: 0.8668 - val_loss: 0.3068 - learning_rate: 5.0000e-06\n",
            "Epoch 29/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8754 - loss: 0.2929 - val_accuracy: 0.8673 - val_loss: 0.3067 - learning_rate: 5.0000e-06\n",
            "Epoch 30/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8735 - loss: 0.2963 - val_accuracy: 0.8673 - val_loss: 0.3067 - learning_rate: 5.0000e-06\n",
            "Epoch 31/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8822 - loss: 0.2861 - val_accuracy: 0.8673 - val_loss: 0.3067 - learning_rate: 5.0000e-06\n",
            "Epoch 32/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8744 - loss: 0.2962 - val_accuracy: 0.8679 - val_loss: 0.3067 - learning_rate: 5.0000e-06\n",
            "Epoch 33/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8804 - loss: 0.2902 - val_accuracy: 0.8679 - val_loss: 0.3066 - learning_rate: 5.0000e-06\n",
            "Epoch 34/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8748 - loss: 0.2947 - val_accuracy: 0.8679 - val_loss: 0.3066 - learning_rate: 5.0000e-06\n",
            "Epoch 35/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8791 - loss: 0.2898 - val_accuracy: 0.8679 - val_loss: 0.3065 - learning_rate: 5.0000e-06\n",
            "Epoch 36/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8730 - loss: 0.2922 - val_accuracy: 0.8679 - val_loss: 0.3065 - learning_rate: 5.0000e-06\n",
            "Epoch 37/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8750 - loss: 0.2971 - val_accuracy: 0.8679 - val_loss: 0.3065 - learning_rate: 5.0000e-06\n",
            "Epoch 38/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8740 - loss: 0.2875 - val_accuracy: 0.8679 - val_loss: 0.3064 - learning_rate: 5.0000e-06\n",
            "Epoch 39/50\n",
            "\u001b[1m166/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8808 - loss: 0.2879\n",
            "Epoch 39: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8808 - loss: 0.2878 - val_accuracy: 0.8679 - val_loss: 0.3064 - learning_rate: 5.0000e-06\n",
            "Epoch 40/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8796 - loss: 0.2939 - val_accuracy: 0.8679 - val_loss: 0.3064 - learning_rate: 1.0000e-06\n",
            "Epoch 41/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8736 - loss: 0.2934 - val_accuracy: 0.8679 - val_loss: 0.3064 - learning_rate: 1.0000e-06\n",
            "Epoch 42/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8739 - loss: 0.2991 - val_accuracy: 0.8679 - val_loss: 0.3064 - learning_rate: 1.0000e-06\n",
            "Epoch 43/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8792 - loss: 0.2740 - val_accuracy: 0.8679 - val_loss: 0.3064 - learning_rate: 1.0000e-06\n",
            "Epoch 44/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8859 - loss: 0.2781 - val_accuracy: 0.8679 - val_loss: 0.3064 - learning_rate: 1.0000e-06\n",
            "Epoch 45/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8829 - loss: 0.2814 - val_accuracy: 0.8679 - val_loss: 0.3064 - learning_rate: 1.0000e-06\n",
            "Epoch 46/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8858 - loss: 0.2717 - val_accuracy: 0.8679 - val_loss: 0.3064 - learning_rate: 1.0000e-06\n",
            "Epoch 47/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8844 - loss: 0.2798 - val_accuracy: 0.8679 - val_loss: 0.3064 - learning_rate: 1.0000e-06\n",
            "Epoch 48/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8822 - loss: 0.2812 - val_accuracy: 0.8679 - val_loss: 0.3064 - learning_rate: 1.0000e-06\n",
            "Epoch 49/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8856 - loss: 0.2753 - val_accuracy: 0.8679 - val_loss: 0.3064 - learning_rate: 1.0000e-06\n",
            "Epoch 50/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8810 - loss: 0.2882 - val_accuracy: 0.8679 - val_loss: 0.3064 - learning_rate: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "X,Y = preprocessing(df_lb)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, Y, test_size=0.2, random_state=100, stratify=Y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "y_train_onehot, y_test_onehot, y_val_onehot, output_shape = one_hot(y_train,y_test,y_val)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "init_neurons = X_train.shape[1]\n",
        "custom_optimizer = Adam(learning_rate=0.00005)\n",
        "\n",
        "# Define the model\n",
        "model_c2_lb = tf.keras.models.Sequential([\n",
        "    Dense(init_neurons, activation='relu', input_shape=input_shape),\n",
        "    Dense(45, activation='relu'),\n",
        "    Dense(45, activation='relu'),\n",
        "    Dense(output_shape, activation='softmax')\n",
        "])\n",
        "\n",
        "model_c2_lb.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_c2_lb = model_c2_lb.fit(X_train, y_train_onehot, epochs=50, validation_data=(X_val, y_val_onehot), callbacks=[lr_schedule, checkpoint])\n",
        "best_val_accuracy_c2_lb = max(history_c2_lb.history['val_accuracy'])\n",
        "validation_accuracies['model_c2_lb'] = best_val_accuracy_c2_lb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDrLw3tANkww"
      },
      "source": [
        "#### LINE - PIE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-72FTGEQTsf5",
        "outputId": "f5b7b3c7-c207-44fc-dfaa-74df7cbfb00c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'line', 1: 'pie'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.7285 - loss: 0.6256 - val_accuracy: 0.9137 - val_loss: 0.4680 - learning_rate: 5.0000e-05\n",
            "Epoch 2/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9264 - loss: 0.4089 - val_accuracy: 0.9353 - val_loss: 0.2739 - learning_rate: 5.0000e-05\n",
            "Epoch 3/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9472 - loss: 0.2352 - val_accuracy: 0.9466 - val_loss: 0.1883 - learning_rate: 5.0000e-05\n",
            "Epoch 4/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9503 - loss: 0.1749 - val_accuracy: 0.9525 - val_loss: 0.1559 - learning_rate: 5.0000e-05\n",
            "Epoch 5/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9603 - loss: 0.1325 - val_accuracy: 0.9552 - val_loss: 0.1397 - learning_rate: 5.0000e-05\n",
            "Epoch 6/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9573 - loss: 0.1332 - val_accuracy: 0.9563 - val_loss: 0.1304 - learning_rate: 5.0000e-05\n",
            "Epoch 7/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9603 - loss: 0.1222 - val_accuracy: 0.9574 - val_loss: 0.1238 - learning_rate: 5.0000e-05\n",
            "Epoch 8/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9664 - loss: 0.1067 - val_accuracy: 0.9558 - val_loss: 0.1191 - learning_rate: 5.0000e-05\n",
            "Epoch 9/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9642 - loss: 0.1104 - val_accuracy: 0.9563 - val_loss: 0.1157 - learning_rate: 5.0000e-05\n",
            "Epoch 10/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9666 - loss: 0.0992 - val_accuracy: 0.9590 - val_loss: 0.1131 - learning_rate: 5.0000e-05\n",
            "Epoch 11/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9687 - loss: 0.0962 - val_accuracy: 0.9606 - val_loss: 0.1104 - learning_rate: 5.0000e-05\n",
            "Epoch 12/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9669 - loss: 0.0999 - val_accuracy: 0.9628 - val_loss: 0.1084 - learning_rate: 5.0000e-05\n",
            "Epoch 13/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9689 - loss: 0.0940 - val_accuracy: 0.9606 - val_loss: 0.1067 - learning_rate: 5.0000e-05\n",
            "Epoch 14/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9723 - loss: 0.0866 - val_accuracy: 0.9595 - val_loss: 0.1050 - learning_rate: 5.0000e-05\n",
            "Epoch 15/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9667 - loss: 0.0934 - val_accuracy: 0.9628 - val_loss: 0.1039 - learning_rate: 5.0000e-05\n",
            "Epoch 16/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9671 - loss: 0.0952 - val_accuracy: 0.9639 - val_loss: 0.1026 - learning_rate: 5.0000e-05\n",
            "Epoch 17/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9700 - loss: 0.0930 - val_accuracy: 0.9644 - val_loss: 0.1019 - learning_rate: 5.0000e-05\n",
            "Epoch 18/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9738 - loss: 0.0823 - val_accuracy: 0.9633 - val_loss: 0.1006 - learning_rate: 5.0000e-05\n",
            "Epoch 19/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9695 - loss: 0.0848 - val_accuracy: 0.9639 - val_loss: 0.0999 - learning_rate: 5.0000e-05\n",
            "Epoch 20/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9673 - loss: 0.0936 - val_accuracy: 0.9655 - val_loss: 0.1001 - learning_rate: 5.0000e-05\n",
            "Epoch 21/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9677 - loss: 0.0938 - val_accuracy: 0.9671 - val_loss: 0.0986 - learning_rate: 5.0000e-05\n",
            "Epoch 22/50\n",
            "\u001b[1m170/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9731 - loss: 0.0864\n",
            "Epoch 22: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9731 - loss: 0.0863 - val_accuracy: 0.9671 - val_loss: 0.0979 - learning_rate: 5.0000e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9742 - loss: 0.0800 - val_accuracy: 0.9660 - val_loss: 0.0977 - learning_rate: 5.0000e-06\n",
            "Epoch 24/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9732 - loss: 0.0809 - val_accuracy: 0.9660 - val_loss: 0.0976 - learning_rate: 5.0000e-06\n",
            "Epoch 25/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9680 - loss: 0.0940 - val_accuracy: 0.9660 - val_loss: 0.0975 - learning_rate: 5.0000e-06\n",
            "Epoch 26/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9750 - loss: 0.0760 - val_accuracy: 0.9660 - val_loss: 0.0975 - learning_rate: 5.0000e-06\n",
            "Epoch 27/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9726 - loss: 0.0861 - val_accuracy: 0.9660 - val_loss: 0.0974 - learning_rate: 5.0000e-06\n",
            "Epoch 28/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9747 - loss: 0.0769 - val_accuracy: 0.9660 - val_loss: 0.0974 - learning_rate: 5.0000e-06\n",
            "Epoch 29/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9776 - loss: 0.0752 - val_accuracy: 0.9660 - val_loss: 0.0973 - learning_rate: 5.0000e-06\n",
            "Epoch 30/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9763 - loss: 0.0808 - val_accuracy: 0.9660 - val_loss: 0.0973 - learning_rate: 5.0000e-06\n",
            "Epoch 31/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9720 - loss: 0.0810 - val_accuracy: 0.9660 - val_loss: 0.0972 - learning_rate: 5.0000e-06\n",
            "Epoch 32/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9741 - loss: 0.0837 - val_accuracy: 0.9660 - val_loss: 0.0971 - learning_rate: 5.0000e-06\n",
            "Epoch 33/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9722 - loss: 0.0848 - val_accuracy: 0.9660 - val_loss: 0.0971 - learning_rate: 5.0000e-06\n",
            "Epoch 34/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9690 - loss: 0.0909 - val_accuracy: 0.9660 - val_loss: 0.0970 - learning_rate: 5.0000e-06\n",
            "Epoch 35/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9748 - loss: 0.0801 - val_accuracy: 0.9660 - val_loss: 0.0970 - learning_rate: 5.0000e-06\n",
            "Epoch 36/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9719 - loss: 0.0828 - val_accuracy: 0.9660 - val_loss: 0.0969 - learning_rate: 5.0000e-06\n",
            "Epoch 37/50\n",
            "\u001b[1m157/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9718 - loss: 0.0866\n",
            "Epoch 37: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9719 - loss: 0.0862 - val_accuracy: 0.9660 - val_loss: 0.0969 - learning_rate: 5.0000e-06\n",
            "Epoch 38/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9701 - loss: 0.0857 - val_accuracy: 0.9660 - val_loss: 0.0969 - learning_rate: 1.0000e-06\n",
            "Epoch 39/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9717 - loss: 0.0845 - val_accuracy: 0.9660 - val_loss: 0.0969 - learning_rate: 1.0000e-06\n",
            "Epoch 40/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9749 - loss: 0.0782 - val_accuracy: 0.9660 - val_loss: 0.0968 - learning_rate: 1.0000e-06\n",
            "Epoch 41/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9716 - loss: 0.0866 - val_accuracy: 0.9660 - val_loss: 0.0968 - learning_rate: 1.0000e-06\n",
            "Epoch 42/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9699 - loss: 0.0817 - val_accuracy: 0.9660 - val_loss: 0.0968 - learning_rate: 1.0000e-06\n",
            "Epoch 43/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9680 - loss: 0.0936 - val_accuracy: 0.9660 - val_loss: 0.0968 - learning_rate: 1.0000e-06\n",
            "Epoch 44/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9733 - loss: 0.0776 - val_accuracy: 0.9660 - val_loss: 0.0968 - learning_rate: 1.0000e-06\n",
            "Epoch 45/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9699 - loss: 0.0811 - val_accuracy: 0.9660 - val_loss: 0.0968 - learning_rate: 1.0000e-06\n",
            "Epoch 46/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9708 - loss: 0.0852 - val_accuracy: 0.9660 - val_loss: 0.0968 - learning_rate: 1.0000e-06\n",
            "Epoch 47/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9754 - loss: 0.0735 - val_accuracy: 0.9660 - val_loss: 0.0968 - learning_rate: 1.0000e-06\n",
            "Epoch 48/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9693 - loss: 0.0887 - val_accuracy: 0.9660 - val_loss: 0.0968 - learning_rate: 1.0000e-06\n",
            "Epoch 49/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9711 - loss: 0.0817 - val_accuracy: 0.9660 - val_loss: 0.0968 - learning_rate: 1.0000e-06\n",
            "Epoch 50/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9733 - loss: 0.0862 - val_accuracy: 0.9660 - val_loss: 0.0967 - learning_rate: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "X,Y = preprocessing(df_lp)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, Y, test_size=0.2, random_state=100, stratify=Y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)  # 0.25 * 0.8 = 0.2\n",
        "y_train_onehot, y_test_onehot, y_val_onehot, output_shape = one_hot(y_train,y_test,y_val)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "init_neurons = X_train.shape[1]\n",
        "custom_optimizer = Adam(learning_rate=0.00005)\n",
        "\n",
        "# Define the model\n",
        "model_c2_lp = tf.keras.models.Sequential([\n",
        "    Dense(init_neurons, activation='relu', input_shape=input_shape),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(output_shape, activation='softmax')\n",
        "])\n",
        "\n",
        "model_c2_lp.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_c2_lp = model_c2_lp.fit(X_train, y_train_onehot, epochs=50, validation_data=(X_val, y_val_onehot), callbacks=[lr_schedule, checkpoint])\n",
        "best_val_accuracy_c2_lp = max(history_c2_lp.history['val_accuracy'])\n",
        "validation_accuracies['model_c2_lp'] = best_val_accuracy_c2_lp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnQ5bZGzNpZJ"
      },
      "source": [
        "#### LINE - SCATTER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9kLuSnGVjHi",
        "outputId": "7ead0df6-8baf-479a-bbc6-955e4ec86cfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'line', 1: 'scatter'}\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4925 - loss: 0.7394 - val_accuracy: 0.5469 - val_loss: 0.6927 - learning_rate: 5.0000e-05\n",
            "Epoch 2/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5557 - loss: 0.6850 - val_accuracy: 0.6084 - val_loss: 0.6700 - learning_rate: 5.0000e-05\n",
            "Epoch 3/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6119 - loss: 0.6662 - val_accuracy: 0.6273 - val_loss: 0.6579 - learning_rate: 5.0000e-05\n",
            "Epoch 4/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6291 - loss: 0.6552 - val_accuracy: 0.6321 - val_loss: 0.6504 - learning_rate: 5.0000e-05\n",
            "Epoch 5/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6362 - loss: 0.6456 - val_accuracy: 0.6338 - val_loss: 0.6452 - learning_rate: 5.0000e-05\n",
            "Epoch 6/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6233 - loss: 0.6466 - val_accuracy: 0.6327 - val_loss: 0.6417 - learning_rate: 5.0000e-05\n",
            "Epoch 7/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6337 - loss: 0.6405 - val_accuracy: 0.6408 - val_loss: 0.6391 - learning_rate: 5.0000e-05\n",
            "Epoch 8/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6302 - loss: 0.6366 - val_accuracy: 0.6370 - val_loss: 0.6364 - learning_rate: 5.0000e-05\n",
            "Epoch 9/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6483 - loss: 0.6244 - val_accuracy: 0.6381 - val_loss: 0.6345 - learning_rate: 5.0000e-05\n",
            "Epoch 10/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6402 - loss: 0.6251 - val_accuracy: 0.6354 - val_loss: 0.6331 - learning_rate: 5.0000e-05\n",
            "Epoch 11/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6325 - loss: 0.6289 - val_accuracy: 0.6381 - val_loss: 0.6318 - learning_rate: 5.0000e-05\n",
            "Epoch 12/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6388 - loss: 0.6226 - val_accuracy: 0.6424 - val_loss: 0.6305 - learning_rate: 5.0000e-05\n",
            "Epoch 13/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6441 - loss: 0.6197 - val_accuracy: 0.6456 - val_loss: 0.6297 - learning_rate: 5.0000e-05\n",
            "Epoch 14/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6520 - loss: 0.6149 - val_accuracy: 0.6419 - val_loss: 0.6285 - learning_rate: 5.0000e-05\n",
            "Epoch 15/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6534 - loss: 0.6121 - val_accuracy: 0.6462 - val_loss: 0.6279 - learning_rate: 5.0000e-05\n",
            "Epoch 16/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6419 - loss: 0.6202 - val_accuracy: 0.6516 - val_loss: 0.6276 - learning_rate: 5.0000e-05\n",
            "Epoch 17/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6492 - loss: 0.6150 - val_accuracy: 0.6467 - val_loss: 0.6267 - learning_rate: 5.0000e-05\n",
            "Epoch 18/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6492 - loss: 0.6136 - val_accuracy: 0.6505 - val_loss: 0.6259 - learning_rate: 5.0000e-05\n",
            "Epoch 19/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6558 - loss: 0.6093 - val_accuracy: 0.6472 - val_loss: 0.6253 - learning_rate: 5.0000e-05\n",
            "Epoch 20/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6482 - loss: 0.6149 - val_accuracy: 0.6521 - val_loss: 0.6252 - learning_rate: 5.0000e-05\n",
            "Epoch 21/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6699 - loss: 0.6007 - val_accuracy: 0.6516 - val_loss: 0.6250 - learning_rate: 5.0000e-05\n",
            "Epoch 22/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6650 - loss: 0.6017 - val_accuracy: 0.6537 - val_loss: 0.6253 - learning_rate: 5.0000e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6558 - loss: 0.6081 - val_accuracy: 0.6516 - val_loss: 0.6252 - learning_rate: 5.0000e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6653 - loss: 0.5999 - val_accuracy: 0.6516 - val_loss: 0.6251 - learning_rate: 5.0000e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6630 - loss: 0.6017 - val_accuracy: 0.6526 - val_loss: 0.6241 - learning_rate: 5.0000e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6645 - loss: 0.5999 - val_accuracy: 0.6570 - val_loss: 0.6244 - learning_rate: 5.0000e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6665 - loss: 0.5965 - val_accuracy: 0.6510 - val_loss: 0.6244 - learning_rate: 5.0000e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6689 - loss: 0.5957 - val_accuracy: 0.6543 - val_loss: 0.6241 - learning_rate: 5.0000e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6626 - loss: 0.5975 - val_accuracy: 0.6543 - val_loss: 0.6248 - learning_rate: 5.0000e-05\n",
            "Epoch 30/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6698 - loss: 0.5872 - val_accuracy: 0.6526 - val_loss: 0.6243 - learning_rate: 5.0000e-05\n",
            "Epoch 31/50\n",
            "\u001b[1m160/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6709 - loss: 0.5938\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6709 - loss: 0.5936 - val_accuracy: 0.6543 - val_loss: 0.6246 - learning_rate: 5.0000e-05\n",
            "Epoch 32/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6795 - loss: 0.5791 - val_accuracy: 0.6532 - val_loss: 0.6245 - learning_rate: 5.0000e-06\n",
            "Epoch 33/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6778 - loss: 0.5880 - val_accuracy: 0.6537 - val_loss: 0.6245 - learning_rate: 5.0000e-06\n",
            "Epoch 34/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6694 - loss: 0.5905 - val_accuracy: 0.6537 - val_loss: 0.6245 - learning_rate: 5.0000e-06\n",
            "Epoch 35/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6802 - loss: 0.5844 - val_accuracy: 0.6548 - val_loss: 0.6245 - learning_rate: 5.0000e-06\n",
            "Epoch 36/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6804 - loss: 0.5836 - val_accuracy: 0.6526 - val_loss: 0.6245 - learning_rate: 5.0000e-06\n",
            "Epoch 37/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6838 - loss: 0.5813 - val_accuracy: 0.6526 - val_loss: 0.6245 - learning_rate: 5.0000e-06\n",
            "Epoch 38/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6737 - loss: 0.5847 - val_accuracy: 0.6532 - val_loss: 0.6245 - learning_rate: 5.0000e-06\n",
            "Epoch 39/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6756 - loss: 0.5880 - val_accuracy: 0.6526 - val_loss: 0.6246 - learning_rate: 5.0000e-06\n",
            "Epoch 40/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6738 - loss: 0.5909 - val_accuracy: 0.6521 - val_loss: 0.6245 - learning_rate: 5.0000e-06\n",
            "Epoch 41/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6689 - loss: 0.5922 - val_accuracy: 0.6526 - val_loss: 0.6245 - learning_rate: 5.0000e-06\n",
            "Epoch 42/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6723 - loss: 0.5905 - val_accuracy: 0.6526 - val_loss: 0.6245 - learning_rate: 5.0000e-06\n",
            "Epoch 43/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6736 - loss: 0.5913 - val_accuracy: 0.6521 - val_loss: 0.6246 - learning_rate: 5.0000e-06\n",
            "Epoch 44/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6774 - loss: 0.5856 - val_accuracy: 0.6521 - val_loss: 0.6246 - learning_rate: 5.0000e-06\n",
            "Epoch 45/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6706 - loss: 0.5886 - val_accuracy: 0.6526 - val_loss: 0.6245 - learning_rate: 5.0000e-06\n",
            "Epoch 46/50\n",
            "\u001b[1m152/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6447 - loss: 0.6043\n",
            "Epoch 46: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6481 - loss: 0.6023 - val_accuracy: 0.6526 - val_loss: 0.6246 - learning_rate: 5.0000e-06\n",
            "Epoch 47/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6773 - loss: 0.5845 - val_accuracy: 0.6526 - val_loss: 0.6246 - learning_rate: 1.0000e-06\n",
            "Epoch 48/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6791 - loss: 0.5832 - val_accuracy: 0.6532 - val_loss: 0.6246 - learning_rate: 1.0000e-06\n",
            "Epoch 49/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6701 - loss: 0.5918 - val_accuracy: 0.6526 - val_loss: 0.6246 - learning_rate: 1.0000e-06\n",
            "Epoch 50/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6814 - loss: 0.5824 - val_accuracy: 0.6532 - val_loss: 0.6246 - learning_rate: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "X,Y = preprocessing(df_ls)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, Y, test_size=0.2, random_state=100, stratify=Y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "y_train_onehot, y_test_onehot, y_val_onehot, output_shape = one_hot(y_train,y_test,y_val)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "init_neurons = X_train.shape[1]\n",
        "custom_optimizer = Adam(learning_rate=0.00005)\n",
        "\n",
        "# Define the model\n",
        "model_c2_ls = tf.keras.models.Sequential([\n",
        "    Dense(init_neurons, activation='relu', input_shape=input_shape),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(output_shape, activation='softmax')\n",
        "])\n",
        "\n",
        "model_c2_ls.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_c2_ls = model_c2_ls.fit(X_train, y_train_onehot, epochs=50, validation_data=(X_val, y_val_onehot), callbacks=[lr_schedule, checkpoint])\n",
        "best_val_accuracy_c2_ls = max(history_c2_ls.history['val_accuracy'])\n",
        "validation_accuracies['model_c2_ls'] = best_val_accuracy_c2_ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnaIPlaJNwsy"
      },
      "source": [
        "#### SCATTER - BAR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2Z3kzYIYxhO",
        "outputId": "7ddc8f1d-c107-49e8-f9cc-191c26904ab9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'bar', 1: 'scatter'}\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5122 - loss: 0.7116 - val_accuracy: 0.6990 - val_loss: 0.6157 - learning_rate: 5.0000e-05\n",
            "Epoch 2/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7181 - loss: 0.6003 - val_accuracy: 0.7654 - val_loss: 0.5314 - learning_rate: 5.0000e-05\n",
            "Epoch 3/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7755 - loss: 0.5170 - val_accuracy: 0.7869 - val_loss: 0.4804 - learning_rate: 5.0000e-05\n",
            "Epoch 4/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7925 - loss: 0.4771 - val_accuracy: 0.8010 - val_loss: 0.4529 - learning_rate: 5.0000e-05\n",
            "Epoch 5/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7978 - loss: 0.4569 - val_accuracy: 0.8112 - val_loss: 0.4361 - learning_rate: 5.0000e-05\n",
            "Epoch 6/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8043 - loss: 0.4473 - val_accuracy: 0.8139 - val_loss: 0.4256 - learning_rate: 5.0000e-05\n",
            "Epoch 7/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8118 - loss: 0.4242 - val_accuracy: 0.8107 - val_loss: 0.4174 - learning_rate: 5.0000e-05\n",
            "Epoch 8/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8224 - loss: 0.4140 - val_accuracy: 0.8161 - val_loss: 0.4126 - learning_rate: 5.0000e-05\n",
            "Epoch 9/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8218 - loss: 0.4174 - val_accuracy: 0.8177 - val_loss: 0.4090 - learning_rate: 5.0000e-05\n",
            "Epoch 10/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8200 - loss: 0.4094 - val_accuracy: 0.8193 - val_loss: 0.4061 - learning_rate: 5.0000e-05\n",
            "Epoch 11/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8209 - loss: 0.4105 - val_accuracy: 0.8193 - val_loss: 0.4035 - learning_rate: 5.0000e-05\n",
            "Epoch 12/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8217 - loss: 0.4045 - val_accuracy: 0.8150 - val_loss: 0.4020 - learning_rate: 5.0000e-05\n",
            "Epoch 13/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8328 - loss: 0.3843 - val_accuracy: 0.8188 - val_loss: 0.4001 - learning_rate: 5.0000e-05\n",
            "Epoch 14/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8234 - loss: 0.3974 - val_accuracy: 0.8182 - val_loss: 0.3987 - learning_rate: 5.0000e-05\n",
            "Epoch 15/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8226 - loss: 0.4020 - val_accuracy: 0.8188 - val_loss: 0.3974 - learning_rate: 5.0000e-05\n",
            "Epoch 16/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8259 - loss: 0.3959 - val_accuracy: 0.8209 - val_loss: 0.3957 - learning_rate: 5.0000e-05\n",
            "Epoch 17/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8284 - loss: 0.3900 - val_accuracy: 0.8236 - val_loss: 0.3946 - learning_rate: 5.0000e-05\n",
            "Epoch 18/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8262 - loss: 0.3975 - val_accuracy: 0.8198 - val_loss: 0.3941 - learning_rate: 5.0000e-05\n",
            "Epoch 19/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8276 - loss: 0.3823 - val_accuracy: 0.8236 - val_loss: 0.3936 - learning_rate: 5.0000e-05\n",
            "Epoch 20/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8296 - loss: 0.3804 - val_accuracy: 0.8193 - val_loss: 0.3924 - learning_rate: 5.0000e-05\n",
            "Epoch 21/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8361 - loss: 0.3724 - val_accuracy: 0.8209 - val_loss: 0.3913 - learning_rate: 5.0000e-05\n",
            "Epoch 22/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8411 - loss: 0.3731 - val_accuracy: 0.8209 - val_loss: 0.3910 - learning_rate: 5.0000e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8422 - loss: 0.3682 - val_accuracy: 0.8242 - val_loss: 0.3903 - learning_rate: 5.0000e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8330 - loss: 0.3860 - val_accuracy: 0.8220 - val_loss: 0.3898 - learning_rate: 5.0000e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8398 - loss: 0.3719 - val_accuracy: 0.8242 - val_loss: 0.3895 - learning_rate: 5.0000e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8300 - loss: 0.3802 - val_accuracy: 0.8247 - val_loss: 0.3888 - learning_rate: 5.0000e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8344 - loss: 0.3720 - val_accuracy: 0.8231 - val_loss: 0.3884 - learning_rate: 5.0000e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8411 - loss: 0.3684 - val_accuracy: 0.8258 - val_loss: 0.3877 - learning_rate: 5.0000e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8312 - loss: 0.3729 - val_accuracy: 0.8236 - val_loss: 0.3881 - learning_rate: 5.0000e-05\n",
            "Epoch 30/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8389 - loss: 0.3646 - val_accuracy: 0.8269 - val_loss: 0.3865 - learning_rate: 5.0000e-05\n",
            "Epoch 31/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8366 - loss: 0.3654 - val_accuracy: 0.8269 - val_loss: 0.3864 - learning_rate: 5.0000e-05\n",
            "Epoch 32/50\n",
            "\u001b[1m155/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8390 - loss: 0.3572\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8391 - loss: 0.3578 - val_accuracy: 0.8225 - val_loss: 0.3859 - learning_rate: 5.0000e-05\n",
            "Epoch 33/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8401 - loss: 0.3581 - val_accuracy: 0.8236 - val_loss: 0.3858 - learning_rate: 5.0000e-06\n",
            "Epoch 34/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8506 - loss: 0.3504 - val_accuracy: 0.8236 - val_loss: 0.3857 - learning_rate: 5.0000e-06\n",
            "Epoch 35/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8425 - loss: 0.3590 - val_accuracy: 0.8247 - val_loss: 0.3857 - learning_rate: 5.0000e-06\n",
            "Epoch 36/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8379 - loss: 0.3623 - val_accuracy: 0.8236 - val_loss: 0.3856 - learning_rate: 5.0000e-06\n",
            "Epoch 37/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8403 - loss: 0.3703 - val_accuracy: 0.8252 - val_loss: 0.3855 - learning_rate: 5.0000e-06\n",
            "Epoch 38/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8316 - loss: 0.3720 - val_accuracy: 0.8247 - val_loss: 0.3855 - learning_rate: 5.0000e-06\n",
            "Epoch 39/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8363 - loss: 0.3612 - val_accuracy: 0.8247 - val_loss: 0.3854 - learning_rate: 5.0000e-06\n",
            "Epoch 40/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8478 - loss: 0.3494 - val_accuracy: 0.8247 - val_loss: 0.3854 - learning_rate: 5.0000e-06\n",
            "Epoch 41/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8386 - loss: 0.3665 - val_accuracy: 0.8252 - val_loss: 0.3853 - learning_rate: 5.0000e-06\n",
            "Epoch 42/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8369 - loss: 0.3643 - val_accuracy: 0.8247 - val_loss: 0.3853 - learning_rate: 5.0000e-06\n",
            "Epoch 43/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8371 - loss: 0.3586 - val_accuracy: 0.8247 - val_loss: 0.3852 - learning_rate: 5.0000e-06\n",
            "Epoch 44/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8444 - loss: 0.3527 - val_accuracy: 0.8247 - val_loss: 0.3852 - learning_rate: 5.0000e-06\n",
            "Epoch 45/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8348 - loss: 0.3710 - val_accuracy: 0.8236 - val_loss: 0.3851 - learning_rate: 5.0000e-06\n",
            "Epoch 46/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8439 - loss: 0.3544 - val_accuracy: 0.8247 - val_loss: 0.3851 - learning_rate: 5.0000e-06\n",
            "Epoch 47/50\n",
            "\u001b[1m149/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8437 - loss: 0.3524\n",
            "Epoch 47: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8434 - loss: 0.3534 - val_accuracy: 0.8247 - val_loss: 0.3850 - learning_rate: 5.0000e-06\n",
            "Epoch 48/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8332 - loss: 0.3689 - val_accuracy: 0.8247 - val_loss: 0.3850 - learning_rate: 1.0000e-06\n",
            "Epoch 49/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8409 - loss: 0.3649 - val_accuracy: 0.8247 - val_loss: 0.3850 - learning_rate: 1.0000e-06\n",
            "Epoch 50/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8358 - loss: 0.3605 - val_accuracy: 0.8242 - val_loss: 0.3850 - learning_rate: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "X,Y = preprocessing(df_sb)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, Y, test_size=0.2, random_state=100, stratify=Y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "y_train_onehot, y_test_onehot, y_val_onehot, output_shape = one_hot(y_train,y_test,y_val)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "init_neurons = X_train.shape[1]\n",
        "custom_optimizer = Adam(learning_rate=0.00005)\n",
        "\n",
        "# Define the model\n",
        "model_c2_sb = tf.keras.models.Sequential([\n",
        "    Dense(init_neurons, activation='relu', input_shape=input_shape),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(output_shape, activation='softmax')\n",
        "])\n",
        "\n",
        "model_c2_sb.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_c2_sb = model_c2_sb.fit(X_train, y_train_onehot, epochs=50, validation_data=(X_val, y_val_onehot), callbacks=[lr_schedule, checkpoint])\n",
        "best_val_accuracy_c2_sb = max(history_c2_sb.history['val_accuracy'])\n",
        "validation_accuracies['model_c2_sb'] = best_val_accuracy_c2_sb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHWLb3jMN3Ou"
      },
      "source": [
        "#### SCATTER - PIE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klZS02r-ZKlE",
        "outputId": "0316e2b3-9f4c-4861-f0b4-217b9e46d850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'pie', 1: 'scatter'}\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5880 - loss: 0.6471 - val_accuracy: 0.8781 - val_loss: 0.4577 - learning_rate: 5.0000e-05\n",
            "Epoch 2/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9056 - loss: 0.4040 - val_accuracy: 0.9142 - val_loss: 0.2970 - learning_rate: 5.0000e-05\n",
            "Epoch 3/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9302 - loss: 0.2618 - val_accuracy: 0.9261 - val_loss: 0.2319 - learning_rate: 5.0000e-05\n",
            "Epoch 4/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9390 - loss: 0.2055 - val_accuracy: 0.9293 - val_loss: 0.2060 - learning_rate: 5.0000e-05\n",
            "Epoch 5/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9325 - loss: 0.1888 - val_accuracy: 0.9293 - val_loss: 0.1935 - learning_rate: 5.0000e-05\n",
            "Epoch 6/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9351 - loss: 0.1726 - val_accuracy: 0.9337 - val_loss: 0.1857 - learning_rate: 5.0000e-05\n",
            "Epoch 7/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9397 - loss: 0.1727 - val_accuracy: 0.9374 - val_loss: 0.1819 - learning_rate: 5.0000e-05\n",
            "Epoch 8/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9435 - loss: 0.1554 - val_accuracy: 0.9374 - val_loss: 0.1769 - learning_rate: 5.0000e-05\n",
            "Epoch 9/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9410 - loss: 0.1577 - val_accuracy: 0.9412 - val_loss: 0.1736 - learning_rate: 5.0000e-05\n",
            "Epoch 10/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9515 - loss: 0.1429 - val_accuracy: 0.9423 - val_loss: 0.1706 - learning_rate: 5.0000e-05\n",
            "Epoch 11/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9517 - loss: 0.1414 - val_accuracy: 0.9444 - val_loss: 0.1727 - learning_rate: 5.0000e-05\n",
            "Epoch 12/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9533 - loss: 0.1402 - val_accuracy: 0.9477 - val_loss: 0.1670 - learning_rate: 5.0000e-05\n",
            "Epoch 13/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9525 - loss: 0.1439 - val_accuracy: 0.9471 - val_loss: 0.1650 - learning_rate: 5.0000e-05\n",
            "Epoch 14/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9530 - loss: 0.1369 - val_accuracy: 0.9482 - val_loss: 0.1638 - learning_rate: 5.0000e-05\n",
            "Epoch 15/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9472 - loss: 0.1474 - val_accuracy: 0.9493 - val_loss: 0.1629 - learning_rate: 5.0000e-05\n",
            "Epoch 16/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9515 - loss: 0.1341 - val_accuracy: 0.9482 - val_loss: 0.1609 - learning_rate: 5.0000e-05\n",
            "Epoch 17/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9512 - loss: 0.1356 - val_accuracy: 0.9509 - val_loss: 0.1620 - learning_rate: 5.0000e-05\n",
            "Epoch 18/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9483 - loss: 0.1426 - val_accuracy: 0.9509 - val_loss: 0.1603 - learning_rate: 5.0000e-05\n",
            "Epoch 19/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9529 - loss: 0.1303 - val_accuracy: 0.9509 - val_loss: 0.1587 - learning_rate: 5.0000e-05\n",
            "Epoch 20/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9503 - loss: 0.1354 - val_accuracy: 0.9504 - val_loss: 0.1581 - learning_rate: 5.0000e-05\n",
            "Epoch 21/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9532 - loss: 0.1342 - val_accuracy: 0.9515 - val_loss: 0.1576 - learning_rate: 5.0000e-05\n",
            "Epoch 22/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9509 - loss: 0.1337 - val_accuracy: 0.9509 - val_loss: 0.1573 - learning_rate: 5.0000e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9570 - loss: 0.1189 - val_accuracy: 0.9504 - val_loss: 0.1562 - learning_rate: 5.0000e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9555 - loss: 0.1250 - val_accuracy: 0.9504 - val_loss: 0.1564 - learning_rate: 5.0000e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9547 - loss: 0.1217 - val_accuracy: 0.9509 - val_loss: 0.1553 - learning_rate: 5.0000e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9561 - loss: 0.1150 - val_accuracy: 0.9515 - val_loss: 0.1551 - learning_rate: 5.0000e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m162/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9520 - loss: 0.1393\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9523 - loss: 0.1380 - val_accuracy: 0.9509 - val_loss: 0.1547 - learning_rate: 5.0000e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9552 - loss: 0.1240 - val_accuracy: 0.9515 - val_loss: 0.1548 - learning_rate: 5.0000e-06\n",
            "Epoch 29/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9568 - loss: 0.1215 - val_accuracy: 0.9520 - val_loss: 0.1549 - learning_rate: 5.0000e-06\n",
            "Epoch 30/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9582 - loss: 0.1178 - val_accuracy: 0.9520 - val_loss: 0.1548 - learning_rate: 5.0000e-06\n",
            "Epoch 31/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9622 - loss: 0.1112 - val_accuracy: 0.9520 - val_loss: 0.1547 - learning_rate: 5.0000e-06\n",
            "Epoch 32/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9572 - loss: 0.1221 - val_accuracy: 0.9520 - val_loss: 0.1547 - learning_rate: 5.0000e-06\n",
            "Epoch 33/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9536 - loss: 0.1243 - val_accuracy: 0.9520 - val_loss: 0.1548 - learning_rate: 5.0000e-06\n",
            "Epoch 34/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9590 - loss: 0.1196 - val_accuracy: 0.9520 - val_loss: 0.1546 - learning_rate: 5.0000e-06\n",
            "Epoch 35/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9572 - loss: 0.1218 - val_accuracy: 0.9520 - val_loss: 0.1546 - learning_rate: 5.0000e-06\n",
            "Epoch 36/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9609 - loss: 0.1155 - val_accuracy: 0.9520 - val_loss: 0.1546 - learning_rate: 5.0000e-06\n",
            "Epoch 37/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9613 - loss: 0.1108 - val_accuracy: 0.9515 - val_loss: 0.1546 - learning_rate: 5.0000e-06\n",
            "Epoch 38/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9571 - loss: 0.1202 - val_accuracy: 0.9520 - val_loss: 0.1546 - learning_rate: 5.0000e-06\n",
            "Epoch 39/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9578 - loss: 0.1174 - val_accuracy: 0.9525 - val_loss: 0.1545 - learning_rate: 5.0000e-06\n",
            "Epoch 40/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9570 - loss: 0.1318 - val_accuracy: 0.9520 - val_loss: 0.1545 - learning_rate: 5.0000e-06\n",
            "Epoch 41/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9597 - loss: 0.1142 - val_accuracy: 0.9525 - val_loss: 0.1545 - learning_rate: 5.0000e-06\n",
            "Epoch 42/50\n",
            "\u001b[1m159/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9562 - loss: 0.1159\n",
            "Epoch 42: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9564 - loss: 0.1162 - val_accuracy: 0.9520 - val_loss: 0.1544 - learning_rate: 5.0000e-06\n",
            "Epoch 43/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9578 - loss: 0.1230 - val_accuracy: 0.9520 - val_loss: 0.1544 - learning_rate: 1.0000e-06\n",
            "Epoch 44/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9596 - loss: 0.1185 - val_accuracy: 0.9520 - val_loss: 0.1544 - learning_rate: 1.0000e-06\n",
            "Epoch 45/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9548 - loss: 0.1298 - val_accuracy: 0.9520 - val_loss: 0.1544 - learning_rate: 1.0000e-06\n",
            "Epoch 46/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9587 - loss: 0.1131 - val_accuracy: 0.9520 - val_loss: 0.1544 - learning_rate: 1.0000e-06\n",
            "Epoch 47/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9539 - loss: 0.1213 - val_accuracy: 0.9525 - val_loss: 0.1544 - learning_rate: 1.0000e-06\n",
            "Epoch 48/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9577 - loss: 0.1189 - val_accuracy: 0.9525 - val_loss: 0.1544 - learning_rate: 1.0000e-06\n",
            "Epoch 49/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9568 - loss: 0.1202 - val_accuracy: 0.9525 - val_loss: 0.1544 - learning_rate: 1.0000e-06\n",
            "Epoch 50/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9600 - loss: 0.1146 - val_accuracy: 0.9525 - val_loss: 0.1544 - learning_rate: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "X,Y = preprocessing(df_sp)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, Y, test_size=0.2, random_state=100, stratify=Y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "y_train_onehot, y_test_onehot, y_val_onehot, output_shape = one_hot(y_train,y_test,y_val)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "init_neurons = X_train.shape[1]\n",
        "custom_optimizer = Adam(learning_rate=0.00005)\n",
        "\n",
        "# Define the model\n",
        "model_c2_sp = tf.keras.models.Sequential([\n",
        "    Dense(init_neurons, activation='relu', input_shape=input_shape),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(output_shape, activation='softmax')\n",
        "])\n",
        "\n",
        "model_c2_sp.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_c2_sp = model_c2_sp.fit(X_train, y_train_onehot, epochs=50, validation_data=(X_val, y_val_onehot), callbacks=[lr_schedule, checkpoint])\n",
        "best_val_accuracy_c2_sp = max(history_c2_sp.history['val_accuracy'])\n",
        "validation_accuracies['model_c2_sp'] = best_val_accuracy_c2_sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hQnaLoDbLlf",
        "outputId": "8bdf3174-06b6-45f8-c18a-dbe091d1be48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best validation accuracy: 95.20%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Best validation accuracy: {best_val_accuracy_c2_sp * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVa5PlDLCOCc"
      },
      "source": [
        "### C=3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J8pk6znN_OO"
      },
      "source": [
        "#### LINE - BAR - PIE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMc5M4gQSoV2",
        "outputId": "f1ef9b71-3171-41ca-899f-b0aa12cd4b60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'bar', 1: 'line', 2: 'pie'}\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6246 - loss: 0.8001 - val_accuracy: 0.7533 - val_loss: 0.5641 - learning_rate: 5.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7546 - loss: 0.5487 - val_accuracy: 0.7652 - val_loss: 0.5434 - learning_rate: 5.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7680 - loss: 0.5251 - val_accuracy: 0.7620 - val_loss: 0.5397 - learning_rate: 5.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7798 - loss: 0.5099 - val_accuracy: 0.7695 - val_loss: 0.5316 - learning_rate: 5.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7909 - loss: 0.4967 - val_accuracy: 0.7666 - val_loss: 0.5277 - learning_rate: 5.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7914 - loss: 0.4888 - val_accuracy: 0.7767 - val_loss: 0.5222 - learning_rate: 5.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7900 - loss: 0.4873 - val_accuracy: 0.7709 - val_loss: 0.5187 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8051 - loss: 0.4672 - val_accuracy: 0.7756 - val_loss: 0.5182 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7998 - loss: 0.4682 - val_accuracy: 0.7792 - val_loss: 0.5182 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7958 - loss: 0.4736 - val_accuracy: 0.7709 - val_loss: 0.5214 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8018 - loss: 0.4580 - val_accuracy: 0.7745 - val_loss: 0.5215 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8147 - loss: 0.4439 - val_accuracy: 0.7670 - val_loss: 0.5290 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8109 - loss: 0.4348 - val_accuracy: 0.7677 - val_loss: 0.5251 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8189 - loss: 0.4327 - val_accuracy: 0.7652 - val_loss: 0.5317 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8045 - loss: 0.4378 - val_accuracy: 0.7641 - val_loss: 0.5367 - learning_rate: 5.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8155 - loss: 0.4359 - val_accuracy: 0.7656 - val_loss: 0.5301 - learning_rate: 5.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8193 - loss: 0.4245 - val_accuracy: 0.7691 - val_loss: 0.5301 - learning_rate: 5.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8175 - loss: 0.4189 - val_accuracy: 0.7742 - val_loss: 0.5345 - learning_rate: 5.0000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8215 - loss: 0.4174 - val_accuracy: 0.7699 - val_loss: 0.5387 - learning_rate: 5.0000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8248 - loss: 0.4010 - val_accuracy: 0.7670 - val_loss: 0.5382 - learning_rate: 5.0000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m247/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8243 - loss: 0.4019\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8241 - loss: 0.4024 - val_accuracy: 0.7709 - val_loss: 0.5346 - learning_rate: 5.0000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8350 - loss: 0.3916 - val_accuracy: 0.7774 - val_loss: 0.5211 - learning_rate: 5.0000e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8308 - loss: 0.3970 - val_accuracy: 0.7767 - val_loss: 0.5226 - learning_rate: 5.0000e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8356 - loss: 0.3879 - val_accuracy: 0.7817 - val_loss: 0.5228 - learning_rate: 5.0000e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8445 - loss: 0.3792 - val_accuracy: 0.7771 - val_loss: 0.5244 - learning_rate: 5.0000e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8467 - loss: 0.3739 - val_accuracy: 0.7749 - val_loss: 0.5261 - learning_rate: 5.0000e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8460 - loss: 0.3719 - val_accuracy: 0.7767 - val_loss: 0.5259 - learning_rate: 5.0000e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8429 - loss: 0.3793 - val_accuracy: 0.7778 - val_loss: 0.5272 - learning_rate: 5.0000e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8416 - loss: 0.3701 - val_accuracy: 0.7767 - val_loss: 0.5283 - learning_rate: 5.0000e-05\n",
            "Epoch 30/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8446 - loss: 0.3745 - val_accuracy: 0.7763 - val_loss: 0.5285 - learning_rate: 5.0000e-05\n",
            "Epoch 31/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8384 - loss: 0.3835 - val_accuracy: 0.7753 - val_loss: 0.5309 - learning_rate: 5.0000e-05\n",
            "Epoch 32/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8411 - loss: 0.3718 - val_accuracy: 0.7745 - val_loss: 0.5308 - learning_rate: 5.0000e-05\n",
            "Epoch 33/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8480 - loss: 0.3640 - val_accuracy: 0.7799 - val_loss: 0.5316 - learning_rate: 5.0000e-05\n",
            "Epoch 34/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8376 - loss: 0.3858 - val_accuracy: 0.7756 - val_loss: 0.5323 - learning_rate: 5.0000e-05\n",
            "Epoch 35/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8476 - loss: 0.3707 - val_accuracy: 0.7778 - val_loss: 0.5322 - learning_rate: 5.0000e-05\n",
            "Epoch 36/50\n",
            "\u001b[1m231/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8484 - loss: 0.3755\n",
            "Epoch 36: ReduceLROnPlateau reducing learning rate to 5.000000237487257e-06.\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8481 - loss: 0.3751 - val_accuracy: 0.7753 - val_loss: 0.5353 - learning_rate: 5.0000e-05\n",
            "Epoch 37/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8436 - loss: 0.3709 - val_accuracy: 0.7781 - val_loss: 0.5341 - learning_rate: 5.0000e-06\n",
            "Epoch 38/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8489 - loss: 0.3582 - val_accuracy: 0.7796 - val_loss: 0.5339 - learning_rate: 5.0000e-06\n",
            "Epoch 39/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8452 - loss: 0.3650 - val_accuracy: 0.7781 - val_loss: 0.5340 - learning_rate: 5.0000e-06\n",
            "Epoch 40/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8450 - loss: 0.3711 - val_accuracy: 0.7767 - val_loss: 0.5339 - learning_rate: 5.0000e-06\n",
            "Epoch 41/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8523 - loss: 0.3559 - val_accuracy: 0.7778 - val_loss: 0.5339 - learning_rate: 5.0000e-06\n",
            "Epoch 42/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8521 - loss: 0.3632 - val_accuracy: 0.7778 - val_loss: 0.5340 - learning_rate: 5.0000e-06\n",
            "Epoch 43/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8533 - loss: 0.3591 - val_accuracy: 0.7771 - val_loss: 0.5340 - learning_rate: 5.0000e-06\n",
            "Epoch 44/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8438 - loss: 0.3674 - val_accuracy: 0.7774 - val_loss: 0.5341 - learning_rate: 5.0000e-06\n",
            "Epoch 45/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8372 - loss: 0.3770 - val_accuracy: 0.7771 - val_loss: 0.5341 - learning_rate: 5.0000e-06\n",
            "Epoch 46/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8495 - loss: 0.3616 - val_accuracy: 0.7771 - val_loss: 0.5342 - learning_rate: 5.0000e-06\n",
            "Epoch 47/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8492 - loss: 0.3645 - val_accuracy: 0.7774 - val_loss: 0.5342 - learning_rate: 5.0000e-06\n",
            "Epoch 48/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8460 - loss: 0.3718 - val_accuracy: 0.7763 - val_loss: 0.5344 - learning_rate: 5.0000e-06\n",
            "Epoch 49/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8521 - loss: 0.3601 - val_accuracy: 0.7763 - val_loss: 0.5344 - learning_rate: 5.0000e-06\n",
            "Epoch 50/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8526 - loss: 0.3611 - val_accuracy: 0.7767 - val_loss: 0.5345 - learning_rate: 5.0000e-06\n"
          ]
        }
      ],
      "source": [
        "X,Y = preprocessing(df_lbp)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, Y, test_size=0.2, random_state=100, stratify=Y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "y_train_onehot, y_test_onehot, y_val_onehot, output_shape = one_hot(y_train,y_test,y_val)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "init_neurons = X_train.shape[1]\n",
        "custom_optimizer = Adam(learning_rate=0.0005)\n",
        "\n",
        "# Define the model\n",
        "model_c3_lbp = tf.keras.models.Sequential([\n",
        "    Dense(init_neurons, activation='relu', input_shape=input_shape),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(output_shape, activation='softmax')\n",
        "])\n",
        "\n",
        "model_c3_lbp.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_c3_lbp = model_c3_lbp.fit(X_train, y_train_onehot, epochs=50, validation_data=(X_val, y_val_onehot), callbacks=[lr_schedule, checkpoint])\n",
        "best_val_accuracy_c3_lbp = max(history_c3_lbp.history['val_accuracy'])\n",
        "validation_accuracies['model_c3_lbp'] = best_val_accuracy_c3_lbp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj2ZvH9ojFms"
      },
      "source": [
        "#### LINE - SCATTER - BAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EeSR7mxFMGj",
        "outputId": "6b92c338-3a2b-4c90-a801-9539a1d78012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'bar', 1: 'line', 2: 'scatter'}\n"
          ]
        }
      ],
      "source": [
        "X,Y = preprocessing(df_lsb)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, Y, test_size=0.2, random_state=100, stratify=Y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "y_train_onehot, y_test_onehot, y_val_onehot, output_shape = one_hot(y_train,y_test,y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h22oEyQKjnJN",
        "outputId": "62fd2793-3d5e-471d-a566-58894a076a78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# best model so far\n",
        "input_shape = X_train.shape[1:]\n",
        "init_neurons = X_train.shape[1]\n",
        "custom_optimizer = Adam(learning_rate=0.00005)\n",
        "\n",
        "# Define the model\n",
        "model_c3_lsb = tf.keras.models.Sequential([\n",
        "    Dense(init_neurons, activation='relu', input_shape=input_shape),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(output_shape, activation='softmax')\n",
        "])\n",
        "\n",
        "model_c3_lsb.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqgEoBNuBqRU",
        "outputId": "18c48be8-dc76-43fa-d706-8b550609a677"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "261/261 [==============================] - 3s 6ms/step - loss: 1.0696 - accuracy: 0.4259 - val_loss: 1.0229 - val_accuracy: 0.5138 - lr: 5.0000e-05\n",
            "Epoch 2/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.9677 - accuracy: 0.5590 - val_loss: 0.9196 - val_accuracy: 0.5778 - lr: 5.0000e-05\n",
            "Epoch 3/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.8777 - accuracy: 0.5901 - val_loss: 0.8566 - val_accuracy: 0.5980 - lr: 5.0000e-05\n",
            "Epoch 4/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.8326 - accuracy: 0.6049 - val_loss: 0.8293 - val_accuracy: 0.6052 - lr: 5.0000e-05\n",
            "Epoch 5/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.8108 - accuracy: 0.6133 - val_loss: 0.8150 - val_accuracy: 0.6142 - lr: 5.0000e-05\n",
            "Epoch 6/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7977 - accuracy: 0.6161 - val_loss: 0.8065 - val_accuracy: 0.6206 - lr: 5.0000e-05\n",
            "Epoch 7/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7885 - accuracy: 0.6226 - val_loss: 0.8000 - val_accuracy: 0.6217 - lr: 5.0000e-05\n",
            "Epoch 8/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7813 - accuracy: 0.6265 - val_loss: 0.7955 - val_accuracy: 0.6289 - lr: 5.0000e-05\n",
            "Epoch 9/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7752 - accuracy: 0.6305 - val_loss: 0.7923 - val_accuracy: 0.6303 - lr: 5.0000e-05\n",
            "Epoch 10/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7701 - accuracy: 0.6344 - val_loss: 0.7896 - val_accuracy: 0.6307 - lr: 5.0000e-05\n",
            "Epoch 11/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7658 - accuracy: 0.6383 - val_loss: 0.7865 - val_accuracy: 0.6325 - lr: 5.0000e-05\n",
            "Epoch 12/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7615 - accuracy: 0.6381 - val_loss: 0.7857 - val_accuracy: 0.6321 - lr: 5.0000e-05\n",
            "Epoch 13/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7580 - accuracy: 0.6432 - val_loss: 0.7829 - val_accuracy: 0.6336 - lr: 5.0000e-05\n",
            "Epoch 14/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7551 - accuracy: 0.6455 - val_loss: 0.7814 - val_accuracy: 0.6354 - lr: 5.0000e-05\n",
            "Epoch 15/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7513 - accuracy: 0.6449 - val_loss: 0.7801 - val_accuracy: 0.6336 - lr: 5.0000e-05\n",
            "Epoch 16/50\n",
            "261/261 [==============================] - 1s 5ms/step - loss: 0.7489 - accuracy: 0.6488 - val_loss: 0.7791 - val_accuracy: 0.6354 - lr: 5.0000e-05\n",
            "Epoch 17/50\n",
            "261/261 [==============================] - 1s 5ms/step - loss: 0.7461 - accuracy: 0.6492 - val_loss: 0.7771 - val_accuracy: 0.6386 - lr: 5.0000e-05\n",
            "Epoch 18/50\n",
            "261/261 [==============================] - 1s 5ms/step - loss: 0.7437 - accuracy: 0.6468 - val_loss: 0.7760 - val_accuracy: 0.6390 - lr: 5.0000e-05\n",
            "Epoch 19/50\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.7414 - accuracy: 0.6504 - val_loss: 0.7748 - val_accuracy: 0.6397 - lr: 5.0000e-05\n",
            "Epoch 20/50\n",
            " 74/261 [=======>......................] - ETA: 0s - loss: 0.7327 - accuracy: 0.6626"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7390 - accuracy: 0.6510 - val_loss: 0.7741 - val_accuracy: 0.6408 - lr: 5.0000e-05\n",
            "Epoch 21/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7370 - accuracy: 0.6501 - val_loss: 0.7749 - val_accuracy: 0.6401 - lr: 5.0000e-05\n",
            "Epoch 22/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7347 - accuracy: 0.6502 - val_loss: 0.7730 - val_accuracy: 0.6411 - lr: 5.0000e-05\n",
            "Epoch 23/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7327 - accuracy: 0.6544 - val_loss: 0.7726 - val_accuracy: 0.6411 - lr: 5.0000e-05\n",
            "Epoch 24/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7310 - accuracy: 0.6548 - val_loss: 0.7724 - val_accuracy: 0.6437 - lr: 5.0000e-05\n",
            "Epoch 25/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7289 - accuracy: 0.6568 - val_loss: 0.7721 - val_accuracy: 0.6411 - lr: 5.0000e-05\n",
            "Epoch 26/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7271 - accuracy: 0.6594 - val_loss: 0.7723 - val_accuracy: 0.6411 - lr: 5.0000e-05\n",
            "Epoch 27/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7253 - accuracy: 0.6577 - val_loss: 0.7718 - val_accuracy: 0.6408 - lr: 5.0000e-05\n",
            "Epoch 28/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7238 - accuracy: 0.6606 - val_loss: 0.7712 - val_accuracy: 0.6422 - lr: 5.0000e-05\n",
            "Epoch 29/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7221 - accuracy: 0.6604 - val_loss: 0.7711 - val_accuracy: 0.6390 - lr: 5.0000e-05\n",
            "Epoch 30/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7204 - accuracy: 0.6639 - val_loss: 0.7709 - val_accuracy: 0.6433 - lr: 5.0000e-05\n",
            "Epoch 31/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7191 - accuracy: 0.6632 - val_loss: 0.7706 - val_accuracy: 0.6415 - lr: 5.0000e-05\n",
            "Epoch 32/50\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.7174 - accuracy: 0.6628 - val_loss: 0.7708 - val_accuracy: 0.6440 - lr: 5.0000e-05\n",
            "Epoch 33/50\n",
            "259/261 [============================>.] - ETA: 0s - loss: 0.7151 - accuracy: 0.6640\n",
            "Epoch 33: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.7159 - accuracy: 0.6637 - val_loss: 0.7707 - val_accuracy: 0.6393 - lr: 5.0000e-05\n",
            "Epoch 34/50\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.7130 - accuracy: 0.6689 - val_loss: 0.7702 - val_accuracy: 0.6393 - lr: 5.0000e-06\n",
            "Epoch 35/50\n",
            "261/261 [==============================] - 1s 5ms/step - loss: 0.7125 - accuracy: 0.6682 - val_loss: 0.7701 - val_accuracy: 0.6433 - lr: 5.0000e-06\n",
            "Epoch 36/50\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.7122 - accuracy: 0.6685 - val_loss: 0.7701 - val_accuracy: 0.6444 - lr: 5.0000e-06\n",
            "Epoch 37/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7121 - accuracy: 0.6683 - val_loss: 0.7701 - val_accuracy: 0.6440 - lr: 5.0000e-06\n",
            "Epoch 38/50\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.7119 - accuracy: 0.6686 - val_loss: 0.7700 - val_accuracy: 0.6426 - lr: 5.0000e-06\n",
            "Epoch 39/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7117 - accuracy: 0.6686 - val_loss: 0.7699 - val_accuracy: 0.6447 - lr: 5.0000e-06\n",
            "Epoch 40/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7116 - accuracy: 0.6675 - val_loss: 0.7700 - val_accuracy: 0.6447 - lr: 5.0000e-06\n",
            "Epoch 41/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7114 - accuracy: 0.6679 - val_loss: 0.7699 - val_accuracy: 0.6451 - lr: 5.0000e-06\n",
            "Epoch 42/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7113 - accuracy: 0.6679 - val_loss: 0.7698 - val_accuracy: 0.6429 - lr: 5.0000e-06\n",
            "Epoch 43/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7111 - accuracy: 0.6686 - val_loss: 0.7697 - val_accuracy: 0.6433 - lr: 5.0000e-06\n",
            "Epoch 44/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7110 - accuracy: 0.6699 - val_loss: 0.7697 - val_accuracy: 0.6433 - lr: 5.0000e-06\n",
            "Epoch 45/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7109 - accuracy: 0.6695 - val_loss: 0.7697 - val_accuracy: 0.6440 - lr: 5.0000e-06\n",
            "Epoch 46/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7107 - accuracy: 0.6698 - val_loss: 0.7697 - val_accuracy: 0.6444 - lr: 5.0000e-06\n",
            "Epoch 47/50\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7106 - accuracy: 0.6687 - val_loss: 0.7697 - val_accuracy: 0.6451 - lr: 5.0000e-06\n",
            "Epoch 48/50\n",
            "245/261 [===========================>..] - ETA: 0s - loss: 0.7059 - accuracy: 0.6721\n",
            "Epoch 48: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "261/261 [==============================] - 1s 3ms/step - loss: 0.7105 - accuracy: 0.6685 - val_loss: 0.7697 - val_accuracy: 0.6451 - lr: 5.0000e-06\n",
            "Epoch 49/50\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.7101 - accuracy: 0.6695 - val_loss: 0.7697 - val_accuracy: 0.6451 - lr: 1.0000e-06\n",
            "Epoch 50/50\n",
            "261/261 [==============================] - 1s 4ms/step - loss: 0.7101 - accuracy: 0.6692 - val_loss: 0.7697 - val_accuracy: 0.6451 - lr: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "#history = model.fit(X_train, y_train, epochs=150, validation_data=(X_test, y_test), callbacks=[reduce_lr])\n",
        "history_c3_lsb = model_c3_lsb.fit(X_train, y_train_onehot, epochs=50, validation_data=(X_val, y_val_onehot), callbacks=[lr_schedule, early_stopping, checkpoint])\n",
        "best_val_accuracy_c3_lsb = max(history_c3_lsb.history['val_accuracy'])\n",
        "validation_accuracies['model_c3_lsb'] = best_val_accuracy_c3_lsb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXdGepipPKlu"
      },
      "source": [
        "#### LINE - SCATTER - PIE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRiAZP9TR9MT",
        "outputId": "c8d8cb55-11f3-42f8-ba09-8bb21eb65a31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'line', 1: 'pie', 2: 'scatter'}\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6213 - loss: 0.7567 - val_accuracy: 0.7220 - val_loss: 0.5552 - learning_rate: 5.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7267 - loss: 0.5459 - val_accuracy: 0.7224 - val_loss: 0.5477 - learning_rate: 5.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7333 - loss: 0.5274 - val_accuracy: 0.7246 - val_loss: 0.5404 - learning_rate: 5.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7346 - loss: 0.5245 - val_accuracy: 0.7213 - val_loss: 0.5423 - learning_rate: 5.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7351 - loss: 0.5124 - val_accuracy: 0.7260 - val_loss: 0.5397 - learning_rate: 5.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7483 - loss: 0.5044 - val_accuracy: 0.7224 - val_loss: 0.5461 - learning_rate: 5.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7506 - loss: 0.4961 - val_accuracy: 0.7292 - val_loss: 0.5316 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7517 - loss: 0.4855 - val_accuracy: 0.7274 - val_loss: 0.5384 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7609 - loss: 0.4775 - val_accuracy: 0.7285 - val_loss: 0.5342 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7655 - loss: 0.4707 - val_accuracy: 0.7256 - val_loss: 0.5398 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7696 - loss: 0.4789 - val_accuracy: 0.7282 - val_loss: 0.5341 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7690 - loss: 0.4662 - val_accuracy: 0.7332 - val_loss: 0.5370 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7729 - loss: 0.4593 - val_accuracy: 0.7292 - val_loss: 0.5433 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7738 - loss: 0.4572 - val_accuracy: 0.7346 - val_loss: 0.5453 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7791 - loss: 0.4468 - val_accuracy: 0.7321 - val_loss: 0.5494 - learning_rate: 5.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7808 - loss: 0.4501 - val_accuracy: 0.7368 - val_loss: 0.5419 - learning_rate: 5.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7929 - loss: 0.4371 - val_accuracy: 0.7346 - val_loss: 0.5508 - learning_rate: 5.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7788 - loss: 0.4460 - val_accuracy: 0.7292 - val_loss: 0.5571 - learning_rate: 5.0000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7830 - loss: 0.4427 - val_accuracy: 0.7357 - val_loss: 0.5497 - learning_rate: 5.0000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7869 - loss: 0.4420 - val_accuracy: 0.7267 - val_loss: 0.5561 - learning_rate: 5.0000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7847 - loss: 0.4377 - val_accuracy: 0.7303 - val_loss: 0.5585 - learning_rate: 5.0000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7914 - loss: 0.4288 - val_accuracy: 0.7339 - val_loss: 0.5665 - learning_rate: 5.0000e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7938 - loss: 0.4239 - val_accuracy: 0.7310 - val_loss: 0.5648 - learning_rate: 5.0000e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7984 - loss: 0.4198 - val_accuracy: 0.7325 - val_loss: 0.5651 - learning_rate: 5.0000e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8007 - loss: 0.4152 - val_accuracy: 0.7274 - val_loss: 0.5661 - learning_rate: 5.0000e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7972 - loss: 0.4158 - val_accuracy: 0.7195 - val_loss: 0.5791 - learning_rate: 5.0000e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m250/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8000 - loss: 0.4096\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7998 - loss: 0.4098 - val_accuracy: 0.7274 - val_loss: 0.5749 - learning_rate: 5.0000e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8080 - loss: 0.3951 - val_accuracy: 0.7325 - val_loss: 0.5614 - learning_rate: 5.0000e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8225 - loss: 0.3891 - val_accuracy: 0.7368 - val_loss: 0.5619 - learning_rate: 5.0000e-05\n",
            "Epoch 30/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8205 - loss: 0.3819 - val_accuracy: 0.7353 - val_loss: 0.5630 - learning_rate: 5.0000e-05\n",
            "Epoch 31/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8163 - loss: 0.3860 - val_accuracy: 0.7339 - val_loss: 0.5637 - learning_rate: 5.0000e-05\n",
            "Epoch 32/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8181 - loss: 0.3874 - val_accuracy: 0.7339 - val_loss: 0.5650 - learning_rate: 5.0000e-05\n",
            "Epoch 33/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8232 - loss: 0.3834 - val_accuracy: 0.7339 - val_loss: 0.5655 - learning_rate: 5.0000e-05\n",
            "Epoch 34/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8258 - loss: 0.3800 - val_accuracy: 0.7314 - val_loss: 0.5659 - learning_rate: 5.0000e-05\n",
            "Epoch 35/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8264 - loss: 0.3799 - val_accuracy: 0.7328 - val_loss: 0.5667 - learning_rate: 5.0000e-05\n",
            "Epoch 36/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8230 - loss: 0.3822 - val_accuracy: 0.7325 - val_loss: 0.5674 - learning_rate: 5.0000e-05\n",
            "Epoch 37/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8200 - loss: 0.3786 - val_accuracy: 0.7321 - val_loss: 0.5687 - learning_rate: 5.0000e-05\n",
            "Epoch 38/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8162 - loss: 0.3883 - val_accuracy: 0.7343 - val_loss: 0.5710 - learning_rate: 5.0000e-05\n",
            "Epoch 39/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8272 - loss: 0.3728 - val_accuracy: 0.7321 - val_loss: 0.5698 - learning_rate: 5.0000e-05\n",
            "Epoch 40/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8169 - loss: 0.3884 - val_accuracy: 0.7332 - val_loss: 0.5709 - learning_rate: 5.0000e-05\n",
            "Epoch 41/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8314 - loss: 0.3683 - val_accuracy: 0.7321 - val_loss: 0.5711 - learning_rate: 5.0000e-05\n",
            "Epoch 42/50\n",
            "\u001b[1m250/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.3676\n",
            "Epoch 42: ReduceLROnPlateau reducing learning rate to 5.000000237487257e-06.\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8289 - loss: 0.3681 - val_accuracy: 0.7318 - val_loss: 0.5731 - learning_rate: 5.0000e-05\n",
            "Epoch 43/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8262 - loss: 0.3710 - val_accuracy: 0.7314 - val_loss: 0.5729 - learning_rate: 5.0000e-06\n",
            "Epoch 44/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8270 - loss: 0.3699 - val_accuracy: 0.7318 - val_loss: 0.5728 - learning_rate: 5.0000e-06\n",
            "Epoch 45/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8198 - loss: 0.3776 - val_accuracy: 0.7314 - val_loss: 0.5728 - learning_rate: 5.0000e-06\n",
            "Epoch 46/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8263 - loss: 0.3806 - val_accuracy: 0.7314 - val_loss: 0.5728 - learning_rate: 5.0000e-06\n",
            "Epoch 47/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8223 - loss: 0.3741 - val_accuracy: 0.7310 - val_loss: 0.5729 - learning_rate: 5.0000e-06\n",
            "Epoch 48/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8276 - loss: 0.3733 - val_accuracy: 0.7307 - val_loss: 0.5729 - learning_rate: 5.0000e-06\n",
            "Epoch 49/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8165 - loss: 0.3877 - val_accuracy: 0.7307 - val_loss: 0.5730 - learning_rate: 5.0000e-06\n",
            "Epoch 50/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8231 - loss: 0.3729 - val_accuracy: 0.7303 - val_loss: 0.5730 - learning_rate: 5.0000e-06\n"
          ]
        }
      ],
      "source": [
        "X,Y = preprocessing(df_lsp)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, Y, test_size=0.2, random_state=100, stratify=Y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "y_train_onehot, y_test_onehot, y_val_onehot, output_shape = one_hot(y_train,y_test,y_val)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "init_neurons = X_train.shape[1]\n",
        "custom_optimizer = Adam(learning_rate=0.0005)\n",
        "\n",
        "# Define the model\n",
        "model_c3_lsp = tf.keras.models.Sequential([\n",
        "    Dense(init_neurons, activation='relu', input_shape=input_shape),\n",
        "    Dense(180, activation='relu'),\n",
        "    Dense(output_shape, activation='softmax')\n",
        "])\n",
        "\n",
        "model_c3_lsp.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_c3_lsp = model_c3_lsp.fit(X_train, y_train_onehot, epochs=50, validation_data=(X_val, y_val_onehot), callbacks=[lr_schedule, checkpoint])\n",
        "best_val_accuracy_c3_lsp = max(history_c3_lsp.history['val_accuracy'])\n",
        "validation_accuracies['model_c3_lsp'] = best_val_accuracy_c3_lsp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHELQNvAjKMl"
      },
      "source": [
        "#### SCATTER - BAR - PIE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgtLCqR7gFzH",
        "outputId": "a3834a52-83b9-4e0f-fedb-4f7cf3b47af5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'bar', 1: 'pie', 2: 'scatter'}\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6344 - loss: 0.7874 - val_accuracy: 0.7278 - val_loss: 0.6382 - learning_rate: 5.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7355 - loss: 0.6018 - val_accuracy: 0.7361 - val_loss: 0.6226 - learning_rate: 5.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7434 - loss: 0.5808 - val_accuracy: 0.7364 - val_loss: 0.6116 - learning_rate: 5.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7493 - loss: 0.5713 - val_accuracy: 0.7379 - val_loss: 0.6129 - learning_rate: 5.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7538 - loss: 0.5556 - val_accuracy: 0.7487 - val_loss: 0.6049 - learning_rate: 5.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7603 - loss: 0.5359 - val_accuracy: 0.7573 - val_loss: 0.5981 - learning_rate: 5.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7671 - loss: 0.5357 - val_accuracy: 0.7458 - val_loss: 0.6078 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7586 - loss: 0.5342 - val_accuracy: 0.7501 - val_loss: 0.6035 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7801 - loss: 0.5116 - val_accuracy: 0.7465 - val_loss: 0.5919 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7702 - loss: 0.5130 - val_accuracy: 0.7544 - val_loss: 0.5967 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7860 - loss: 0.4980 - val_accuracy: 0.7504 - val_loss: 0.6051 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7844 - loss: 0.4948 - val_accuracy: 0.7508 - val_loss: 0.6014 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7919 - loss: 0.4858 - val_accuracy: 0.7483 - val_loss: 0.6040 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7851 - loss: 0.4933 - val_accuracy: 0.7537 - val_loss: 0.6015 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7882 - loss: 0.4946 - val_accuracy: 0.7522 - val_loss: 0.5975 - learning_rate: 5.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7926 - loss: 0.4786 - val_accuracy: 0.7443 - val_loss: 0.6098 - learning_rate: 5.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7895 - loss: 0.4752 - val_accuracy: 0.7533 - val_loss: 0.6098 - learning_rate: 5.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7925 - loss: 0.4723 - val_accuracy: 0.7512 - val_loss: 0.6063 - learning_rate: 5.0000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7994 - loss: 0.4571 - val_accuracy: 0.7487 - val_loss: 0.6113 - learning_rate: 5.0000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m229/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7999 - loss: 0.4682\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8003 - loss: 0.4666 - val_accuracy: 0.7530 - val_loss: 0.6137 - learning_rate: 5.0000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8197 - loss: 0.4320 - val_accuracy: 0.7548 - val_loss: 0.6022 - learning_rate: 5.0000e-05\n",
            "Epoch 22/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8152 - loss: 0.4359 - val_accuracy: 0.7573 - val_loss: 0.6013 - learning_rate: 5.0000e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8173 - loss: 0.4268 - val_accuracy: 0.7558 - val_loss: 0.6007 - learning_rate: 5.0000e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8200 - loss: 0.4218 - val_accuracy: 0.7555 - val_loss: 0.6013 - learning_rate: 5.0000e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8167 - loss: 0.4306 - val_accuracy: 0.7555 - val_loss: 0.6031 - learning_rate: 5.0000e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8100 - loss: 0.4429 - val_accuracy: 0.7587 - val_loss: 0.6026 - learning_rate: 5.0000e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8166 - loss: 0.4313 - val_accuracy: 0.7580 - val_loss: 0.6022 - learning_rate: 5.0000e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8238 - loss: 0.4215 - val_accuracy: 0.7584 - val_loss: 0.6035 - learning_rate: 5.0000e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8243 - loss: 0.4161 - val_accuracy: 0.7566 - val_loss: 0.6043 - learning_rate: 5.0000e-05\n",
            "Epoch 30/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8168 - loss: 0.4288 - val_accuracy: 0.7605 - val_loss: 0.6052 - learning_rate: 5.0000e-05\n",
            "Epoch 31/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8231 - loss: 0.4191 - val_accuracy: 0.7591 - val_loss: 0.6057 - learning_rate: 5.0000e-05\n",
            "Epoch 32/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8224 - loss: 0.4224 - val_accuracy: 0.7605 - val_loss: 0.6059 - learning_rate: 5.0000e-05\n",
            "Epoch 33/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8224 - loss: 0.4216 - val_accuracy: 0.7587 - val_loss: 0.6058 - learning_rate: 5.0000e-05\n",
            "Epoch 34/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8153 - loss: 0.4331 - val_accuracy: 0.7616 - val_loss: 0.6068 - learning_rate: 5.0000e-05\n",
            "Epoch 35/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8226 - loss: 0.4138 - val_accuracy: 0.7569 - val_loss: 0.6070 - learning_rate: 5.0000e-05\n",
            "Epoch 36/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8253 - loss: 0.4222 - val_accuracy: 0.7580 - val_loss: 0.6074 - learning_rate: 5.0000e-05\n",
            "Epoch 37/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8208 - loss: 0.4216 - val_accuracy: 0.7569 - val_loss: 0.6121 - learning_rate: 5.0000e-05\n",
            "Epoch 38/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8227 - loss: 0.4182 - val_accuracy: 0.7558 - val_loss: 0.6095 - learning_rate: 5.0000e-05\n",
            "Epoch 39/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8228 - loss: 0.4169 - val_accuracy: 0.7591 - val_loss: 0.6083 - learning_rate: 5.0000e-05\n",
            "Epoch 40/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8208 - loss: 0.4188 - val_accuracy: 0.7594 - val_loss: 0.6099 - learning_rate: 5.0000e-05\n",
            "Epoch 41/50\n",
            "\u001b[1m256/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8206 - loss: 0.4206\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 5.000000237487257e-06.\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8207 - loss: 0.4206 - val_accuracy: 0.7598 - val_loss: 0.6100 - learning_rate: 5.0000e-05\n",
            "Epoch 42/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8299 - loss: 0.4147 - val_accuracy: 0.7594 - val_loss: 0.6099 - learning_rate: 5.0000e-06\n",
            "Epoch 43/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8296 - loss: 0.4087 - val_accuracy: 0.7576 - val_loss: 0.6098 - learning_rate: 5.0000e-06\n",
            "Epoch 44/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8280 - loss: 0.4057 - val_accuracy: 0.7573 - val_loss: 0.6098 - learning_rate: 5.0000e-06\n",
            "Epoch 45/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8289 - loss: 0.4090 - val_accuracy: 0.7587 - val_loss: 0.6098 - learning_rate: 5.0000e-06\n",
            "Epoch 46/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8216 - loss: 0.4211 - val_accuracy: 0.7580 - val_loss: 0.6098 - learning_rate: 5.0000e-06\n",
            "Epoch 47/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8227 - loss: 0.4192 - val_accuracy: 0.7566 - val_loss: 0.6099 - learning_rate: 5.0000e-06\n",
            "Epoch 48/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8202 - loss: 0.4250 - val_accuracy: 0.7555 - val_loss: 0.6098 - learning_rate: 5.0000e-06\n",
            "Epoch 49/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8174 - loss: 0.4190 - val_accuracy: 0.7569 - val_loss: 0.6099 - learning_rate: 5.0000e-06\n",
            "Epoch 50/50\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8217 - loss: 0.4169 - val_accuracy: 0.7562 - val_loss: 0.6098 - learning_rate: 5.0000e-06\n"
          ]
        }
      ],
      "source": [
        "X,Y = preprocessing(df_sbp)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, Y, test_size=0.2, random_state=100, stratify=Y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "y_train_onehot, y_test_onehot, y_val_onehot, output_shape = one_hot(y_train,y_test,y_val)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "init_neurons = X_train.shape[1]\n",
        "custom_optimizer = Adam(learning_rate=0.0005)\n",
        "\n",
        "# Define the model\n",
        "model_c3_sbp = tf.keras.models.Sequential([\n",
        "    Dense(init_neurons, activation='relu', input_shape=input_shape),\n",
        "    Dense(180, activation='relu'),\n",
        "    Dense(output_shape, activation='softmax')\n",
        "])\n",
        "\n",
        "model_c3_sbp.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_c3_sbp = model_c3_sbp.fit(X_train, y_train_onehot, epochs=50, validation_data=(X_val, y_val_onehot), callbacks=[lr_schedule, checkpoint])\n",
        "best_val_accuracy_c3_sbp = max(history_c3_sbp.history['val_accuracy'])\n",
        "validation_accuracies['model_c3_sbp'] = best_val_accuracy_c3_sbp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkB0W1OW8J5E"
      },
      "source": [
        "### C=4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZA8z-Wg8Nns"
      },
      "source": [
        "#### LINE - SCATTER - BAR - PIE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IWN1kKS9D28",
        "outputId": "166b8784-7410-4959-dc80-d8f78635d6b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'bar', 1: 'line', 2: 'pie', 3: 'scatter'}\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5098 - loss: 1.0754 - val_accuracy: 0.5928 - val_loss: 0.8640 - learning_rate: 5.0000e-04\n",
            "Epoch 2/100\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6079 - loss: 0.8415 - val_accuracy: 0.6173 - val_loss: 0.8516 - learning_rate: 5.0000e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m 33/348\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6214 - loss: 0.8168    "
          ]
        }
      ],
      "source": [
        "X,Y = preprocessing(df_lsbp)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, Y, test_size=0.2, random_state=100, stratify=Y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "y_train_onehot, y_test_onehot, y_val_onehot, output_shape = one_hot(y_train,y_test,y_val)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "init_neurons = X_train.shape[1]\n",
        "custom_optimizer = Adam(learning_rate=0.0005)\n",
        "\n",
        "# Define the model\n",
        "model_c4_lsbp = tf.keras.models.Sequential([\n",
        "    Dense(init_neurons, activation='relu', input_shape=input_shape),\n",
        "    Dense(180, activation='relu'),\n",
        "    Dense(output_shape, activation='softmax')\n",
        "])\n",
        "\n",
        "model_c4_lsbp.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_c4_lsbp = model_c4_lsbp.fit(X_train, y_train_onehot, epochs=100, validation_data=(X_val, y_val_onehot), callbacks=[lr_schedule, checkpoint])\n",
        "best_val_accuracy_c4_lsbp = max(history_c4_lsbp.history['val_accuracy'])\n",
        "validation_accuracies['model_c4_lsbp'] = best_val_accuracy_c4_lsbp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mals8BQ---25"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtcszNMyWZCq"
      },
      "outputs": [],
      "source": [
        "model_c2_bp.save('c2_bp.h5')\n",
        "model_c2_lb.save('c2_lb.h5')\n",
        "model_c2_lp.save('c2_lp.h5')\n",
        "model_c2_ls.save('c2_ls.h5')\n",
        "model_c2_sb.save('c2_sb.h5')\n",
        "model_c2_sp.save('c2_sp.h5')\n",
        "\n",
        "model_c3_lbp.save('c3_lbp.h5')\n",
        "model_c3_lsb.save('c3_lsb.h5')\n",
        "model_c3_lsp.save('c3_lsp.h5')\n",
        "model_c3_sbp.save('c3_sbp.h5')\n",
        "\n",
        "model_c4_lsbp.save('c4_lsbp.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOF3ic8kaxi0",
        "outputId": "6a49d740-0dfe-4717-877c-2b9d1cd8148f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "model_c2_lb.save('c2_lb.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxIY_f7N_BWd",
        "outputId": "5bcf5387-7ab3-426e-f54b-9f2c91791477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           Model  Validation Accuracy\n",
            "0    model_c2_bp             0.809601\n",
            "1   model_c3_sbl             0.640058\n",
            "2   model_c3_sbp             0.759079\n",
            "3  model_c4_lsbp             0.630259\n"
          ]
        }
      ],
      "source": [
        "accuracy_df = pd.DataFrame(list(validation_accuracies.items()), columns=['Model', 'Validation Accuracy'])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(accuracy_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48rI3wX0egWU"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxl3Ymu2FaV6",
        "outputId": "cccc4b76-28a8-4a87-e843-d6f9a39c0e18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "87/87 [==============================] - 0s 4ms/step - loss: 0.7512 - accuracy: 0.6516\n",
            "Test Loss: 0.7512138485908508\n",
            "Test Accuracy: 0.6515641808509827\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test_onehot)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "boTbTbCtKbvb",
        "outputId": "7b189738-7f67-4a36-ab59-038c119ceb8f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x600 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Plot accuracy\n",
        "axes[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
        "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "axes[0].set_title('Model Accuracy')\n",
        "axes[0].set_xlabel('Epochs')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Plot accuracy vs validation accuracy\n",
        "axes[1].plot(history.history['accuracy'], history.history['val_accuracy'], color='b')\n",
        "axes[1].set_title('Accuracy vs Validation Accuracy')\n",
        "axes[1].set_xlabel('Accuracy')\n",
        "axes[1].set_ylabel('Validation Accuracy')\n",
        "axes[1].grid(True)\n",
        "\n",
        "# Plot loss\n",
        "axes[2].plot(history.history['loss'], label='Training Loss')\n",
        "axes[2].plot(history.history['val_loss'], label='Validation Loss')\n",
        "axes[2].set_title('Training & Validation Loss')\n",
        "axes[2].set_xlabel('Epochs')\n",
        "axes[2].set_ylabel('Loss')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbqiKn2_HugT",
        "outputId": "cbb445a1-0bfe-408b-ca3b-ab1ba8d2fd1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_82 (Dense)            (None, 94)                8930      \n",
            "                                                                 \n",
            " dense_83 (Dense)            (None, 60)                5700      \n",
            "                                                                 \n",
            " dense_84 (Dense)            (None, 120)               7320      \n",
            "                                                                 \n",
            " dense_85 (Dense)            (None, 3)                 363       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22313 (87.16 KB)\n",
            "Trainable params: 22313 (87.16 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EhtDH7OeWIN"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1myhKY0Bm_Nz",
        "outputId": "10bf3644-a7fe-491f-c888-2560a723a297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "58/58 [==============================] - 0s 2ms/step - loss: 0.4362 - accuracy: 0.7977\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.4361858069896698, 0.7977346181869507]"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loaded_model.evaluate(X_test, y_test_onehot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjgz6tghTdNT",
        "outputId": "4839aeb9-914d-4b4d-8258-a680d4eff39a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "87/87 [==============================] - 0s 3ms/step\n",
            "Predictions for the first 5 entries:\n",
            "[[0.13459568 0.17437282 0.6910316 ]\n",
            " [0.23643495 0.3224262  0.4411387 ]\n",
            " [0.36317217 0.16284217 0.47398567]\n",
            " [0.85395026 0.04604641 0.1000033 ]\n",
            " [0.06831136 0.7595224  0.17216623]]\n"
          ]
        }
      ],
      "source": [
        "# Predictions for the first 5 entries in X_test\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Get the first 5 entries of X_test\n",
        "first_5_features = X_test.iloc[:5]\n",
        "\n",
        "#print(\"Features of the first 5 entries:\")\n",
        "#print(first_5_features)\n",
        "\n",
        "print(\"Predictions for the first 5 entries:\")\n",
        "print(predictions[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9taxuTtUUX-f",
        "outputId": "a8e1f346-ee37-476a-e0f4-69a5ddfd1100"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted labels for the first 5 entries:\n",
            "['scatter', 'scatter', 'scatter', 'bar', 'line']\n"
          ]
        }
      ],
      "source": [
        "# Convert predictions to labels\n",
        "predicted_labels = [np.argmax(prediction) for prediction in predictions[:5]]\n",
        "\n",
        "# Define your label mapping\n",
        "label_mapping = {0: 'bar', 1: 'line', 2: 'scatter'}\n",
        "\n",
        "# Convert labels to their corresponding strings\n",
        "predicted_labels = [label_mapping[label] for label in predicted_labels]\n",
        "\n",
        "print(\"Predicted labels for the first 5 entries:\")\n",
        "print(predicted_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyjS7E8EY9fz",
        "outputId": "30aa380a-67df-4959-8d92-6741a07e99ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of line: 893\n",
            "Number of bar: 1032\n",
            "Number of scatter: 856\n"
          ]
        }
      ],
      "source": [
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Count the occurrences of each class\n",
        "counts = {\n",
        "    'line': np.count_nonzero(predicted_labels == 0),\n",
        "    'bar': np.count_nonzero(predicted_labels == 1),\n",
        "    'scatter': np.count_nonzero(predicted_labels == 2)\n",
        "}\n",
        "\n",
        "# Print the counts\n",
        "print(\"Number of line:\", counts['line'])\n",
        "print(\"Number of bar:\", counts['bar'])\n",
        "print(\"Number of scatter:\", counts['scatter'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryO9gTOJpw-r"
      },
      "source": [
        "### Learning Rate - Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNA-QP_xXIwf"
      },
      "outputs": [],
      "source": [
        "# Define the learning rate schedule\n",
        "lr_schedule = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=20, min_delta=1e-3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Define early stopping criteria\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=100, min_delta=1e-3, verbose=1)\n",
        "\n",
        "# Define the custom optimizer\n",
        "custom_optimizer = Adam(learning_rate=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-EgsoKEXLU9"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    Dense(64, activation='relu', input_shape=input_shape),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5a23Ua3PXPH8",
        "outputId": "44ebc785-168c-468b-ee47-a3697085defb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "348/348 [==============================] - 5s 9ms/step - loss: 1.2727 - accuracy: 0.3084 - val_loss: 1.2371 - val_accuracy: 0.3254 - lr: 1.0000e-05\n",
            "Epoch 2/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 1.2157 - accuracy: 0.3360 - val_loss: 1.1873 - val_accuracy: 0.3567 - lr: 1.0000e-05\n",
            "Epoch 3/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 1.1668 - accuracy: 0.3637 - val_loss: 1.1444 - val_accuracy: 0.3740 - lr: 1.0000e-05\n",
            "Epoch 4/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 1.1245 - accuracy: 0.3892 - val_loss: 1.1071 - val_accuracy: 0.3948 - lr: 1.0000e-05\n",
            "Epoch 5/300\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 1.0878 - accuracy: 0.4160 - val_loss: 1.0746 - val_accuracy: 0.4211 - lr: 1.0000e-05\n",
            "Epoch 6/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 1.0556 - accuracy: 0.4413 - val_loss: 1.0460 - val_accuracy: 0.4506 - lr: 1.0000e-05\n",
            "Epoch 7/300\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 1.0275 - accuracy: 0.4718 - val_loss: 1.0207 - val_accuracy: 0.4840 - lr: 1.0000e-05\n",
            "Epoch 8/300\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 1.0026 - accuracy: 0.5035 - val_loss: 0.9982 - val_accuracy: 0.5088 - lr: 1.0000e-05\n",
            "Epoch 9/300\n",
            "348/348 [==============================] - 3s 10ms/step - loss: 0.9807 - accuracy: 0.5305 - val_loss: 0.9783 - val_accuracy: 0.5268 - lr: 1.0000e-05\n",
            "Epoch 10/300\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.9615 - accuracy: 0.5474 - val_loss: 0.9606 - val_accuracy: 0.5469 - lr: 1.0000e-05\n",
            "Epoch 11/300\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.9446 - accuracy: 0.5633 - val_loss: 0.9450 - val_accuracy: 0.5617 - lr: 1.0000e-05\n",
            "Epoch 12/300\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.9295 - accuracy: 0.5751 - val_loss: 0.9312 - val_accuracy: 0.5671 - lr: 1.0000e-05\n",
            "Epoch 13/300\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.9162 - accuracy: 0.5831 - val_loss: 0.9188 - val_accuracy: 0.5707 - lr: 1.0000e-05\n",
            "Epoch 14/300\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.9043 - accuracy: 0.5922 - val_loss: 0.9077 - val_accuracy: 0.5789 - lr: 1.0000e-05\n",
            "Epoch 15/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.8938 - accuracy: 0.5947 - val_loss: 0.8979 - val_accuracy: 0.5872 - lr: 1.0000e-05\n",
            "Epoch 16/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.8844 - accuracy: 0.5971 - val_loss: 0.8891 - val_accuracy: 0.5894 - lr: 1.0000e-05\n",
            "Epoch 17/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.8761 - accuracy: 0.5984 - val_loss: 0.8812 - val_accuracy: 0.5962 - lr: 1.0000e-05\n",
            "Epoch 18/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.8686 - accuracy: 0.6011 - val_loss: 0.8740 - val_accuracy: 0.6023 - lr: 1.0000e-05\n",
            "Epoch 19/300\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.8619 - accuracy: 0.6043 - val_loss: 0.8675 - val_accuracy: 0.6066 - lr: 1.0000e-05\n",
            "Epoch 20/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.8559 - accuracy: 0.6082 - val_loss: 0.8618 - val_accuracy: 0.6084 - lr: 1.0000e-05\n",
            "Epoch 21/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.8504 - accuracy: 0.6092 - val_loss: 0.8566 - val_accuracy: 0.6099 - lr: 1.0000e-05\n",
            "Epoch 22/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.8455 - accuracy: 0.6106 - val_loss: 0.8518 - val_accuracy: 0.6109 - lr: 1.0000e-05\n",
            "Epoch 23/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.8410 - accuracy: 0.6117 - val_loss: 0.8475 - val_accuracy: 0.6106 - lr: 1.0000e-05\n",
            "Epoch 24/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.8368 - accuracy: 0.6112 - val_loss: 0.8435 - val_accuracy: 0.6131 - lr: 1.0000e-05\n",
            "Epoch 25/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.8330 - accuracy: 0.6134 - val_loss: 0.8399 - val_accuracy: 0.6163 - lr: 1.0000e-05\n",
            "Epoch 26/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.8295 - accuracy: 0.6127 - val_loss: 0.8366 - val_accuracy: 0.6174 - lr: 1.0000e-05\n",
            "Epoch 27/300\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.8263 - accuracy: 0.6138 - val_loss: 0.8335 - val_accuracy: 0.6192 - lr: 1.0000e-05\n",
            "Epoch 28/300\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.8233 - accuracy: 0.6152 - val_loss: 0.8308 - val_accuracy: 0.6196 - lr: 1.0000e-05\n",
            "Epoch 29/300\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.8205 - accuracy: 0.6155 - val_loss: 0.8281 - val_accuracy: 0.6192 - lr: 1.0000e-05\n",
            "Epoch 30/300\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.8179 - accuracy: 0.6178 - val_loss: 0.8257 - val_accuracy: 0.6221 - lr: 1.0000e-05\n",
            "Epoch 31/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.8156 - accuracy: 0.6175 - val_loss: 0.8235 - val_accuracy: 0.6206 - lr: 1.0000e-05\n",
            "Epoch 32/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.8133 - accuracy: 0.6191 - val_loss: 0.8214 - val_accuracy: 0.6203 - lr: 1.0000e-05\n",
            "Epoch 33/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.8112 - accuracy: 0.6190 - val_loss: 0.8195 - val_accuracy: 0.6210 - lr: 1.0000e-05\n",
            "Epoch 34/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.8092 - accuracy: 0.6209 - val_loss: 0.8176 - val_accuracy: 0.6199 - lr: 1.0000e-05\n",
            "Epoch 35/300\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.8074 - accuracy: 0.6206 - val_loss: 0.8159 - val_accuracy: 0.6214 - lr: 1.0000e-05\n",
            "Epoch 36/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.8056 - accuracy: 0.6204 - val_loss: 0.8143 - val_accuracy: 0.6228 - lr: 1.0000e-05\n",
            "Epoch 37/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.8040 - accuracy: 0.6214 - val_loss: 0.8128 - val_accuracy: 0.6228 - lr: 1.0000e-05\n",
            "Epoch 38/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.8024 - accuracy: 0.6222 - val_loss: 0.8114 - val_accuracy: 0.6239 - lr: 1.0000e-05\n",
            "Epoch 39/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.8010 - accuracy: 0.6223 - val_loss: 0.8100 - val_accuracy: 0.6242 - lr: 1.0000e-05\n",
            "Epoch 40/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7996 - accuracy: 0.6233 - val_loss: 0.8088 - val_accuracy: 0.6264 - lr: 1.0000e-05\n",
            "Epoch 41/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7982 - accuracy: 0.6237 - val_loss: 0.8075 - val_accuracy: 0.6260 - lr: 1.0000e-05\n",
            "Epoch 42/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7970 - accuracy: 0.6245 - val_loss: 0.8064 - val_accuracy: 0.6264 - lr: 1.0000e-05\n",
            "Epoch 43/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7958 - accuracy: 0.6249 - val_loss: 0.8053 - val_accuracy: 0.6275 - lr: 1.0000e-05\n",
            "Epoch 44/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7946 - accuracy: 0.6249 - val_loss: 0.8043 - val_accuracy: 0.6271 - lr: 1.0000e-05\n",
            "Epoch 45/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7936 - accuracy: 0.6251 - val_loss: 0.8032 - val_accuracy: 0.6264 - lr: 1.0000e-05\n",
            "Epoch 46/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7925 - accuracy: 0.6264 - val_loss: 0.8023 - val_accuracy: 0.6250 - lr: 1.0000e-05\n",
            "Epoch 47/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7916 - accuracy: 0.6272 - val_loss: 0.8014 - val_accuracy: 0.6257 - lr: 1.0000e-05\n",
            "Epoch 48/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7906 - accuracy: 0.6263 - val_loss: 0.8006 - val_accuracy: 0.6253 - lr: 1.0000e-05\n",
            "Epoch 49/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7897 - accuracy: 0.6274 - val_loss: 0.7998 - val_accuracy: 0.6250 - lr: 1.0000e-05\n",
            "Epoch 50/300\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.7888 - accuracy: 0.6273 - val_loss: 0.7990 - val_accuracy: 0.6253 - lr: 1.0000e-05\n",
            "Epoch 51/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7880 - accuracy: 0.6278 - val_loss: 0.7983 - val_accuracy: 0.6253 - lr: 1.0000e-05\n",
            "Epoch 52/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7872 - accuracy: 0.6271 - val_loss: 0.7976 - val_accuracy: 0.6253 - lr: 1.0000e-05\n",
            "Epoch 53/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7864 - accuracy: 0.6281 - val_loss: 0.7969 - val_accuracy: 0.6257 - lr: 1.0000e-05\n",
            "Epoch 54/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7857 - accuracy: 0.6278 - val_loss: 0.7963 - val_accuracy: 0.6260 - lr: 1.0000e-05\n",
            "Epoch 55/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7850 - accuracy: 0.6283 - val_loss: 0.7956 - val_accuracy: 0.6264 - lr: 1.0000e-05\n",
            "Epoch 56/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7843 - accuracy: 0.6283 - val_loss: 0.7950 - val_accuracy: 0.6253 - lr: 1.0000e-05\n",
            "Epoch 57/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7836 - accuracy: 0.6290 - val_loss: 0.7944 - val_accuracy: 0.6250 - lr: 1.0000e-05\n",
            "Epoch 58/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7829 - accuracy: 0.6299 - val_loss: 0.7939 - val_accuracy: 0.6250 - lr: 1.0000e-05\n",
            "Epoch 59/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7823 - accuracy: 0.6297 - val_loss: 0.7933 - val_accuracy: 0.6253 - lr: 1.0000e-05\n",
            "Epoch 60/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7817 - accuracy: 0.6300 - val_loss: 0.7928 - val_accuracy: 0.6260 - lr: 1.0000e-05\n",
            "Epoch 61/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7811 - accuracy: 0.6301 - val_loss: 0.7923 - val_accuracy: 0.6278 - lr: 1.0000e-05\n",
            "Epoch 62/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7805 - accuracy: 0.6308 - val_loss: 0.7918 - val_accuracy: 0.6278 - lr: 1.0000e-05\n",
            "Epoch 63/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.7792 - accuracy: 0.6313\n",
            "Epoch 63: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7800 - accuracy: 0.6309 - val_loss: 0.7913 - val_accuracy: 0.6271 - lr: 1.0000e-05\n",
            "Epoch 64/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7794 - accuracy: 0.6319 - val_loss: 0.7911 - val_accuracy: 0.6264 - lr: 5.0000e-06\n",
            "Epoch 65/300\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.7792 - accuracy: 0.6320 - val_loss: 0.7909 - val_accuracy: 0.6264 - lr: 5.0000e-06\n",
            "Epoch 66/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7789 - accuracy: 0.6321 - val_loss: 0.7906 - val_accuracy: 0.6260 - lr: 5.0000e-06\n",
            "Epoch 67/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7786 - accuracy: 0.6323 - val_loss: 0.7904 - val_accuracy: 0.6260 - lr: 5.0000e-06\n",
            "Epoch 68/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7784 - accuracy: 0.6320 - val_loss: 0.7902 - val_accuracy: 0.6260 - lr: 5.0000e-06\n",
            "Epoch 69/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7781 - accuracy: 0.6321 - val_loss: 0.7900 - val_accuracy: 0.6257 - lr: 5.0000e-06\n",
            "Epoch 70/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7779 - accuracy: 0.6321 - val_loss: 0.7898 - val_accuracy: 0.6268 - lr: 5.0000e-06\n",
            "Epoch 71/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7776 - accuracy: 0.6320 - val_loss: 0.7896 - val_accuracy: 0.6275 - lr: 5.0000e-06\n",
            "Epoch 72/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7774 - accuracy: 0.6323 - val_loss: 0.7894 - val_accuracy: 0.6275 - lr: 5.0000e-06\n",
            "Epoch 73/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7771 - accuracy: 0.6325 - val_loss: 0.7892 - val_accuracy: 0.6282 - lr: 5.0000e-06\n",
            "Epoch 74/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7769 - accuracy: 0.6329 - val_loss: 0.7890 - val_accuracy: 0.6282 - lr: 5.0000e-06\n",
            "Epoch 75/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7766 - accuracy: 0.6328 - val_loss: 0.7888 - val_accuracy: 0.6286 - lr: 5.0000e-06\n",
            "Epoch 76/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7764 - accuracy: 0.6328 - val_loss: 0.7887 - val_accuracy: 0.6289 - lr: 5.0000e-06\n",
            "Epoch 77/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7762 - accuracy: 0.6331 - val_loss: 0.7884 - val_accuracy: 0.6289 - lr: 5.0000e-06\n",
            "Epoch 78/300\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7760 - accuracy: 0.6333 - val_loss: 0.7883 - val_accuracy: 0.6286 - lr: 5.0000e-06\n",
            "Epoch 79/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7757 - accuracy: 0.6336 - val_loss: 0.7881 - val_accuracy: 0.6286 - lr: 5.0000e-06\n",
            "Epoch 80/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7755 - accuracy: 0.6337 - val_loss: 0.7879 - val_accuracy: 0.6282 - lr: 5.0000e-06\n",
            "Epoch 81/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7753 - accuracy: 0.6339 - val_loss: 0.7877 - val_accuracy: 0.6282 - lr: 5.0000e-06\n",
            "Epoch 82/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7751 - accuracy: 0.6344 - val_loss: 0.7875 - val_accuracy: 0.6282 - lr: 5.0000e-06\n",
            "Epoch 83/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7748 - accuracy: 0.6347 - val_loss: 0.7874 - val_accuracy: 0.6282 - lr: 5.0000e-06\n",
            "Epoch 84/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7746 - accuracy: 0.6348 - val_loss: 0.7872 - val_accuracy: 0.6282 - lr: 5.0000e-06\n",
            "Epoch 85/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7744 - accuracy: 0.6347 - val_loss: 0.7870 - val_accuracy: 0.6286 - lr: 5.0000e-06\n",
            "Epoch 86/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7742 - accuracy: 0.6345 - val_loss: 0.7869 - val_accuracy: 0.6286 - lr: 5.0000e-06\n",
            "Epoch 87/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7740 - accuracy: 0.6345 - val_loss: 0.7867 - val_accuracy: 0.6296 - lr: 5.0000e-06\n",
            "Epoch 88/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7738 - accuracy: 0.6344 - val_loss: 0.7865 - val_accuracy: 0.6293 - lr: 5.0000e-06\n",
            "Epoch 89/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7736 - accuracy: 0.6351 - val_loss: 0.7864 - val_accuracy: 0.6300 - lr: 5.0000e-06\n",
            "Epoch 90/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7734 - accuracy: 0.6341 - val_loss: 0.7862 - val_accuracy: 0.6296 - lr: 5.0000e-06\n",
            "Epoch 91/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7732 - accuracy: 0.6345 - val_loss: 0.7861 - val_accuracy: 0.6300 - lr: 5.0000e-06\n",
            "Epoch 92/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7730 - accuracy: 0.6343 - val_loss: 0.7859 - val_accuracy: 0.6300 - lr: 5.0000e-06\n",
            "Epoch 93/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7728 - accuracy: 0.6346 - val_loss: 0.7858 - val_accuracy: 0.6303 - lr: 5.0000e-06\n",
            "Epoch 94/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7726 - accuracy: 0.6347 - val_loss: 0.7856 - val_accuracy: 0.6303 - lr: 5.0000e-06\n",
            "Epoch 95/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7725 - accuracy: 0.6345 - val_loss: 0.7855 - val_accuracy: 0.6300 - lr: 5.0000e-06\n",
            "Epoch 96/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7723 - accuracy: 0.6347 - val_loss: 0.7853 - val_accuracy: 0.6303 - lr: 5.0000e-06\n",
            "Epoch 97/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7721 - accuracy: 0.6347 - val_loss: 0.7851 - val_accuracy: 0.6303 - lr: 5.0000e-06\n",
            "Epoch 98/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7719 - accuracy: 0.6345 - val_loss: 0.7850 - val_accuracy: 0.6303 - lr: 5.0000e-06\n",
            "Epoch 99/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7717 - accuracy: 0.6346 - val_loss: 0.7848 - val_accuracy: 0.6303 - lr: 5.0000e-06\n",
            "Epoch 100/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7715 - accuracy: 0.6341 - val_loss: 0.7847 - val_accuracy: 0.6307 - lr: 5.0000e-06\n",
            "Epoch 101/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7713 - accuracy: 0.6342 - val_loss: 0.7846 - val_accuracy: 0.6311 - lr: 5.0000e-06\n",
            "Epoch 102/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7711 - accuracy: 0.6344 - val_loss: 0.7844 - val_accuracy: 0.6307 - lr: 5.0000e-06\n",
            "Epoch 103/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7710 - accuracy: 0.6350 - val_loss: 0.7843 - val_accuracy: 0.6300 - lr: 5.0000e-06\n",
            "Epoch 104/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7708 - accuracy: 0.6343 - val_loss: 0.7841 - val_accuracy: 0.6307 - lr: 5.0000e-06\n",
            "Epoch 105/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7706 - accuracy: 0.6346 - val_loss: 0.7840 - val_accuracy: 0.6307 - lr: 5.0000e-06\n",
            "Epoch 106/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7704 - accuracy: 0.6348 - val_loss: 0.7839 - val_accuracy: 0.6314 - lr: 5.0000e-06\n",
            "Epoch 107/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7703 - accuracy: 0.6342 - val_loss: 0.7838 - val_accuracy: 0.6307 - lr: 5.0000e-06\n",
            "Epoch 108/300\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.7701 - accuracy: 0.6349 - val_loss: 0.7836 - val_accuracy: 0.6321 - lr: 5.0000e-06\n",
            "Epoch 109/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7700 - accuracy: 0.6348 - val_loss: 0.7835 - val_accuracy: 0.6321 - lr: 5.0000e-06\n",
            "Epoch 110/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7698 - accuracy: 0.6348 - val_loss: 0.7833 - val_accuracy: 0.6321 - lr: 5.0000e-06\n",
            "Epoch 111/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7696 - accuracy: 0.6357 - val_loss: 0.7832 - val_accuracy: 0.6314 - lr: 5.0000e-06\n",
            "Epoch 112/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7694 - accuracy: 0.6353 - val_loss: 0.7831 - val_accuracy: 0.6321 - lr: 5.0000e-06\n",
            "Epoch 113/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7693 - accuracy: 0.6360 - val_loss: 0.7830 - val_accuracy: 0.6329 - lr: 5.0000e-06\n",
            "Epoch 114/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7691 - accuracy: 0.6360 - val_loss: 0.7828 - val_accuracy: 0.6325 - lr: 5.0000e-06\n",
            "Epoch 115/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7690 - accuracy: 0.6355 - val_loss: 0.7827 - val_accuracy: 0.6332 - lr: 5.0000e-06\n",
            "Epoch 116/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7688 - accuracy: 0.6356 - val_loss: 0.7826 - val_accuracy: 0.6325 - lr: 5.0000e-06\n",
            "Epoch 117/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7686 - accuracy: 0.6362 - val_loss: 0.7824 - val_accuracy: 0.6339 - lr: 5.0000e-06\n",
            "Epoch 118/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7685 - accuracy: 0.6361 - val_loss: 0.7823 - val_accuracy: 0.6336 - lr: 5.0000e-06\n",
            "Epoch 119/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7683 - accuracy: 0.6360 - val_loss: 0.7822 - val_accuracy: 0.6332 - lr: 5.0000e-06\n",
            "Epoch 120/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7682 - accuracy: 0.6361 - val_loss: 0.7821 - val_accuracy: 0.6336 - lr: 5.0000e-06\n",
            "Epoch 121/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7680 - accuracy: 0.6365 - val_loss: 0.7819 - val_accuracy: 0.6332 - lr: 5.0000e-06\n",
            "Epoch 122/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7679 - accuracy: 0.6359 - val_loss: 0.7818 - val_accuracy: 0.6336 - lr: 5.0000e-06\n",
            "Epoch 123/300\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.7677 - accuracy: 0.6370 - val_loss: 0.7817 - val_accuracy: 0.6336 - lr: 5.0000e-06\n",
            "Epoch 124/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7676 - accuracy: 0.6357 - val_loss: 0.7816 - val_accuracy: 0.6336 - lr: 5.0000e-06\n",
            "Epoch 125/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7674 - accuracy: 0.6360 - val_loss: 0.7815 - val_accuracy: 0.6336 - lr: 5.0000e-06\n",
            "Epoch 126/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7673 - accuracy: 0.6363 - val_loss: 0.7814 - val_accuracy: 0.6332 - lr: 5.0000e-06\n",
            "Epoch 127/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7671 - accuracy: 0.6364 - val_loss: 0.7812 - val_accuracy: 0.6332 - lr: 5.0000e-06\n",
            "Epoch 128/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7670 - accuracy: 0.6362 - val_loss: 0.7811 - val_accuracy: 0.6332 - lr: 5.0000e-06\n",
            "Epoch 129/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7668 - accuracy: 0.6365 - val_loss: 0.7810 - val_accuracy: 0.6332 - lr: 5.0000e-06\n",
            "Epoch 130/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7667 - accuracy: 0.6371 - val_loss: 0.7809 - val_accuracy: 0.6339 - lr: 5.0000e-06\n",
            "Epoch 131/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7665 - accuracy: 0.6371 - val_loss: 0.7808 - val_accuracy: 0.6336 - lr: 5.0000e-06\n",
            "Epoch 132/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7664 - accuracy: 0.6370 - val_loss: 0.7807 - val_accuracy: 0.6336 - lr: 5.0000e-06\n",
            "Epoch 133/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7663 - accuracy: 0.6373 - val_loss: 0.7806 - val_accuracy: 0.6339 - lr: 5.0000e-06\n",
            "Epoch 134/300\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7661 - accuracy: 0.6369 - val_loss: 0.7805 - val_accuracy: 0.6343 - lr: 5.0000e-06\n",
            "Epoch 135/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7660 - accuracy: 0.6372 - val_loss: 0.7803 - val_accuracy: 0.6343 - lr: 5.0000e-06\n",
            "Epoch 136/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7658 - accuracy: 0.6373 - val_loss: 0.7802 - val_accuracy: 0.6336 - lr: 5.0000e-06\n",
            "Epoch 137/300\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.7657 - accuracy: 0.6371 - val_loss: 0.7801 - val_accuracy: 0.6343 - lr: 5.0000e-06\n",
            "Epoch 138/300\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7656 - accuracy: 0.6375 - val_loss: 0.7800 - val_accuracy: 0.6343 - lr: 5.0000e-06\n",
            "Epoch 139/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7654 - accuracy: 0.6377 - val_loss: 0.7799 - val_accuracy: 0.6339 - lr: 5.0000e-06\n",
            "Epoch 140/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7653 - accuracy: 0.6369 - val_loss: 0.7798 - val_accuracy: 0.6336 - lr: 5.0000e-06\n",
            "Epoch 141/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7652 - accuracy: 0.6374 - val_loss: 0.7797 - val_accuracy: 0.6343 - lr: 5.0000e-06\n",
            "Epoch 142/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7650 - accuracy: 0.6380 - val_loss: 0.7796 - val_accuracy: 0.6339 - lr: 5.0000e-06\n",
            "Epoch 143/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7649 - accuracy: 0.6373 - val_loss: 0.7795 - val_accuracy: 0.6336 - lr: 5.0000e-06\n",
            "Epoch 144/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7648 - accuracy: 0.6377 - val_loss: 0.7794 - val_accuracy: 0.6339 - lr: 5.0000e-06\n",
            "Epoch 145/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7646 - accuracy: 0.6375 - val_loss: 0.7793 - val_accuracy: 0.6336 - lr: 5.0000e-06\n",
            "Epoch 146/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7645 - accuracy: 0.6381 - val_loss: 0.7792 - val_accuracy: 0.6343 - lr: 5.0000e-06\n",
            "Epoch 147/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7644 - accuracy: 0.6377 - val_loss: 0.7791 - val_accuracy: 0.6339 - lr: 5.0000e-06\n",
            "Epoch 148/300\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7642 - accuracy: 0.6377 - val_loss: 0.7790 - val_accuracy: 0.6343 - lr: 5.0000e-06\n",
            "Epoch 149/300\n",
            "118/348 [=========>....................] - ETA: 0s - loss: 0.7690 - accuracy: 0.6390"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-e570cadc9603>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(X_train, y_train_onehot,\n\u001b[0m\u001b[1;32m      3\u001b[0m                     \u001b[0;31m#batch_size=16,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0;31m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36munpack_inputs\u001b[0;34m(self, bound_parameters)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m       flat.extend(\n\u001b[0;32m--> 391\u001b[0;31m           \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_constraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m       )\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36mto_tensors\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     nest.map_structure(\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_component_specs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mwrong\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mare\u001b[0m \u001b[0mprovided\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m   \"\"\"\n\u001b[0;32m--> 631\u001b[0;31m   return nest_util.map_structure(\n\u001b[0m\u001b[1;32m    632\u001b[0m       \u001b[0mnest_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1064\u001b[0m   \"\"\"\n\u001b[1;32m   1065\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_core_map_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_data_map_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m   return _tf_core_pack_sequence_as(\n\u001b[1;32m   1105\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1104\u001b[0m   return _tf_core_pack_sequence_as(\n\u001b[1;32m   1105\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(spec, v)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     nest.map_structure(\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mlambda\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_component_specs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         self._to_components(value))\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36mto_tensors\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mto_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInternalCastContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_subtype_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m       raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/trace_type/trace_type_builder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, allow_specs)\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0;34m\"\"\"Default casting behaviors.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_specs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allow_specs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallow_specs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Fit the model\n",
        "history = model.fit(X_train, y_train_onehot,\n",
        "                    #batch_size=16,\n",
        "                    epochs=300,\n",
        "                    validation_data=(X_test, y_test_onehot),\n",
        "                    callbacks=[lr_schedule, early_stopping],\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "u-ryJoIsLJEH",
        "outputId": "9a19ce16-2eb2-41c2-f672-88cf1e6d8daf"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x600 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Plot accuracy\n",
        "axes[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
        "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "axes[0].set_title('Model Accuracy')\n",
        "axes[0].set_xlabel('Epochs')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Plot accuracy vs validation accuracy\n",
        "axes[1].plot(history.history['accuracy'], history.history['val_accuracy'], color='b')\n",
        "axes[1].set_title('Accuracy vs Validation Accuracy')\n",
        "axes[1].set_xlabel('Accuracy')\n",
        "axes[1].set_ylabel('Validation Accuracy')\n",
        "axes[1].grid(True)\n",
        "\n",
        "# Plot loss\n",
        "axes[2].plot(history.history['loss'], label='Training Loss')\n",
        "axes[2].plot(history.history['val_loss'], label='Validation Loss')\n",
        "axes[2].set_title('Training & Validation Loss')\n",
        "axes[2].set_xlabel('Epochs')\n",
        "axes[2].set_ylabel('Loss')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdQBlm4S_a7U"
      },
      "outputs": [],
      "source": [
        "input_shape = X_train.shape[1:]\n",
        "custom_optimizer = Adam(learning_rate=0.00001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd_wntLK_TfT"
      },
      "source": [
        "trial 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPnzaeKI_UQs",
        "outputId": "c35c18b8-985d-456b-9e3d-5cdde91d61c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.8600 - accuracy: 0.5847 - val_loss: 0.7983 - val_accuracy: 0.6214\n",
            "Epoch 2/10\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7864 - accuracy: 0.6220 - val_loss: 0.7945 - val_accuracy: 0.6260\n",
            "Epoch 3/10\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7743 - accuracy: 0.6294 - val_loss: 0.7932 - val_accuracy: 0.6134\n",
            "Epoch 4/10\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7677 - accuracy: 0.6328 - val_loss: 0.7881 - val_accuracy: 0.6188\n",
            "Epoch 5/10\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7615 - accuracy: 0.6387 - val_loss: 0.7843 - val_accuracy: 0.6296\n",
            "Epoch 6/10\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7575 - accuracy: 0.6402 - val_loss: 0.7878 - val_accuracy: 0.6232\n",
            "Epoch 7/10\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7516 - accuracy: 0.6415 - val_loss: 0.7880 - val_accuracy: 0.6339\n",
            "Epoch 8/10\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7478 - accuracy: 0.6424 - val_loss: 0.7866 - val_accuracy: 0.6224\n",
            "Epoch 9/10\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7448 - accuracy: 0.6447 - val_loss: 0.7857 - val_accuracy: 0.6289\n",
            "Epoch 10/10\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7403 - accuracy: 0.6485 - val_loss: 0.7856 - val_accuracy: 0.6232\n"
          ]
        }
      ],
      "source": [
        "# Define the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    Dense(58, activation='relu', input_shape=input_shape),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train_onehot, epochs=10, validation_data=(X_test, y_test_onehot))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKUJWMnG_k4C"
      },
      "source": [
        "trial 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymIyOush_m-Q",
        "outputId": "04da2911-d66b-498e-843b-5c8fca432b91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 1: Number of layers = 1, Number of neurons = 30\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 2s 3ms/step - loss: 0.8204 - accuracy: 0.6083 - val_loss: 0.7755 - val_accuracy: 0.6293\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7625 - accuracy: 0.6376 - val_loss: 0.7686 - val_accuracy: 0.6368\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7488 - accuracy: 0.6462 - val_loss: 0.7584 - val_accuracy: 0.6429\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7366 - accuracy: 0.6521 - val_loss: 0.7590 - val_accuracy: 0.6419\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7282 - accuracy: 0.6573 - val_loss: 0.7582 - val_accuracy: 0.6476\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7202 - accuracy: 0.6593 - val_loss: 0.7558 - val_accuracy: 0.6480\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7160 - accuracy: 0.6612 - val_loss: 0.7560 - val_accuracy: 0.6271\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7086 - accuracy: 0.6650 - val_loss: 0.7639 - val_accuracy: 0.6404\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.7030 - accuracy: 0.6670 - val_loss: 0.7572 - val_accuracy: 0.6408\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6965 - accuracy: 0.6720 - val_loss: 0.7656 - val_accuracy: 0.6361\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6926 - accuracy: 0.6742 - val_loss: 0.7543 - val_accuracy: 0.6365\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6891 - accuracy: 0.6733 - val_loss: 0.7547 - val_accuracy: 0.6422\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6820 - accuracy: 0.6821 - val_loss: 0.7605 - val_accuracy: 0.6296\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6779 - accuracy: 0.6832 - val_loss: 0.7628 - val_accuracy: 0.6451\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.6749 - accuracy: 0.6841 - val_loss: 0.7638 - val_accuracy: 0.6455\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6703 - accuracy: 0.6887 - val_loss: 0.7644 - val_accuracy: 0.6368\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6663 - accuracy: 0.6893 - val_loss: 0.7634 - val_accuracy: 0.6411\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6933 - val_loss: 0.7763 - val_accuracy: 0.6350\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6544 - accuracy: 0.6960 - val_loss: 0.7674 - val_accuracy: 0.6408\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6506 - accuracy: 0.6954 - val_loss: 0.7768 - val_accuracy: 0.6336\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.7011 - val_loss: 0.7753 - val_accuracy: 0.6422\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.7019 - val_loss: 0.7857 - val_accuracy: 0.6422\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6409 - accuracy: 0.7023 - val_loss: 0.7893 - val_accuracy: 0.6365\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6377 - accuracy: 0.7053 - val_loss: 0.7755 - val_accuracy: 0.6451\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6359 - accuracy: 0.7054 - val_loss: 0.7842 - val_accuracy: 0.6397\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6295 - accuracy: 0.7090 - val_loss: 0.7915 - val_accuracy: 0.6498\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.7114 - val_loss: 0.7968 - val_accuracy: 0.6300\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.7134 - val_loss: 0.7914 - val_accuracy: 0.6339\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6213 - accuracy: 0.7098 - val_loss: 0.7950 - val_accuracy: 0.6397\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6179 - accuracy: 0.7195 - val_loss: 0.8050 - val_accuracy: 0.6343\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6150 - accuracy: 0.7158 - val_loss: 0.8256 - val_accuracy: 0.6278\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6112 - accuracy: 0.7181 - val_loss: 0.8028 - val_accuracy: 0.6368\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6106 - accuracy: 0.7158 - val_loss: 0.7994 - val_accuracy: 0.6339\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6074 - accuracy: 0.7184 - val_loss: 0.8058 - val_accuracy: 0.6242\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6017 - accuracy: 0.7229 - val_loss: 0.8221 - val_accuracy: 0.6275\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5989 - accuracy: 0.7242 - val_loss: 0.8352 - val_accuracy: 0.6275\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5997 - accuracy: 0.7233 - val_loss: 0.8261 - val_accuracy: 0.6368\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5942 - accuracy: 0.7255 - val_loss: 0.8317 - val_accuracy: 0.6307\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5915 - accuracy: 0.7264 - val_loss: 0.8301 - val_accuracy: 0.6271\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5906 - accuracy: 0.7314 - val_loss: 0.8301 - val_accuracy: 0.6282\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5862 - accuracy: 0.7324 - val_loss: 0.8463 - val_accuracy: 0.6228\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5823 - accuracy: 0.7334 - val_loss: 0.8248 - val_accuracy: 0.6393\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5808 - accuracy: 0.7313 - val_loss: 0.8476 - val_accuracy: 0.6293\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5793 - accuracy: 0.7355 - val_loss: 0.8354 - val_accuracy: 0.6386\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5762 - accuracy: 0.7373 - val_loss: 0.8569 - val_accuracy: 0.6383\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5752 - accuracy: 0.7351 - val_loss: 0.8426 - val_accuracy: 0.6275\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5723 - accuracy: 0.7384 - val_loss: 0.8509 - val_accuracy: 0.6250\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5690 - accuracy: 0.7371 - val_loss: 0.8492 - val_accuracy: 0.6372\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5673 - accuracy: 0.7408 - val_loss: 0.8638 - val_accuracy: 0.6242\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5638 - accuracy: 0.7431 - val_loss: 0.8578 - val_accuracy: 0.6268\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5660 - accuracy: 0.7438 - val_loss: 0.8755 - val_accuracy: 0.6386\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5622 - accuracy: 0.7453 - val_loss: 0.8736 - val_accuracy: 0.6210\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5599 - accuracy: 0.7441 - val_loss: 0.8890 - val_accuracy: 0.6167\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5554 - accuracy: 0.7505 - val_loss: 0.8628 - val_accuracy: 0.6271\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5548 - accuracy: 0.7459 - val_loss: 0.8788 - val_accuracy: 0.6350\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5522 - accuracy: 0.7461 - val_loss: 0.8860 - val_accuracy: 0.6361\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5513 - accuracy: 0.7484 - val_loss: 0.8891 - val_accuracy: 0.6296\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5478 - accuracy: 0.7464 - val_loss: 0.8914 - val_accuracy: 0.6325\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5487 - accuracy: 0.7494 - val_loss: 0.9058 - val_accuracy: 0.6296\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5474 - accuracy: 0.7487 - val_loss: 0.8968 - val_accuracy: 0.6321\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5433 - accuracy: 0.7506 - val_loss: 0.9125 - val_accuracy: 0.6214\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5449 - accuracy: 0.7534 - val_loss: 0.9050 - val_accuracy: 0.6228\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7549 - val_loss: 0.9102 - val_accuracy: 0.6268\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5375 - accuracy: 0.7567 - val_loss: 0.9117 - val_accuracy: 0.6246\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5366 - accuracy: 0.7540 - val_loss: 0.9125 - val_accuracy: 0.6232\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5362 - accuracy: 0.7545 - val_loss: 0.9174 - val_accuracy: 0.6242\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5351 - accuracy: 0.7602 - val_loss: 0.9208 - val_accuracy: 0.6210\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5321 - accuracy: 0.7563 - val_loss: 0.9254 - val_accuracy: 0.6235\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5312 - accuracy: 0.7588 - val_loss: 0.9164 - val_accuracy: 0.6289\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5297 - accuracy: 0.7608 - val_loss: 0.9423 - val_accuracy: 0.6282\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5259 - accuracy: 0.7602 - val_loss: 0.9408 - val_accuracy: 0.6264\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5260 - accuracy: 0.7597 - val_loss: 0.9243 - val_accuracy: 0.6253\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5248 - accuracy: 0.7644 - val_loss: 0.9409 - val_accuracy: 0.6210\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5213 - accuracy: 0.7618 - val_loss: 0.9473 - val_accuracy: 0.6246\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5216 - accuracy: 0.7650 - val_loss: 0.9639 - val_accuracy: 0.6199\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5197 - accuracy: 0.7611 - val_loss: 0.9906 - val_accuracy: 0.6260\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7666 - val_loss: 0.9639 - val_accuracy: 0.6303\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5184 - accuracy: 0.7646 - val_loss: 0.9645 - val_accuracy: 0.6163\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.7690 - val_loss: 0.9703 - val_accuracy: 0.6178\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5157 - accuracy: 0.7665 - val_loss: 0.9839 - val_accuracy: 0.6196\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5153 - accuracy: 0.7618 - val_loss: 0.9858 - val_accuracy: 0.6127\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5162 - accuracy: 0.7673 - val_loss: 0.9736 - val_accuracy: 0.6228\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5105 - accuracy: 0.7652 - val_loss: 0.9845 - val_accuracy: 0.6127\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7664 - val_loss: 0.9951 - val_accuracy: 0.6214\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5090 - accuracy: 0.7682 - val_loss: 0.9860 - val_accuracy: 0.6160\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5079 - accuracy: 0.7686 - val_loss: 0.9887 - val_accuracy: 0.6185\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5065 - accuracy: 0.7709 - val_loss: 0.9765 - val_accuracy: 0.6203\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5029 - accuracy: 0.7712 - val_loss: 0.9937 - val_accuracy: 0.6300\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5017 - accuracy: 0.7692 - val_loss: 1.0177 - val_accuracy: 0.6134\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5051 - accuracy: 0.7715 - val_loss: 1.0049 - val_accuracy: 0.6401\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5037 - accuracy: 0.7721 - val_loss: 1.0037 - val_accuracy: 0.6221\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4993 - accuracy: 0.7745 - val_loss: 0.9989 - val_accuracy: 0.6257\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4962 - accuracy: 0.7740 - val_loss: 1.0085 - val_accuracy: 0.6196\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4950 - accuracy: 0.7779 - val_loss: 1.0272 - val_accuracy: 0.6109\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4977 - accuracy: 0.7741 - val_loss: 1.0288 - val_accuracy: 0.6163\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4953 - accuracy: 0.7749 - val_loss: 1.0121 - val_accuracy: 0.6156\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4949 - accuracy: 0.7712 - val_loss: 1.0336 - val_accuracy: 0.6192\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4923 - accuracy: 0.7744 - val_loss: 1.0300 - val_accuracy: 0.6138\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4917 - accuracy: 0.7786 - val_loss: 1.0375 - val_accuracy: 0.6091\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4911 - accuracy: 0.7759 - val_loss: 1.0473 - val_accuracy: 0.6124\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4917 - accuracy: 0.7787 - val_loss: 1.0080 - val_accuracy: 0.6318\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4894 - accuracy: 0.7777 - val_loss: 1.0301 - val_accuracy: 0.6224\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4872 - accuracy: 0.7796 - val_loss: 1.0511 - val_accuracy: 0.6210\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4886 - accuracy: 0.7775 - val_loss: 1.0437 - val_accuracy: 0.6181\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4838 - accuracy: 0.7773 - val_loss: 1.0400 - val_accuracy: 0.6242\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4859 - accuracy: 0.7810 - val_loss: 1.0579 - val_accuracy: 0.6124\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4806 - accuracy: 0.7817 - val_loss: 1.0522 - val_accuracy: 0.6271\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4827 - accuracy: 0.7824 - val_loss: 1.0750 - val_accuracy: 0.6081\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4825 - accuracy: 0.7813 - val_loss: 1.0629 - val_accuracy: 0.6232\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4831 - accuracy: 0.7825 - val_loss: 1.0598 - val_accuracy: 0.6253\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4812 - accuracy: 0.7822 - val_loss: 1.0458 - val_accuracy: 0.6152\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4803 - accuracy: 0.7868 - val_loss: 1.0690 - val_accuracy: 0.6134\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4784 - accuracy: 0.7828 - val_loss: 1.0659 - val_accuracy: 0.6124\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4736 - accuracy: 0.7811 - val_loss: 1.0641 - val_accuracy: 0.6264\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4813 - accuracy: 0.7815 - val_loss: 1.0665 - val_accuracy: 0.6167\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4736 - accuracy: 0.7880 - val_loss: 1.0861 - val_accuracy: 0.6174\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4730 - accuracy: 0.7853 - val_loss: 1.0715 - val_accuracy: 0.6124\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4742 - accuracy: 0.7834 - val_loss: 1.0657 - val_accuracy: 0.6117\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4713 - accuracy: 0.7825 - val_loss: 1.0987 - val_accuracy: 0.6188\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4735 - accuracy: 0.7828 - val_loss: 1.0848 - val_accuracy: 0.6160\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4699 - accuracy: 0.7877 - val_loss: 1.1029 - val_accuracy: 0.6163\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4691 - accuracy: 0.7886 - val_loss: 1.0730 - val_accuracy: 0.6235\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4676 - accuracy: 0.7871 - val_loss: 1.1254 - val_accuracy: 0.6167\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4723 - accuracy: 0.7816 - val_loss: 1.0728 - val_accuracy: 0.6286\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4704 - accuracy: 0.7878 - val_loss: 1.0918 - val_accuracy: 0.6286\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4653 - accuracy: 0.7912 - val_loss: 1.1027 - val_accuracy: 0.6174\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4681 - accuracy: 0.7891 - val_loss: 1.0994 - val_accuracy: 0.6210\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4669 - accuracy: 0.7859 - val_loss: 1.1144 - val_accuracy: 0.6142\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4665 - accuracy: 0.7842 - val_loss: 1.1157 - val_accuracy: 0.6242\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4593 - accuracy: 0.7904 - val_loss: 1.1176 - val_accuracy: 0.6142\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4637 - accuracy: 0.7936 - val_loss: 1.1139 - val_accuracy: 0.6052\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4617 - accuracy: 0.7917 - val_loss: 1.1319 - val_accuracy: 0.6152\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4588 - accuracy: 0.7917 - val_loss: 1.1293 - val_accuracy: 0.6149\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4608 - accuracy: 0.7882 - val_loss: 1.1477 - val_accuracy: 0.6095\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4599 - accuracy: 0.7891 - val_loss: 1.1529 - val_accuracy: 0.6149\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4596 - accuracy: 0.7905 - val_loss: 1.1346 - val_accuracy: 0.6138\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4574 - accuracy: 0.7872 - val_loss: 1.1343 - val_accuracy: 0.6138\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4539 - accuracy: 0.7963 - val_loss: 1.1251 - val_accuracy: 0.6163\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4573 - accuracy: 0.7931 - val_loss: 1.1436 - val_accuracy: 0.6228\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4536 - accuracy: 0.7940 - val_loss: 1.1384 - val_accuracy: 0.6250\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4539 - accuracy: 0.7910 - val_loss: 1.1306 - val_accuracy: 0.6120\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4546 - accuracy: 0.7929 - val_loss: 1.1733 - val_accuracy: 0.6239\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4486 - accuracy: 0.7951 - val_loss: 1.1607 - val_accuracy: 0.6242\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4548 - accuracy: 0.7941 - val_loss: 1.1435 - val_accuracy: 0.6239\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4529 - accuracy: 0.7959 - val_loss: 1.1486 - val_accuracy: 0.6199\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4515 - accuracy: 0.7940 - val_loss: 1.1756 - val_accuracy: 0.6134\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4500 - accuracy: 0.7969 - val_loss: 1.1774 - val_accuracy: 0.6084\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4492 - accuracy: 0.7945 - val_loss: 1.1827 - val_accuracy: 0.6160\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4476 - accuracy: 0.7969 - val_loss: 1.1723 - val_accuracy: 0.6041\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4453 - accuracy: 0.7957 - val_loss: 1.1421 - val_accuracy: 0.6156\n",
            "History for model 1: <keras.src.callbacks.History object at 0x7b0941270eb0>\n",
            "Trial 2: Number of layers = 1, Number of neurons = 90\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 3s 5ms/step - loss: 0.8209 - accuracy: 0.6125 - val_loss: 0.7843 - val_accuracy: 0.6268\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.7645 - accuracy: 0.6384 - val_loss: 0.7766 - val_accuracy: 0.6386\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7483 - accuracy: 0.6443 - val_loss: 0.7555 - val_accuracy: 0.6422\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7365 - accuracy: 0.6511 - val_loss: 0.7565 - val_accuracy: 0.6487\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7270 - accuracy: 0.6576 - val_loss: 0.7597 - val_accuracy: 0.6455\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7154 - accuracy: 0.6643 - val_loss: 0.7617 - val_accuracy: 0.6465\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7090 - accuracy: 0.6657 - val_loss: 0.7506 - val_accuracy: 0.6429\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7028 - accuracy: 0.6681 - val_loss: 0.7497 - val_accuracy: 0.6404\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6954 - accuracy: 0.6739 - val_loss: 0.7600 - val_accuracy: 0.6260\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6879 - accuracy: 0.6790 - val_loss: 0.7565 - val_accuracy: 0.6505\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6821 - accuracy: 0.6750 - val_loss: 0.7544 - val_accuracy: 0.6379\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6766 - accuracy: 0.6861 - val_loss: 0.7512 - val_accuracy: 0.6462\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.6708 - accuracy: 0.6877 - val_loss: 0.7680 - val_accuracy: 0.6462\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.6634 - accuracy: 0.6928 - val_loss: 0.7713 - val_accuracy: 0.6458\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6595 - accuracy: 0.6963 - val_loss: 0.7609 - val_accuracy: 0.6530\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6943 - val_loss: 0.7673 - val_accuracy: 0.6487\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6484 - accuracy: 0.6970 - val_loss: 0.7742 - val_accuracy: 0.6465\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6991 - val_loss: 0.7805 - val_accuracy: 0.6350\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6394 - accuracy: 0.7035 - val_loss: 0.7685 - val_accuracy: 0.6314\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6339 - accuracy: 0.7017 - val_loss: 0.7897 - val_accuracy: 0.6386\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.7084 - val_loss: 0.7878 - val_accuracy: 0.6379\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.7163 - val_loss: 0.7859 - val_accuracy: 0.6393\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6193 - accuracy: 0.7138 - val_loss: 0.7796 - val_accuracy: 0.6411\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6146 - accuracy: 0.7190 - val_loss: 0.7755 - val_accuracy: 0.6408\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.6077 - accuracy: 0.7207 - val_loss: 0.7932 - val_accuracy: 0.6437\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6077 - accuracy: 0.7228 - val_loss: 0.8032 - val_accuracy: 0.6419\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6011 - accuracy: 0.7211 - val_loss: 0.7844 - val_accuracy: 0.6476\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5977 - accuracy: 0.7259 - val_loss: 0.7966 - val_accuracy: 0.6357\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5934 - accuracy: 0.7299 - val_loss: 0.8092 - val_accuracy: 0.6282\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5886 - accuracy: 0.7282 - val_loss: 0.8268 - val_accuracy: 0.6163\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5861 - accuracy: 0.7343 - val_loss: 0.8212 - val_accuracy: 0.6268\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5803 - accuracy: 0.7309 - val_loss: 0.8433 - val_accuracy: 0.6390\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5795 - accuracy: 0.7350 - val_loss: 0.8146 - val_accuracy: 0.6264\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5742 - accuracy: 0.7383 - val_loss: 0.8289 - val_accuracy: 0.6415\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5704 - accuracy: 0.7401 - val_loss: 0.8365 - val_accuracy: 0.6354\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5656 - accuracy: 0.7422 - val_loss: 0.8552 - val_accuracy: 0.6192\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5653 - accuracy: 0.7396 - val_loss: 0.8453 - val_accuracy: 0.6408\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5585 - accuracy: 0.7422 - val_loss: 0.8550 - val_accuracy: 0.6332\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5570 - accuracy: 0.7474 - val_loss: 0.8416 - val_accuracy: 0.6321\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5522 - accuracy: 0.7504 - val_loss: 0.8545 - val_accuracy: 0.6250\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5520 - accuracy: 0.7449 - val_loss: 0.8716 - val_accuracy: 0.6357\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5465 - accuracy: 0.7496 - val_loss: 0.8616 - val_accuracy: 0.6343\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5438 - accuracy: 0.7530 - val_loss: 0.8715 - val_accuracy: 0.6214\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5405 - accuracy: 0.7513 - val_loss: 0.8926 - val_accuracy: 0.6271\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5398 - accuracy: 0.7520 - val_loss: 0.8877 - val_accuracy: 0.6275\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5343 - accuracy: 0.7576 - val_loss: 0.9049 - val_accuracy: 0.6224\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5307 - accuracy: 0.7594 - val_loss: 0.9015 - val_accuracy: 0.6196\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5292 - accuracy: 0.7593 - val_loss: 0.8900 - val_accuracy: 0.6365\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5257 - accuracy: 0.7554 - val_loss: 0.8853 - val_accuracy: 0.6311\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5263 - accuracy: 0.7583 - val_loss: 0.8988 - val_accuracy: 0.6318\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5220 - accuracy: 0.7627 - val_loss: 0.9176 - val_accuracy: 0.6300\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5163 - accuracy: 0.7665 - val_loss: 0.9152 - val_accuracy: 0.6339\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5148 - accuracy: 0.7655 - val_loss: 0.9194 - val_accuracy: 0.6199\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5105 - accuracy: 0.7683 - val_loss: 0.9204 - val_accuracy: 0.6228\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5055 - accuracy: 0.7684 - val_loss: 0.9311 - val_accuracy: 0.6286\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5062 - accuracy: 0.7735 - val_loss: 0.9500 - val_accuracy: 0.6264\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5038 - accuracy: 0.7689 - val_loss: 0.9677 - val_accuracy: 0.6268\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5025 - accuracy: 0.7713 - val_loss: 0.9513 - val_accuracy: 0.6239\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5017 - accuracy: 0.7730 - val_loss: 0.9640 - val_accuracy: 0.6167\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4967 - accuracy: 0.7782 - val_loss: 0.9669 - val_accuracy: 0.6242\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4944 - accuracy: 0.7777 - val_loss: 0.9605 - val_accuracy: 0.6278\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4957 - accuracy: 0.7745 - val_loss: 0.9640 - val_accuracy: 0.6293\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4903 - accuracy: 0.7762 - val_loss: 0.9665 - val_accuracy: 0.6203\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4921 - accuracy: 0.7744 - val_loss: 0.9634 - val_accuracy: 0.6275\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4862 - accuracy: 0.7816 - val_loss: 0.9721 - val_accuracy: 0.6138\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4874 - accuracy: 0.7771 - val_loss: 0.9902 - val_accuracy: 0.6181\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4799 - accuracy: 0.7829 - val_loss: 0.9865 - val_accuracy: 0.6268\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4817 - accuracy: 0.7795 - val_loss: 0.9919 - val_accuracy: 0.6300\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4802 - accuracy: 0.7828 - val_loss: 0.9926 - val_accuracy: 0.6232\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4747 - accuracy: 0.7813 - val_loss: 0.9915 - val_accuracy: 0.6217\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4705 - accuracy: 0.7832 - val_loss: 1.0028 - val_accuracy: 0.6221\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4712 - accuracy: 0.7860 - val_loss: 1.0115 - val_accuracy: 0.6113\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4668 - accuracy: 0.7924 - val_loss: 1.0240 - val_accuracy: 0.6163\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4685 - accuracy: 0.7870 - val_loss: 1.0320 - val_accuracy: 0.6289\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4673 - accuracy: 0.7865 - val_loss: 1.0200 - val_accuracy: 0.6286\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4625 - accuracy: 0.7857 - val_loss: 1.0378 - val_accuracy: 0.6120\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4621 - accuracy: 0.7916 - val_loss: 1.0377 - val_accuracy: 0.6160\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4626 - accuracy: 0.7905 - val_loss: 1.0674 - val_accuracy: 0.6081\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4623 - accuracy: 0.7882 - val_loss: 1.0540 - val_accuracy: 0.6070\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4551 - accuracy: 0.7937 - val_loss: 1.0387 - val_accuracy: 0.6253\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4578 - accuracy: 0.7924 - val_loss: 1.0906 - val_accuracy: 0.6117\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4538 - accuracy: 0.7923 - val_loss: 1.0497 - val_accuracy: 0.6253\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4519 - accuracy: 0.7963 - val_loss: 1.0457 - val_accuracy: 0.6152\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4553 - accuracy: 0.7946 - val_loss: 1.0687 - val_accuracy: 0.6170\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4504 - accuracy: 0.7923 - val_loss: 1.0596 - val_accuracy: 0.6235\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4453 - accuracy: 0.7989 - val_loss: 1.0844 - val_accuracy: 0.6134\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4497 - accuracy: 0.7978 - val_loss: 1.0633 - val_accuracy: 0.6052\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4453 - accuracy: 0.7978 - val_loss: 1.0740 - val_accuracy: 0.6199\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4433 - accuracy: 0.8012 - val_loss: 1.0764 - val_accuracy: 0.6228\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4391 - accuracy: 0.8014 - val_loss: 1.1137 - val_accuracy: 0.6152\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4373 - accuracy: 0.8043 - val_loss: 1.1040 - val_accuracy: 0.6156\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4386 - accuracy: 0.7975 - val_loss: 1.0908 - val_accuracy: 0.6246\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4361 - accuracy: 0.7970 - val_loss: 1.0924 - val_accuracy: 0.6163\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4331 - accuracy: 0.8083 - val_loss: 1.1273 - val_accuracy: 0.6239\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4334 - accuracy: 0.7991 - val_loss: 1.1213 - val_accuracy: 0.6156\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4308 - accuracy: 0.8027 - val_loss: 1.1033 - val_accuracy: 0.6192\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4309 - accuracy: 0.8063 - val_loss: 1.1430 - val_accuracy: 0.6181\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4301 - accuracy: 0.8019 - val_loss: 1.1331 - val_accuracy: 0.6217\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4273 - accuracy: 0.8041 - val_loss: 1.1289 - val_accuracy: 0.6170\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4249 - accuracy: 0.8073 - val_loss: 1.1456 - val_accuracy: 0.6188\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4283 - accuracy: 0.8069 - val_loss: 1.1566 - val_accuracy: 0.6156\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4265 - accuracy: 0.8063 - val_loss: 1.1553 - val_accuracy: 0.6145\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4238 - accuracy: 0.8047 - val_loss: 1.1484 - val_accuracy: 0.6170\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4213 - accuracy: 0.8063 - val_loss: 1.1614 - val_accuracy: 0.6145\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4190 - accuracy: 0.8084 - val_loss: 1.1745 - val_accuracy: 0.6217\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4181 - accuracy: 0.8128 - val_loss: 1.1679 - val_accuracy: 0.6102\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4184 - accuracy: 0.8091 - val_loss: 1.1701 - val_accuracy: 0.6077\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4176 - accuracy: 0.8121 - val_loss: 1.1371 - val_accuracy: 0.6246\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4147 - accuracy: 0.8107 - val_loss: 1.1813 - val_accuracy: 0.6232\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4195 - accuracy: 0.8074 - val_loss: 1.1751 - val_accuracy: 0.6260\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4147 - accuracy: 0.8096 - val_loss: 1.1658 - val_accuracy: 0.6174\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4079 - accuracy: 0.8121 - val_loss: 1.2207 - val_accuracy: 0.6019\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4109 - accuracy: 0.8105 - val_loss: 1.1835 - val_accuracy: 0.6142\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4091 - accuracy: 0.8129 - val_loss: 1.1941 - val_accuracy: 0.6170\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4113 - accuracy: 0.8119 - val_loss: 1.2058 - val_accuracy: 0.6048\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4061 - accuracy: 0.8146 - val_loss: 1.2170 - val_accuracy: 0.5969\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4077 - accuracy: 0.8145 - val_loss: 1.1793 - val_accuracy: 0.6109\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4077 - accuracy: 0.8137 - val_loss: 1.2084 - val_accuracy: 0.6242\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4076 - accuracy: 0.8161 - val_loss: 1.2044 - val_accuracy: 0.6235\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3995 - accuracy: 0.8145 - val_loss: 1.2387 - val_accuracy: 0.6023\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4002 - accuracy: 0.8182 - val_loss: 1.2226 - val_accuracy: 0.6152\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3979 - accuracy: 0.8180 - val_loss: 1.2206 - val_accuracy: 0.6214\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3997 - accuracy: 0.8165 - val_loss: 1.2494 - val_accuracy: 0.6152\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3996 - accuracy: 0.8152 - val_loss: 1.2746 - val_accuracy: 0.6109\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3980 - accuracy: 0.8179 - val_loss: 1.2401 - val_accuracy: 0.6242\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3979 - accuracy: 0.8177 - val_loss: 1.2277 - val_accuracy: 0.6059\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3950 - accuracy: 0.8186 - val_loss: 1.2255 - val_accuracy: 0.6142\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3930 - accuracy: 0.8223 - val_loss: 1.2459 - val_accuracy: 0.6170\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3959 - accuracy: 0.8200 - val_loss: 1.2627 - val_accuracy: 0.6088\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3931 - accuracy: 0.8168 - val_loss: 1.2829 - val_accuracy: 0.6134\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3921 - accuracy: 0.8203 - val_loss: 1.2806 - val_accuracy: 0.6113\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3899 - accuracy: 0.8235 - val_loss: 1.2530 - val_accuracy: 0.6203\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3923 - accuracy: 0.8197 - val_loss: 1.2587 - val_accuracy: 0.6206\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3886 - accuracy: 0.8234 - val_loss: 1.2648 - val_accuracy: 0.6160\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3908 - accuracy: 0.8226 - val_loss: 1.2676 - val_accuracy: 0.6210\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3856 - accuracy: 0.8208 - val_loss: 1.2811 - val_accuracy: 0.6041\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3895 - accuracy: 0.8223 - val_loss: 1.2926 - val_accuracy: 0.6109\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3881 - accuracy: 0.8267 - val_loss: 1.2830 - val_accuracy: 0.6156\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3833 - accuracy: 0.8250 - val_loss: 1.3106 - val_accuracy: 0.6134\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3836 - accuracy: 0.8239 - val_loss: 1.3260 - val_accuracy: 0.5998\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3830 - accuracy: 0.8243 - val_loss: 1.3178 - val_accuracy: 0.6163\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3813 - accuracy: 0.8246 - val_loss: 1.2973 - val_accuracy: 0.6138\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3840 - accuracy: 0.8255 - val_loss: 1.3118 - val_accuracy: 0.6037\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3799 - accuracy: 0.8247 - val_loss: 1.3457 - val_accuracy: 0.6037\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3787 - accuracy: 0.8274 - val_loss: 1.3053 - val_accuracy: 0.6149\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3801 - accuracy: 0.8249 - val_loss: 1.3262 - val_accuracy: 0.6091\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3787 - accuracy: 0.8260 - val_loss: 1.3560 - val_accuracy: 0.6063\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3759 - accuracy: 0.8265 - val_loss: 1.3461 - val_accuracy: 0.6045\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3772 - accuracy: 0.8241 - val_loss: 1.3788 - val_accuracy: 0.6152\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3748 - accuracy: 0.8277 - val_loss: 1.3439 - val_accuracy: 0.6113\n",
            "History for model 2: <keras.src.callbacks.History object at 0x7b094c728d00>\n",
            "Trial 3: Number of layers = 1, Number of neurons = 120\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 2s 3ms/step - loss: 0.8233 - accuracy: 0.6018 - val_loss: 0.7744 - val_accuracy: 0.6329\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7625 - accuracy: 0.6391 - val_loss: 0.7758 - val_accuracy: 0.6321\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7469 - accuracy: 0.6464 - val_loss: 0.7618 - val_accuracy: 0.6365\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7353 - accuracy: 0.6550 - val_loss: 0.7671 - val_accuracy: 0.6314\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7252 - accuracy: 0.6590 - val_loss: 0.7599 - val_accuracy: 0.6411\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7164 - accuracy: 0.6643 - val_loss: 0.7543 - val_accuracy: 0.6465\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7083 - accuracy: 0.6663 - val_loss: 0.7583 - val_accuracy: 0.6447\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7009 - accuracy: 0.6724 - val_loss: 0.7492 - val_accuracy: 0.6465\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6914 - accuracy: 0.6741 - val_loss: 0.7566 - val_accuracy: 0.6354\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6860 - accuracy: 0.6755 - val_loss: 0.7563 - val_accuracy: 0.6437\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6802 - accuracy: 0.6829 - val_loss: 0.7566 - val_accuracy: 0.6516\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6740 - accuracy: 0.6815 - val_loss: 0.7634 - val_accuracy: 0.6440\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6663 - accuracy: 0.6876 - val_loss: 0.7663 - val_accuracy: 0.6383\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6584 - accuracy: 0.6978 - val_loss: 0.7682 - val_accuracy: 0.6386\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6519 - accuracy: 0.6982 - val_loss: 0.7785 - val_accuracy: 0.6433\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.6494 - accuracy: 0.6965 - val_loss: 0.7741 - val_accuracy: 0.6354\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6420 - accuracy: 0.7001 - val_loss: 0.7878 - val_accuracy: 0.6296\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.7009 - val_loss: 0.7666 - val_accuracy: 0.6347\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6312 - accuracy: 0.7084 - val_loss: 0.7791 - val_accuracy: 0.6372\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.7134 - val_loss: 0.7843 - val_accuracy: 0.6451\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6221 - accuracy: 0.7129 - val_loss: 0.7872 - val_accuracy: 0.6321\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6142 - accuracy: 0.7165 - val_loss: 0.7878 - val_accuracy: 0.6401\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6103 - accuracy: 0.7163 - val_loss: 0.8193 - val_accuracy: 0.6426\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6053 - accuracy: 0.7200 - val_loss: 0.7965 - val_accuracy: 0.6361\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5997 - accuracy: 0.7213 - val_loss: 0.8003 - val_accuracy: 0.6354\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5963 - accuracy: 0.7245 - val_loss: 0.8122 - val_accuracy: 0.6286\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5940 - accuracy: 0.7277 - val_loss: 0.8141 - val_accuracy: 0.6271\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5863 - accuracy: 0.7305 - val_loss: 0.8286 - val_accuracy: 0.6260\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5841 - accuracy: 0.7310 - val_loss: 0.8297 - val_accuracy: 0.6286\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5775 - accuracy: 0.7381 - val_loss: 0.8371 - val_accuracy: 0.6314\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5769 - accuracy: 0.7351 - val_loss: 0.8209 - val_accuracy: 0.6368\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5702 - accuracy: 0.7357 - val_loss: 0.8376 - val_accuracy: 0.6365\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5645 - accuracy: 0.7393 - val_loss: 0.8360 - val_accuracy: 0.6246\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5639 - accuracy: 0.7426 - val_loss: 0.8518 - val_accuracy: 0.6246\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5588 - accuracy: 0.7440 - val_loss: 0.8750 - val_accuracy: 0.6185\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5559 - accuracy: 0.7442 - val_loss: 0.8649 - val_accuracy: 0.6303\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5504 - accuracy: 0.7487 - val_loss: 0.8838 - val_accuracy: 0.6217\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5510 - accuracy: 0.7479 - val_loss: 0.8669 - val_accuracy: 0.6307\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5452 - accuracy: 0.7506 - val_loss: 0.8645 - val_accuracy: 0.6336\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5438 - accuracy: 0.7487 - val_loss: 0.8800 - val_accuracy: 0.6181\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5382 - accuracy: 0.7504 - val_loss: 0.8995 - val_accuracy: 0.6199\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5371 - accuracy: 0.7548 - val_loss: 0.8986 - val_accuracy: 0.6127\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5320 - accuracy: 0.7563 - val_loss: 0.8990 - val_accuracy: 0.6156\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5279 - accuracy: 0.7576 - val_loss: 0.9055 - val_accuracy: 0.6314\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5242 - accuracy: 0.7606 - val_loss: 0.9054 - val_accuracy: 0.6260\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5236 - accuracy: 0.7632 - val_loss: 0.8942 - val_accuracy: 0.6246\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5195 - accuracy: 0.7630 - val_loss: 0.9119 - val_accuracy: 0.6253\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5162 - accuracy: 0.7602 - val_loss: 0.9316 - val_accuracy: 0.6221\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5144 - accuracy: 0.7657 - val_loss: 0.9288 - val_accuracy: 0.6239\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5123 - accuracy: 0.7672 - val_loss: 0.9350 - val_accuracy: 0.6239\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5078 - accuracy: 0.7686 - val_loss: 0.9473 - val_accuracy: 0.6203\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5047 - accuracy: 0.7668 - val_loss: 0.9418 - val_accuracy: 0.6156\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5010 - accuracy: 0.7715 - val_loss: 0.9401 - val_accuracy: 0.6224\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5011 - accuracy: 0.7727 - val_loss: 0.9482 - val_accuracy: 0.6185\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4960 - accuracy: 0.7749 - val_loss: 0.9846 - val_accuracy: 0.6102\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4922 - accuracy: 0.7754 - val_loss: 1.0000 - val_accuracy: 0.6134\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4947 - accuracy: 0.7733 - val_loss: 0.9789 - val_accuracy: 0.6224\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4851 - accuracy: 0.7760 - val_loss: 0.9858 - val_accuracy: 0.6203\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4879 - accuracy: 0.7729 - val_loss: 1.0001 - val_accuracy: 0.6282\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4859 - accuracy: 0.7780 - val_loss: 0.9726 - val_accuracy: 0.6120\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4784 - accuracy: 0.7824 - val_loss: 0.9967 - val_accuracy: 0.6163\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4795 - accuracy: 0.7817 - val_loss: 0.9866 - val_accuracy: 0.6196\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4751 - accuracy: 0.7845 - val_loss: 0.9864 - val_accuracy: 0.6268\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4730 - accuracy: 0.7849 - val_loss: 1.0338 - val_accuracy: 0.6278\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4722 - accuracy: 0.7832 - val_loss: 1.0165 - val_accuracy: 0.6181\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4720 - accuracy: 0.7859 - val_loss: 1.0378 - val_accuracy: 0.6095\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4674 - accuracy: 0.7862 - val_loss: 1.0192 - val_accuracy: 0.6210\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4617 - accuracy: 0.7869 - val_loss: 1.0281 - val_accuracy: 0.6145\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4612 - accuracy: 0.7860 - val_loss: 1.0475 - val_accuracy: 0.6099\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4608 - accuracy: 0.7921 - val_loss: 1.0619 - val_accuracy: 0.6084\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4618 - accuracy: 0.7905 - val_loss: 1.0641 - val_accuracy: 0.6102\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4562 - accuracy: 0.7910 - val_loss: 1.0708 - val_accuracy: 0.6120\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4528 - accuracy: 0.7916 - val_loss: 1.0721 - val_accuracy: 0.6117\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4540 - accuracy: 0.7894 - val_loss: 1.0832 - val_accuracy: 0.6149\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4508 - accuracy: 0.7902 - val_loss: 1.0840 - val_accuracy: 0.6063\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4495 - accuracy: 0.7936 - val_loss: 1.1123 - val_accuracy: 0.6099\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4462 - accuracy: 0.7947 - val_loss: 1.1012 - val_accuracy: 0.6124\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4441 - accuracy: 0.7955 - val_loss: 1.1001 - val_accuracy: 0.6081\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4443 - accuracy: 0.7972 - val_loss: 1.0986 - val_accuracy: 0.6127\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4435 - accuracy: 0.7930 - val_loss: 1.0933 - val_accuracy: 0.6055\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4365 - accuracy: 0.8020 - val_loss: 1.1019 - val_accuracy: 0.6260\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4363 - accuracy: 0.8022 - val_loss: 1.1291 - val_accuracy: 0.6174\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4371 - accuracy: 0.8003 - val_loss: 1.1447 - val_accuracy: 0.6095\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4319 - accuracy: 0.8020 - val_loss: 1.1684 - val_accuracy: 0.6081\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4393 - accuracy: 0.8029 - val_loss: 1.1366 - val_accuracy: 0.6095\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4322 - accuracy: 0.8039 - val_loss: 1.1624 - val_accuracy: 0.6099\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4290 - accuracy: 0.8060 - val_loss: 1.1242 - val_accuracy: 0.6181\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4268 - accuracy: 0.8054 - val_loss: 1.1650 - val_accuracy: 0.5998\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4257 - accuracy: 0.8033 - val_loss: 1.1787 - val_accuracy: 0.6009\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4252 - accuracy: 0.8054 - val_loss: 1.1694 - val_accuracy: 0.6134\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4251 - accuracy: 0.8038 - val_loss: 1.1879 - val_accuracy: 0.6005\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4209 - accuracy: 0.8079 - val_loss: 1.1660 - val_accuracy: 0.6138\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4208 - accuracy: 0.8064 - val_loss: 1.1775 - val_accuracy: 0.6019\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4158 - accuracy: 0.8091 - val_loss: 1.1883 - val_accuracy: 0.5991\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4194 - accuracy: 0.8050 - val_loss: 1.2074 - val_accuracy: 0.6052\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4155 - accuracy: 0.8066 - val_loss: 1.1914 - val_accuracy: 0.6059\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4134 - accuracy: 0.8120 - val_loss: 1.2106 - val_accuracy: 0.6134\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4151 - accuracy: 0.8099 - val_loss: 1.2169 - val_accuracy: 0.6037\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4133 - accuracy: 0.8092 - val_loss: 1.2225 - val_accuracy: 0.6019\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4082 - accuracy: 0.8148 - val_loss: 1.2491 - val_accuracy: 0.6009\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4136 - accuracy: 0.8128 - val_loss: 1.2181 - val_accuracy: 0.6063\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4084 - accuracy: 0.8143 - val_loss: 1.2554 - val_accuracy: 0.5991\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4039 - accuracy: 0.8123 - val_loss: 1.2355 - val_accuracy: 0.6034\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4014 - accuracy: 0.8120 - val_loss: 1.2334 - val_accuracy: 0.6048\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4042 - accuracy: 0.8140 - val_loss: 1.2373 - val_accuracy: 0.6102\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4051 - accuracy: 0.8107 - val_loss: 1.2536 - val_accuracy: 0.6030\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3946 - accuracy: 0.8189 - val_loss: 1.2232 - val_accuracy: 0.6081\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3970 - accuracy: 0.8174 - val_loss: 1.2741 - val_accuracy: 0.6005\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4006 - accuracy: 0.8174 - val_loss: 1.2837 - val_accuracy: 0.6066\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3950 - accuracy: 0.8177 - val_loss: 1.2744 - val_accuracy: 0.6023\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3965 - accuracy: 0.8208 - val_loss: 1.2837 - val_accuracy: 0.5991\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3926 - accuracy: 0.8204 - val_loss: 1.2998 - val_accuracy: 0.6102\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3945 - accuracy: 0.8191 - val_loss: 1.2950 - val_accuracy: 0.6127\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3922 - accuracy: 0.8204 - val_loss: 1.3038 - val_accuracy: 0.6120\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3891 - accuracy: 0.8233 - val_loss: 1.3177 - val_accuracy: 0.6099\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3901 - accuracy: 0.8226 - val_loss: 1.3261 - val_accuracy: 0.6019\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3901 - accuracy: 0.8164 - val_loss: 1.3247 - val_accuracy: 0.6095\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3860 - accuracy: 0.8219 - val_loss: 1.3060 - val_accuracy: 0.6160\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3823 - accuracy: 0.8266 - val_loss: 1.3248 - val_accuracy: 0.6070\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3873 - accuracy: 0.8232 - val_loss: 1.3298 - val_accuracy: 0.6037\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3853 - accuracy: 0.8172 - val_loss: 1.3373 - val_accuracy: 0.6091\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3835 - accuracy: 0.8236 - val_loss: 1.3410 - val_accuracy: 0.6084\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3854 - accuracy: 0.8243 - val_loss: 1.3642 - val_accuracy: 0.6037\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3791 - accuracy: 0.8238 - val_loss: 1.3012 - val_accuracy: 0.6149\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3836 - accuracy: 0.8224 - val_loss: 1.3492 - val_accuracy: 0.6019\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3765 - accuracy: 0.8268 - val_loss: 1.3368 - val_accuracy: 0.6045\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3754 - accuracy: 0.8249 - val_loss: 1.3324 - val_accuracy: 0.6037\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3790 - accuracy: 0.8246 - val_loss: 1.3994 - val_accuracy: 0.5976\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3712 - accuracy: 0.8275 - val_loss: 1.3969 - val_accuracy: 0.6009\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3768 - accuracy: 0.8264 - val_loss: 1.4029 - val_accuracy: 0.5955\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3729 - accuracy: 0.8291 - val_loss: 1.3787 - val_accuracy: 0.6019\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3716 - accuracy: 0.8291 - val_loss: 1.3764 - val_accuracy: 0.6095\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3731 - accuracy: 0.8261 - val_loss: 1.3796 - val_accuracy: 0.6037\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3662 - accuracy: 0.8283 - val_loss: 1.3739 - val_accuracy: 0.6059\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3694 - accuracy: 0.8285 - val_loss: 1.4021 - val_accuracy: 0.6048\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3651 - accuracy: 0.8260 - val_loss: 1.3965 - val_accuracy: 0.5933\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3692 - accuracy: 0.8309 - val_loss: 1.4295 - val_accuracy: 0.6019\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3680 - accuracy: 0.8321 - val_loss: 1.4107 - val_accuracy: 0.6045\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3643 - accuracy: 0.8324 - val_loss: 1.4323 - val_accuracy: 0.5937\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3624 - accuracy: 0.8317 - val_loss: 1.4026 - val_accuracy: 0.6063\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3625 - accuracy: 0.8286 - val_loss: 1.4437 - val_accuracy: 0.5994\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3578 - accuracy: 0.8360 - val_loss: 1.4507 - val_accuracy: 0.6027\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3573 - accuracy: 0.8341 - val_loss: 1.4767 - val_accuracy: 0.6001\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3614 - accuracy: 0.8326 - val_loss: 1.4203 - val_accuracy: 0.6041\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3610 - accuracy: 0.8346 - val_loss: 1.4509 - val_accuracy: 0.6048\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3556 - accuracy: 0.8363 - val_loss: 1.4721 - val_accuracy: 0.5944\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3624 - accuracy: 0.8339 - val_loss: 1.4532 - val_accuracy: 0.5976\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3525 - accuracy: 0.8340 - val_loss: 1.4900 - val_accuracy: 0.6102\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3576 - accuracy: 0.8294 - val_loss: 1.5195 - val_accuracy: 0.6059\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3553 - accuracy: 0.8333 - val_loss: 1.4668 - val_accuracy: 0.6045\n",
            "History for model 3: <keras.src.callbacks.History object at 0x7b095e282860>\n",
            "Trial 4: Number of layers = 1, Number of neurons = 150\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.8108 - accuracy: 0.6157 - val_loss: 0.7686 - val_accuracy: 0.6311\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7618 - accuracy: 0.6382 - val_loss: 0.7669 - val_accuracy: 0.6404\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7450 - accuracy: 0.6526 - val_loss: 0.7537 - val_accuracy: 0.6480\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7330 - accuracy: 0.6528 - val_loss: 0.7541 - val_accuracy: 0.6508\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7215 - accuracy: 0.6621 - val_loss: 0.7493 - val_accuracy: 0.6530\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7126 - accuracy: 0.6669 - val_loss: 0.7544 - val_accuracy: 0.6379\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7019 - accuracy: 0.6701 - val_loss: 0.7437 - val_accuracy: 0.6494\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6945 - accuracy: 0.6732 - val_loss: 0.7557 - val_accuracy: 0.6483\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6847 - accuracy: 0.6775 - val_loss: 0.7525 - val_accuracy: 0.6325\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.6797 - accuracy: 0.6846 - val_loss: 0.7621 - val_accuracy: 0.6429\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6695 - accuracy: 0.6893 - val_loss: 0.7594 - val_accuracy: 0.6458\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6644 - accuracy: 0.6918 - val_loss: 0.7666 - val_accuracy: 0.6293\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6552 - accuracy: 0.6980 - val_loss: 0.7661 - val_accuracy: 0.6426\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6476 - accuracy: 0.7036 - val_loss: 0.7710 - val_accuracy: 0.6408\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6455 - accuracy: 0.7042 - val_loss: 0.7834 - val_accuracy: 0.6250\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.7038 - val_loss: 0.7771 - val_accuracy: 0.6462\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.7063 - val_loss: 0.7747 - val_accuracy: 0.6379\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.7123 - val_loss: 0.7726 - val_accuracy: 0.6350\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6194 - accuracy: 0.7159 - val_loss: 0.7948 - val_accuracy: 0.6426\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6110 - accuracy: 0.7214 - val_loss: 0.7870 - val_accuracy: 0.6307\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6048 - accuracy: 0.7215 - val_loss: 0.8119 - val_accuracy: 0.6397\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5995 - accuracy: 0.7250 - val_loss: 0.7984 - val_accuracy: 0.6480\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5962 - accuracy: 0.7282 - val_loss: 0.8240 - val_accuracy: 0.6332\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5887 - accuracy: 0.7304 - val_loss: 0.8361 - val_accuracy: 0.6235\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5870 - accuracy: 0.7325 - val_loss: 0.8244 - val_accuracy: 0.6411\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5796 - accuracy: 0.7349 - val_loss: 0.8398 - val_accuracy: 0.6365\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5769 - accuracy: 0.7396 - val_loss: 0.8275 - val_accuracy: 0.6350\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5696 - accuracy: 0.7420 - val_loss: 0.8365 - val_accuracy: 0.6379\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5677 - accuracy: 0.7414 - val_loss: 0.8586 - val_accuracy: 0.6332\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5616 - accuracy: 0.7467 - val_loss: 0.8511 - val_accuracy: 0.6282\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5550 - accuracy: 0.7458 - val_loss: 0.8513 - val_accuracy: 0.6314\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5487 - accuracy: 0.7540 - val_loss: 0.8761 - val_accuracy: 0.6278\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5474 - accuracy: 0.7515 - val_loss: 0.8801 - val_accuracy: 0.6350\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5442 - accuracy: 0.7511 - val_loss: 0.8889 - val_accuracy: 0.6314\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5400 - accuracy: 0.7545 - val_loss: 0.8974 - val_accuracy: 0.6325\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5344 - accuracy: 0.7542 - val_loss: 0.8980 - val_accuracy: 0.6300\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5327 - accuracy: 0.7571 - val_loss: 0.9031 - val_accuracy: 0.6275\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5250 - accuracy: 0.7626 - val_loss: 0.9282 - val_accuracy: 0.6156\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5271 - accuracy: 0.7624 - val_loss: 0.9006 - val_accuracy: 0.6303\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5217 - accuracy: 0.7659 - val_loss: 0.9113 - val_accuracy: 0.6379\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5145 - accuracy: 0.7665 - val_loss: 0.9256 - val_accuracy: 0.6336\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5133 - accuracy: 0.7652 - val_loss: 0.9344 - val_accuracy: 0.6250\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5078 - accuracy: 0.7692 - val_loss: 0.9525 - val_accuracy: 0.6134\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7699 - val_loss: 0.9465 - val_accuracy: 0.6282\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5000 - accuracy: 0.7763 - val_loss: 0.9319 - val_accuracy: 0.6239\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4987 - accuracy: 0.7736 - val_loss: 0.9607 - val_accuracy: 0.6239\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5010 - accuracy: 0.7722 - val_loss: 0.9555 - val_accuracy: 0.6250\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4921 - accuracy: 0.7771 - val_loss: 0.9530 - val_accuracy: 0.6264\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4900 - accuracy: 0.7766 - val_loss: 0.9866 - val_accuracy: 0.6246\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4875 - accuracy: 0.7800 - val_loss: 0.9839 - val_accuracy: 0.6332\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4854 - accuracy: 0.7786 - val_loss: 0.9648 - val_accuracy: 0.6336\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4802 - accuracy: 0.7853 - val_loss: 0.9937 - val_accuracy: 0.6181\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4778 - accuracy: 0.7846 - val_loss: 0.9847 - val_accuracy: 0.6246\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4770 - accuracy: 0.7851 - val_loss: 0.9900 - val_accuracy: 0.6214\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4715 - accuracy: 0.7845 - val_loss: 1.0044 - val_accuracy: 0.6160\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4720 - accuracy: 0.7834 - val_loss: 1.0027 - val_accuracy: 0.6185\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4691 - accuracy: 0.7833 - val_loss: 0.9940 - val_accuracy: 0.6206\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4647 - accuracy: 0.7860 - val_loss: 1.0413 - val_accuracy: 0.6174\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4649 - accuracy: 0.7892 - val_loss: 1.0520 - val_accuracy: 0.6196\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4602 - accuracy: 0.7880 - val_loss: 1.0254 - val_accuracy: 0.6124\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4603 - accuracy: 0.7918 - val_loss: 1.0445 - val_accuracy: 0.6149\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4579 - accuracy: 0.7900 - val_loss: 1.0416 - val_accuracy: 0.6192\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4539 - accuracy: 0.7938 - val_loss: 1.0616 - val_accuracy: 0.6185\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4489 - accuracy: 0.7959 - val_loss: 1.0632 - val_accuracy: 0.6257\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4484 - accuracy: 0.7965 - val_loss: 1.0929 - val_accuracy: 0.6081\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4460 - accuracy: 0.7955 - val_loss: 1.0635 - val_accuracy: 0.6271\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4442 - accuracy: 0.7961 - val_loss: 1.0803 - val_accuracy: 0.6185\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4435 - accuracy: 0.7976 - val_loss: 1.0830 - val_accuracy: 0.6117\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4414 - accuracy: 0.7966 - val_loss: 1.0864 - val_accuracy: 0.6203\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4364 - accuracy: 0.8002 - val_loss: 1.0996 - val_accuracy: 0.6134\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4357 - accuracy: 0.7995 - val_loss: 1.1256 - val_accuracy: 0.6073\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4390 - accuracy: 0.8020 - val_loss: 1.1063 - val_accuracy: 0.6178\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4293 - accuracy: 0.8050 - val_loss: 1.1168 - val_accuracy: 0.6142\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4304 - accuracy: 0.8008 - val_loss: 1.1156 - val_accuracy: 0.6117\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4270 - accuracy: 0.8076 - val_loss: 1.1437 - val_accuracy: 0.6077\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4289 - accuracy: 0.8003 - val_loss: 1.1456 - val_accuracy: 0.6224\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4237 - accuracy: 0.8061 - val_loss: 1.1481 - val_accuracy: 0.6142\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4204 - accuracy: 0.8084 - val_loss: 1.1570 - val_accuracy: 0.6084\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4190 - accuracy: 0.8102 - val_loss: 1.1491 - val_accuracy: 0.6055\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4159 - accuracy: 0.8132 - val_loss: 1.1629 - val_accuracy: 0.6091\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4156 - accuracy: 0.8084 - val_loss: 1.1526 - val_accuracy: 0.6203\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4153 - accuracy: 0.8109 - val_loss: 1.1653 - val_accuracy: 0.6167\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4140 - accuracy: 0.8123 - val_loss: 1.1750 - val_accuracy: 0.6167\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4093 - accuracy: 0.8151 - val_loss: 1.1469 - val_accuracy: 0.6199\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4140 - accuracy: 0.8095 - val_loss: 1.1596 - val_accuracy: 0.6185\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4062 - accuracy: 0.8156 - val_loss: 1.1771 - val_accuracy: 0.6156\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4086 - accuracy: 0.8145 - val_loss: 1.2083 - val_accuracy: 0.6127\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4038 - accuracy: 0.8189 - val_loss: 1.1900 - val_accuracy: 0.6149\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4040 - accuracy: 0.8151 - val_loss: 1.2225 - val_accuracy: 0.6088\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4017 - accuracy: 0.8165 - val_loss: 1.1947 - val_accuracy: 0.6070\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3991 - accuracy: 0.8192 - val_loss: 1.2307 - val_accuracy: 0.6077\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3998 - accuracy: 0.8173 - val_loss: 1.2073 - val_accuracy: 0.6149\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3945 - accuracy: 0.8164 - val_loss: 1.2259 - val_accuracy: 0.6181\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3964 - accuracy: 0.8227 - val_loss: 1.2217 - val_accuracy: 0.6095\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3965 - accuracy: 0.8176 - val_loss: 1.2379 - val_accuracy: 0.6152\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3926 - accuracy: 0.8201 - val_loss: 1.2588 - val_accuracy: 0.6052\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3914 - accuracy: 0.8218 - val_loss: 1.2638 - val_accuracy: 0.6106\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3919 - accuracy: 0.8198 - val_loss: 1.2554 - val_accuracy: 0.6145\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3844 - accuracy: 0.8243 - val_loss: 1.2581 - val_accuracy: 0.6192\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3883 - accuracy: 0.8223 - val_loss: 1.2505 - val_accuracy: 0.6203\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3867 - accuracy: 0.8250 - val_loss: 1.2729 - val_accuracy: 0.6077\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3791 - accuracy: 0.8268 - val_loss: 1.2643 - val_accuracy: 0.6081\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3802 - accuracy: 0.8262 - val_loss: 1.3006 - val_accuracy: 0.6084\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3842 - accuracy: 0.8225 - val_loss: 1.2473 - val_accuracy: 0.6099\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3806 - accuracy: 0.8242 - val_loss: 1.2793 - val_accuracy: 0.6102\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3831 - accuracy: 0.8238 - val_loss: 1.2688 - val_accuracy: 0.6113\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3735 - accuracy: 0.8318 - val_loss: 1.2934 - val_accuracy: 0.6124\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3765 - accuracy: 0.8292 - val_loss: 1.2877 - val_accuracy: 0.6138\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3729 - accuracy: 0.8287 - val_loss: 1.2633 - val_accuracy: 0.6106\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3699 - accuracy: 0.8323 - val_loss: 1.3145 - val_accuracy: 0.6066\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3703 - accuracy: 0.8299 - val_loss: 1.3303 - val_accuracy: 0.6066\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3704 - accuracy: 0.8292 - val_loss: 1.3155 - val_accuracy: 0.6134\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3706 - accuracy: 0.8323 - val_loss: 1.3269 - val_accuracy: 0.6106\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3687 - accuracy: 0.8314 - val_loss: 1.3195 - val_accuracy: 0.6160\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3680 - accuracy: 0.8305 - val_loss: 1.3427 - val_accuracy: 0.6070\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3682 - accuracy: 0.8318 - val_loss: 1.3721 - val_accuracy: 0.6027\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3630 - accuracy: 0.8332 - val_loss: 1.3268 - val_accuracy: 0.6145\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3608 - accuracy: 0.8355 - val_loss: 1.3515 - val_accuracy: 0.6009\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3686 - accuracy: 0.8327 - val_loss: 1.3688 - val_accuracy: 0.6063\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3642 - accuracy: 0.8333 - val_loss: 1.3665 - val_accuracy: 0.6170\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3567 - accuracy: 0.8351 - val_loss: 1.3516 - val_accuracy: 0.6084\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3582 - accuracy: 0.8353 - val_loss: 1.3607 - val_accuracy: 0.6099\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3556 - accuracy: 0.8346 - val_loss: 1.3894 - val_accuracy: 0.6070\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3564 - accuracy: 0.8363 - val_loss: 1.3945 - val_accuracy: 0.6016\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3607 - accuracy: 0.8327 - val_loss: 1.3859 - val_accuracy: 0.6045\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3614 - accuracy: 0.8348 - val_loss: 1.3757 - val_accuracy: 0.6088\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3493 - accuracy: 0.8383 - val_loss: 1.3543 - val_accuracy: 0.6131\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3510 - accuracy: 0.8417 - val_loss: 1.4147 - val_accuracy: 0.5991\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3527 - accuracy: 0.8395 - val_loss: 1.4080 - val_accuracy: 0.6145\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3485 - accuracy: 0.8390 - val_loss: 1.3995 - val_accuracy: 0.6048\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3485 - accuracy: 0.8392 - val_loss: 1.4008 - val_accuracy: 0.6034\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3500 - accuracy: 0.8382 - val_loss: 1.4082 - val_accuracy: 0.6178\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3465 - accuracy: 0.8413 - val_loss: 1.4718 - val_accuracy: 0.6005\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3466 - accuracy: 0.8412 - val_loss: 1.4528 - val_accuracy: 0.6131\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3480 - accuracy: 0.8391 - val_loss: 1.4635 - val_accuracy: 0.6005\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3446 - accuracy: 0.8444 - val_loss: 1.4158 - val_accuracy: 0.6052\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3453 - accuracy: 0.8393 - val_loss: 1.4732 - val_accuracy: 0.5976\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3428 - accuracy: 0.8424 - val_loss: 1.4701 - val_accuracy: 0.5933\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3417 - accuracy: 0.8403 - val_loss: 1.4247 - val_accuracy: 0.6016\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3386 - accuracy: 0.8446 - val_loss: 1.4578 - val_accuracy: 0.6059\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3401 - accuracy: 0.8454 - val_loss: 1.4235 - val_accuracy: 0.6052\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3373 - accuracy: 0.8448 - val_loss: 1.4433 - val_accuracy: 0.6063\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3346 - accuracy: 0.8448 - val_loss: 1.4881 - val_accuracy: 0.5894\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3383 - accuracy: 0.8438 - val_loss: 1.4711 - val_accuracy: 0.6102\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3390 - accuracy: 0.8424 - val_loss: 1.5059 - val_accuracy: 0.5933\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3355 - accuracy: 0.8440 - val_loss: 1.4158 - val_accuracy: 0.6023\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3388 - accuracy: 0.8449 - val_loss: 1.4904 - val_accuracy: 0.6063\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3333 - accuracy: 0.8481 - val_loss: 1.4702 - val_accuracy: 0.6091\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3301 - accuracy: 0.8473 - val_loss: 1.5031 - val_accuracy: 0.6030\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3314 - accuracy: 0.8478 - val_loss: 1.4913 - val_accuracy: 0.6045\n",
            "History for model 4: <keras.src.callbacks.History object at 0x7b094d41c850>\n",
            "Trial 5: Number of layers = 1, Number of neurons = 180\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.8115 - accuracy: 0.6157 - val_loss: 0.7863 - val_accuracy: 0.6246\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7614 - accuracy: 0.6394 - val_loss: 0.7782 - val_accuracy: 0.6343\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7459 - accuracy: 0.6433 - val_loss: 0.7535 - val_accuracy: 0.6390\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7323 - accuracy: 0.6544 - val_loss: 0.7561 - val_accuracy: 0.6480\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7216 - accuracy: 0.6608 - val_loss: 0.7586 - val_accuracy: 0.6415\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7131 - accuracy: 0.6648 - val_loss: 0.7551 - val_accuracy: 0.6462\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7048 - accuracy: 0.6686 - val_loss: 0.7555 - val_accuracy: 0.6383\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6948 - accuracy: 0.6724 - val_loss: 0.7576 - val_accuracy: 0.6419\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6892 - accuracy: 0.6757 - val_loss: 0.7561 - val_accuracy: 0.6429\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6799 - accuracy: 0.6884 - val_loss: 0.7603 - val_accuracy: 0.6375\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6733 - accuracy: 0.6845 - val_loss: 0.7530 - val_accuracy: 0.6487\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6659 - accuracy: 0.6900 - val_loss: 0.7518 - val_accuracy: 0.6447\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6584 - accuracy: 0.6930 - val_loss: 0.7549 - val_accuracy: 0.6469\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6535 - accuracy: 0.6962 - val_loss: 0.7562 - val_accuracy: 0.6548\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6466 - accuracy: 0.7001 - val_loss: 0.7582 - val_accuracy: 0.6505\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6385 - accuracy: 0.7037 - val_loss: 0.7662 - val_accuracy: 0.6555\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.7087 - val_loss: 0.7698 - val_accuracy: 0.6570\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6255 - accuracy: 0.7061 - val_loss: 0.7856 - val_accuracy: 0.6329\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.7109 - val_loss: 0.7842 - val_accuracy: 0.6483\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6144 - accuracy: 0.7178 - val_loss: 0.7981 - val_accuracy: 0.6426\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6088 - accuracy: 0.7212 - val_loss: 0.7807 - val_accuracy: 0.6462\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.6038 - accuracy: 0.7190 - val_loss: 0.7982 - val_accuracy: 0.6469\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5977 - accuracy: 0.7270 - val_loss: 0.7901 - val_accuracy: 0.6512\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5935 - accuracy: 0.7253 - val_loss: 0.8121 - val_accuracy: 0.6311\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5869 - accuracy: 0.7291 - val_loss: 0.8102 - val_accuracy: 0.6339\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5814 - accuracy: 0.7309 - val_loss: 0.8061 - val_accuracy: 0.6505\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5783 - accuracy: 0.7362 - val_loss: 0.8069 - val_accuracy: 0.6447\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5716 - accuracy: 0.7380 - val_loss: 0.8022 - val_accuracy: 0.6523\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5665 - accuracy: 0.7363 - val_loss: 0.8432 - val_accuracy: 0.6339\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5652 - accuracy: 0.7407 - val_loss: 0.8482 - val_accuracy: 0.6494\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5562 - accuracy: 0.7485 - val_loss: 0.8592 - val_accuracy: 0.6422\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5511 - accuracy: 0.7489 - val_loss: 0.8467 - val_accuracy: 0.6368\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5455 - accuracy: 0.7500 - val_loss: 0.8458 - val_accuracy: 0.6437\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5414 - accuracy: 0.7530 - val_loss: 0.8725 - val_accuracy: 0.6408\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5417 - accuracy: 0.7503 - val_loss: 0.8705 - val_accuracy: 0.6311\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5395 - accuracy: 0.7511 - val_loss: 0.8830 - val_accuracy: 0.6397\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5316 - accuracy: 0.7578 - val_loss: 0.8743 - val_accuracy: 0.6275\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5261 - accuracy: 0.7573 - val_loss: 0.8928 - val_accuracy: 0.6332\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5246 - accuracy: 0.7597 - val_loss: 0.9032 - val_accuracy: 0.6343\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5211 - accuracy: 0.7611 - val_loss: 0.8971 - val_accuracy: 0.6307\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5151 - accuracy: 0.7639 - val_loss: 0.9134 - val_accuracy: 0.6336\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5138 - accuracy: 0.7665 - val_loss: 0.9081 - val_accuracy: 0.6303\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5098 - accuracy: 0.7655 - val_loss: 0.9101 - val_accuracy: 0.6429\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5076 - accuracy: 0.7696 - val_loss: 0.9199 - val_accuracy: 0.6224\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5021 - accuracy: 0.7683 - val_loss: 0.9262 - val_accuracy: 0.6350\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5002 - accuracy: 0.7662 - val_loss: 0.9337 - val_accuracy: 0.6393\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4942 - accuracy: 0.7713 - val_loss: 0.9406 - val_accuracy: 0.6357\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4944 - accuracy: 0.7730 - val_loss: 0.9488 - val_accuracy: 0.6332\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4887 - accuracy: 0.7745 - val_loss: 0.9594 - val_accuracy: 0.6325\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4878 - accuracy: 0.7760 - val_loss: 0.9586 - val_accuracy: 0.6311\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4811 - accuracy: 0.7783 - val_loss: 0.9847 - val_accuracy: 0.6386\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4829 - accuracy: 0.7783 - val_loss: 0.9850 - val_accuracy: 0.6257\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4773 - accuracy: 0.7813 - val_loss: 1.0053 - val_accuracy: 0.6188\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4752 - accuracy: 0.7835 - val_loss: 1.0186 - val_accuracy: 0.6235\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4715 - accuracy: 0.7822 - val_loss: 0.9860 - val_accuracy: 0.6235\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4734 - accuracy: 0.7834 - val_loss: 0.9847 - val_accuracy: 0.6264\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4634 - accuracy: 0.7895 - val_loss: 1.0261 - val_accuracy: 0.6196\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4624 - accuracy: 0.7882 - val_loss: 1.0097 - val_accuracy: 0.6282\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4657 - accuracy: 0.7869 - val_loss: 1.0275 - val_accuracy: 0.6264\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4585 - accuracy: 0.7899 - val_loss: 1.0260 - val_accuracy: 0.6163\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4546 - accuracy: 0.7931 - val_loss: 1.0417 - val_accuracy: 0.6293\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4489 - accuracy: 0.7930 - val_loss: 1.0451 - val_accuracy: 0.6275\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4505 - accuracy: 0.7949 - val_loss: 1.0485 - val_accuracy: 0.6303\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4482 - accuracy: 0.7967 - val_loss: 1.0611 - val_accuracy: 0.6239\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4473 - accuracy: 0.7925 - val_loss: 1.0796 - val_accuracy: 0.6210\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4431 - accuracy: 0.7963 - val_loss: 1.0726 - val_accuracy: 0.6224\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4366 - accuracy: 0.7988 - val_loss: 1.0733 - val_accuracy: 0.6235\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4360 - accuracy: 0.7976 - val_loss: 1.0667 - val_accuracy: 0.6314\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4399 - accuracy: 0.7984 - val_loss: 1.1136 - val_accuracy: 0.6145\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4342 - accuracy: 0.7985 - val_loss: 1.0989 - val_accuracy: 0.6224\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4344 - accuracy: 0.7962 - val_loss: 1.1139 - val_accuracy: 0.6124\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4302 - accuracy: 0.8007 - val_loss: 1.1145 - val_accuracy: 0.6185\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4294 - accuracy: 0.8044 - val_loss: 1.1306 - val_accuracy: 0.6156\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4257 - accuracy: 0.8083 - val_loss: 1.1244 - val_accuracy: 0.6167\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4197 - accuracy: 0.8083 - val_loss: 1.1484 - val_accuracy: 0.6282\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4195 - accuracy: 0.8080 - val_loss: 1.1473 - val_accuracy: 0.6174\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4171 - accuracy: 0.8029 - val_loss: 1.1834 - val_accuracy: 0.5991\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4181 - accuracy: 0.8084 - val_loss: 1.1326 - val_accuracy: 0.6196\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4156 - accuracy: 0.8085 - val_loss: 1.1864 - val_accuracy: 0.6149\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4137 - accuracy: 0.8112 - val_loss: 1.1473 - val_accuracy: 0.6188\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4122 - accuracy: 0.8073 - val_loss: 1.2133 - val_accuracy: 0.6081\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4103 - accuracy: 0.8095 - val_loss: 1.2118 - val_accuracy: 0.6196\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4101 - accuracy: 0.8110 - val_loss: 1.1928 - val_accuracy: 0.6206\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4083 - accuracy: 0.8126 - val_loss: 1.1949 - val_accuracy: 0.6203\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4038 - accuracy: 0.8132 - val_loss: 1.1923 - val_accuracy: 0.6224\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4055 - accuracy: 0.8133 - val_loss: 1.2324 - val_accuracy: 0.6019\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3982 - accuracy: 0.8162 - val_loss: 1.1995 - val_accuracy: 0.6170\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3999 - accuracy: 0.8146 - val_loss: 1.2038 - val_accuracy: 0.6160\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3984 - accuracy: 0.8140 - val_loss: 1.2227 - val_accuracy: 0.6124\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3938 - accuracy: 0.8175 - val_loss: 1.2304 - val_accuracy: 0.6124\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3927 - accuracy: 0.8154 - val_loss: 1.2566 - val_accuracy: 0.6084\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3947 - accuracy: 0.8204 - val_loss: 1.2413 - val_accuracy: 0.6192\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3898 - accuracy: 0.8235 - val_loss: 1.2513 - val_accuracy: 0.6117\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3936 - accuracy: 0.8224 - val_loss: 1.2863 - val_accuracy: 0.6019\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3874 - accuracy: 0.8227 - val_loss: 1.2778 - val_accuracy: 0.6030\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3868 - accuracy: 0.8192 - val_loss: 1.2810 - val_accuracy: 0.6041\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3895 - accuracy: 0.8204 - val_loss: 1.2784 - val_accuracy: 0.6239\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3860 - accuracy: 0.8220 - val_loss: 1.2873 - val_accuracy: 0.6102\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3842 - accuracy: 0.8203 - val_loss: 1.3358 - val_accuracy: 0.6102\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3843 - accuracy: 0.8224 - val_loss: 1.2806 - val_accuracy: 0.6120\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3778 - accuracy: 0.8227 - val_loss: 1.3312 - val_accuracy: 0.6073\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3733 - accuracy: 0.8267 - val_loss: 1.3033 - val_accuracy: 0.6145\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3771 - accuracy: 0.8232 - val_loss: 1.3134 - val_accuracy: 0.6185\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3756 - accuracy: 0.8248 - val_loss: 1.3377 - val_accuracy: 0.6091\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3755 - accuracy: 0.8252 - val_loss: 1.3256 - val_accuracy: 0.6019\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3738 - accuracy: 0.8259 - val_loss: 1.3942 - val_accuracy: 0.6023\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3759 - accuracy: 0.8234 - val_loss: 1.3401 - val_accuracy: 0.6117\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3698 - accuracy: 0.8302 - val_loss: 1.3747 - val_accuracy: 0.6091\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3714 - accuracy: 0.8270 - val_loss: 1.3927 - val_accuracy: 0.5948\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3682 - accuracy: 0.8295 - val_loss: 1.3790 - val_accuracy: 0.6203\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3666 - accuracy: 0.8323 - val_loss: 1.3619 - val_accuracy: 0.6070\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3649 - accuracy: 0.8314 - val_loss: 1.3862 - val_accuracy: 0.6063\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3657 - accuracy: 0.8286 - val_loss: 1.3650 - val_accuracy: 0.6048\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3652 - accuracy: 0.8305 - val_loss: 1.3843 - val_accuracy: 0.6030\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3616 - accuracy: 0.8314 - val_loss: 1.4286 - val_accuracy: 0.6016\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3613 - accuracy: 0.8314 - val_loss: 1.4066 - val_accuracy: 0.6124\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3624 - accuracy: 0.8289 - val_loss: 1.3971 - val_accuracy: 0.6019\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3588 - accuracy: 0.8350 - val_loss: 1.4090 - val_accuracy: 0.6005\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3551 - accuracy: 0.8361 - val_loss: 1.3877 - val_accuracy: 0.6019\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3588 - accuracy: 0.8318 - val_loss: 1.4419 - val_accuracy: 0.6102\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3565 - accuracy: 0.8361 - val_loss: 1.4210 - val_accuracy: 0.5994\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3524 - accuracy: 0.8363 - val_loss: 1.4002 - val_accuracy: 0.6124\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3515 - accuracy: 0.8352 - val_loss: 1.4363 - val_accuracy: 0.6073\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3539 - accuracy: 0.8351 - val_loss: 1.4634 - val_accuracy: 0.6034\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3541 - accuracy: 0.8368 - val_loss: 1.4704 - val_accuracy: 0.6063\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3487 - accuracy: 0.8359 - val_loss: 1.4789 - val_accuracy: 0.6048\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3512 - accuracy: 0.8379 - val_loss: 1.4835 - val_accuracy: 0.6059\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3476 - accuracy: 0.8394 - val_loss: 1.4512 - val_accuracy: 0.6052\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3459 - accuracy: 0.8394 - val_loss: 1.4811 - val_accuracy: 0.6034\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3443 - accuracy: 0.8385 - val_loss: 1.4843 - val_accuracy: 0.6117\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3467 - accuracy: 0.8404 - val_loss: 1.4714 - val_accuracy: 0.6059\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3470 - accuracy: 0.8412 - val_loss: 1.4691 - val_accuracy: 0.6203\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3476 - accuracy: 0.8379 - val_loss: 1.4962 - val_accuracy: 0.6091\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3390 - accuracy: 0.8422 - val_loss: 1.4974 - val_accuracy: 0.6106\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3360 - accuracy: 0.8420 - val_loss: 1.5223 - val_accuracy: 0.6160\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3395 - accuracy: 0.8420 - val_loss: 1.5282 - val_accuracy: 0.6009\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3393 - accuracy: 0.8414 - val_loss: 1.5043 - val_accuracy: 0.6127\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3433 - accuracy: 0.8410 - val_loss: 1.5430 - val_accuracy: 0.6109\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3376 - accuracy: 0.8413 - val_loss: 1.5337 - val_accuracy: 0.6055\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3362 - accuracy: 0.8448 - val_loss: 1.5368 - val_accuracy: 0.6027\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3320 - accuracy: 0.8474 - val_loss: 1.5118 - val_accuracy: 0.6196\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3352 - accuracy: 0.8427 - val_loss: 1.5127 - val_accuracy: 0.6066\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3331 - accuracy: 0.8421 - val_loss: 1.5825 - val_accuracy: 0.6059\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3346 - accuracy: 0.8407 - val_loss: 1.5387 - val_accuracy: 0.6012\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3351 - accuracy: 0.8465 - val_loss: 1.5603 - val_accuracy: 0.5987\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3321 - accuracy: 0.8451 - val_loss: 1.5722 - val_accuracy: 0.6012\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3314 - accuracy: 0.8454 - val_loss: 1.5748 - val_accuracy: 0.6045\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3291 - accuracy: 0.8454 - val_loss: 1.5818 - val_accuracy: 0.5991\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3276 - accuracy: 0.8429 - val_loss: 1.5914 - val_accuracy: 0.5980\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3284 - accuracy: 0.8474 - val_loss: 1.5914 - val_accuracy: 0.5948\n",
            "History for model 5: <keras.src.callbacks.History object at 0x7b094d453f10>\n"
          ]
        }
      ],
      "source": [
        "# Define the models and their configurations\n",
        "init_neurons = X_train.shape[1]\n",
        "model_configs = [\n",
        "    (68, 30),\n",
        "    (68, 90),\n",
        "    (68, 120),\n",
        "    (68, 150),\n",
        "    (68, 180)\n",
        "]\n",
        "\n",
        "# Initialize lists to store models and histories\n",
        "models1 = []\n",
        "histories1 = []\n",
        "\n",
        "# Loop over model configurations\n",
        "for i, config in enumerate(model_configs, start=1):\n",
        "    num_layers = len(config) - 1\n",
        "    neurons = config[1]\n",
        "\n",
        "    # Print the number of layers and neurons\n",
        "    print(f\"Trial {i}: Number of layers = {num_layers}, Number of neurons = {neurons}\")\n",
        "    model = tf.keras.models.Sequential([\n",
        "        Dense(init_neurons, activation='relu', input_shape=input_shape),\n",
        "        Dense(config[1], activation='relu'),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model and store the history\n",
        "    history = model.fit(X_train, y_train_onehot, epochs=150, validation_data=(X_test, y_test_onehot))\n",
        "\n",
        "    # Append the model and its history to the lists\n",
        "    models1.append(model)\n",
        "    histories1.append(history)\n",
        "\n",
        "    # Print the history for reference\n",
        "    print(f\"History for model {i}:\", history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFb4NYiSDSRb",
        "outputId": "b00d24d8-c753-4bbd-ca63-bce2f4705aa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 2: Number of layers = 3, Number of neurons per layer = [30, 40, 50]\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 4s 6ms/step - loss: 0.8238 - accuracy: 0.6072 - val_loss: 0.7783 - val_accuracy: 0.6260\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7651 - accuracy: 0.6393 - val_loss: 0.7709 - val_accuracy: 0.6221\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7468 - accuracy: 0.6452 - val_loss: 0.7604 - val_accuracy: 0.6361\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7402 - accuracy: 0.6495 - val_loss: 0.7495 - val_accuracy: 0.6401\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7282 - accuracy: 0.6561 - val_loss: 0.7606 - val_accuracy: 0.6393\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7206 - accuracy: 0.6591 - val_loss: 0.7504 - val_accuracy: 0.6519\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7147 - accuracy: 0.6627 - val_loss: 0.7503 - val_accuracy: 0.6386\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7054 - accuracy: 0.6679 - val_loss: 0.7537 - val_accuracy: 0.6537\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7001 - accuracy: 0.6712 - val_loss: 0.7526 - val_accuracy: 0.6501\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.6714 - val_loss: 0.7609 - val_accuracy: 0.6483\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6888 - accuracy: 0.6779 - val_loss: 0.7585 - val_accuracy: 0.6483\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6801 - accuracy: 0.6834 - val_loss: 0.7568 - val_accuracy: 0.6361\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6748 - accuracy: 0.6806 - val_loss: 0.7586 - val_accuracy: 0.6512\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6700 - accuracy: 0.6878 - val_loss: 0.7581 - val_accuracy: 0.6365\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.6640 - accuracy: 0.6876 - val_loss: 0.7706 - val_accuracy: 0.6458\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6906 - val_loss: 0.7662 - val_accuracy: 0.6422\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6982 - val_loss: 0.7715 - val_accuracy: 0.6476\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6457 - accuracy: 0.7021 - val_loss: 0.7770 - val_accuracy: 0.6429\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6441 - accuracy: 0.6992 - val_loss: 0.7738 - val_accuracy: 0.6397\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.7009 - val_loss: 0.7774 - val_accuracy: 0.6447\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6361 - accuracy: 0.7036 - val_loss: 0.7904 - val_accuracy: 0.6455\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.7057 - val_loss: 0.8144 - val_accuracy: 0.6372\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.7099 - val_loss: 0.7782 - val_accuracy: 0.6476\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6204 - accuracy: 0.7131 - val_loss: 0.8007 - val_accuracy: 0.6379\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.7098 - val_loss: 0.8129 - val_accuracy: 0.6289\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6127 - accuracy: 0.7121 - val_loss: 0.8022 - val_accuracy: 0.6372\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6059 - accuracy: 0.7210 - val_loss: 0.8111 - val_accuracy: 0.6372\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6068 - accuracy: 0.7161 - val_loss: 0.7964 - val_accuracy: 0.6411\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5999 - accuracy: 0.7193 - val_loss: 0.8231 - val_accuracy: 0.6411\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5947 - accuracy: 0.7238 - val_loss: 0.8172 - val_accuracy: 0.6404\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5902 - accuracy: 0.7251 - val_loss: 0.8368 - val_accuracy: 0.6271\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5841 - accuracy: 0.7224 - val_loss: 0.8459 - val_accuracy: 0.6203\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.5817 - accuracy: 0.7328 - val_loss: 0.8403 - val_accuracy: 0.6293\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5761 - accuracy: 0.7346 - val_loss: 0.8628 - val_accuracy: 0.6390\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5741 - accuracy: 0.7327 - val_loss: 0.8695 - val_accuracy: 0.6401\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5756 - accuracy: 0.7297 - val_loss: 0.8673 - val_accuracy: 0.6372\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5713 - accuracy: 0.7333 - val_loss: 0.8612 - val_accuracy: 0.6379\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5642 - accuracy: 0.7387 - val_loss: 0.8837 - val_accuracy: 0.6361\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5600 - accuracy: 0.7408 - val_loss: 0.8636 - val_accuracy: 0.6354\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5588 - accuracy: 0.7387 - val_loss: 0.8993 - val_accuracy: 0.6357\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5562 - accuracy: 0.7412 - val_loss: 0.9062 - val_accuracy: 0.6325\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5524 - accuracy: 0.7442 - val_loss: 0.8855 - val_accuracy: 0.6354\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5515 - accuracy: 0.7457 - val_loss: 0.8725 - val_accuracy: 0.6221\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5445 - accuracy: 0.7478 - val_loss: 0.8888 - val_accuracy: 0.6192\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5407 - accuracy: 0.7473 - val_loss: 0.8893 - val_accuracy: 0.6296\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5379 - accuracy: 0.7468 - val_loss: 0.9106 - val_accuracy: 0.6271\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5348 - accuracy: 0.7496 - val_loss: 0.8995 - val_accuracy: 0.6332\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5353 - accuracy: 0.7498 - val_loss: 0.9126 - val_accuracy: 0.6321\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5273 - accuracy: 0.7548 - val_loss: 0.9190 - val_accuracy: 0.6206\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5306 - accuracy: 0.7541 - val_loss: 0.8952 - val_accuracy: 0.6224\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5250 - accuracy: 0.7567 - val_loss: 0.9387 - val_accuracy: 0.6152\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5246 - accuracy: 0.7597 - val_loss: 0.9180 - val_accuracy: 0.6228\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5186 - accuracy: 0.7568 - val_loss: 0.9513 - val_accuracy: 0.6318\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5190 - accuracy: 0.7602 - val_loss: 0.9211 - val_accuracy: 0.6332\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5118 - accuracy: 0.7597 - val_loss: 0.9714 - val_accuracy: 0.6278\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5127 - accuracy: 0.7602 - val_loss: 0.9657 - val_accuracy: 0.6275\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5093 - accuracy: 0.7619 - val_loss: 0.9578 - val_accuracy: 0.6260\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5157 - accuracy: 0.7603 - val_loss: 0.9590 - val_accuracy: 0.6286\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5055 - accuracy: 0.7656 - val_loss: 1.0121 - val_accuracy: 0.6239\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5041 - accuracy: 0.7672 - val_loss: 0.9934 - val_accuracy: 0.6257\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4989 - accuracy: 0.7673 - val_loss: 0.9737 - val_accuracy: 0.6293\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4980 - accuracy: 0.7683 - val_loss: 0.9715 - val_accuracy: 0.6203\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4933 - accuracy: 0.7750 - val_loss: 1.0527 - val_accuracy: 0.6375\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4954 - accuracy: 0.7698 - val_loss: 1.0168 - val_accuracy: 0.6232\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4915 - accuracy: 0.7718 - val_loss: 1.0304 - val_accuracy: 0.6224\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4913 - accuracy: 0.7694 - val_loss: 1.0673 - val_accuracy: 0.6124\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4890 - accuracy: 0.7733 - val_loss: 1.0132 - val_accuracy: 0.6239\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4861 - accuracy: 0.7761 - val_loss: 1.0720 - val_accuracy: 0.6214\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4821 - accuracy: 0.7761 - val_loss: 1.0529 - val_accuracy: 0.6253\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4791 - accuracy: 0.7786 - val_loss: 1.0587 - val_accuracy: 0.6278\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4772 - accuracy: 0.7771 - val_loss: 1.0436 - val_accuracy: 0.6257\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4762 - accuracy: 0.7795 - val_loss: 1.0613 - val_accuracy: 0.6300\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4743 - accuracy: 0.7808 - val_loss: 1.0574 - val_accuracy: 0.6332\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4745 - accuracy: 0.7783 - val_loss: 1.0872 - val_accuracy: 0.6224\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4678 - accuracy: 0.7839 - val_loss: 1.1069 - val_accuracy: 0.6127\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4703 - accuracy: 0.7804 - val_loss: 1.1051 - val_accuracy: 0.6336\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4655 - accuracy: 0.7881 - val_loss: 1.0635 - val_accuracy: 0.6293\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4653 - accuracy: 0.7852 - val_loss: 1.0760 - val_accuracy: 0.6224\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4712 - accuracy: 0.7825 - val_loss: 1.0857 - val_accuracy: 0.6339\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4700 - accuracy: 0.7810 - val_loss: 1.1297 - val_accuracy: 0.6196\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4622 - accuracy: 0.7884 - val_loss: 1.0974 - val_accuracy: 0.6210\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4597 - accuracy: 0.7835 - val_loss: 1.1315 - val_accuracy: 0.6257\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4624 - accuracy: 0.7860 - val_loss: 1.1077 - val_accuracy: 0.6203\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4526 - accuracy: 0.7887 - val_loss: 1.1421 - val_accuracy: 0.6188\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4599 - accuracy: 0.7873 - val_loss: 1.1756 - val_accuracy: 0.6217\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4543 - accuracy: 0.7900 - val_loss: 1.1059 - val_accuracy: 0.6239\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4487 - accuracy: 0.7927 - val_loss: 1.1702 - val_accuracy: 0.6196\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4515 - accuracy: 0.7917 - val_loss: 1.1674 - val_accuracy: 0.6257\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4535 - accuracy: 0.7916 - val_loss: 1.1504 - val_accuracy: 0.6203\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4480 - accuracy: 0.7923 - val_loss: 1.1486 - val_accuracy: 0.6303\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4417 - accuracy: 0.7963 - val_loss: 1.1847 - val_accuracy: 0.6167\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4426 - accuracy: 0.7935 - val_loss: 1.1911 - val_accuracy: 0.6203\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4476 - accuracy: 0.7919 - val_loss: 1.1444 - val_accuracy: 0.6181\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4466 - accuracy: 0.7940 - val_loss: 1.1575 - val_accuracy: 0.6131\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4398 - accuracy: 0.7970 - val_loss: 1.1718 - val_accuracy: 0.6156\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4403 - accuracy: 0.7952 - val_loss: 1.2086 - val_accuracy: 0.6196\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4347 - accuracy: 0.8025 - val_loss: 1.1829 - val_accuracy: 0.6268\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4443 - accuracy: 0.7960 - val_loss: 1.2448 - val_accuracy: 0.6099\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4368 - accuracy: 0.7971 - val_loss: 1.2782 - val_accuracy: 0.6185\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4317 - accuracy: 0.7992 - val_loss: 1.1835 - val_accuracy: 0.6156\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4288 - accuracy: 0.8030 - val_loss: 1.2095 - val_accuracy: 0.6181\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4243 - accuracy: 0.8041 - val_loss: 1.2266 - val_accuracy: 0.6081\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4316 - accuracy: 0.8001 - val_loss: 1.2349 - val_accuracy: 0.6149\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4305 - accuracy: 0.7995 - val_loss: 1.2471 - val_accuracy: 0.6102\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4247 - accuracy: 0.8061 - val_loss: 1.2651 - val_accuracy: 0.6232\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.4281 - accuracy: 0.8004 - val_loss: 1.2631 - val_accuracy: 0.6106\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4227 - accuracy: 0.8077 - val_loss: 1.2756 - val_accuracy: 0.6059\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4215 - accuracy: 0.8074 - val_loss: 1.2759 - val_accuracy: 0.6131\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4157 - accuracy: 0.8066 - val_loss: 1.3570 - val_accuracy: 0.6027\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4214 - accuracy: 0.8056 - val_loss: 1.3104 - val_accuracy: 0.6091\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4191 - accuracy: 0.8051 - val_loss: 1.2956 - val_accuracy: 0.6163\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4210 - accuracy: 0.8061 - val_loss: 1.2645 - val_accuracy: 0.6221\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4186 - accuracy: 0.8050 - val_loss: 1.3412 - val_accuracy: 0.6124\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4094 - accuracy: 0.8103 - val_loss: 1.2815 - val_accuracy: 0.6149\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4147 - accuracy: 0.8085 - val_loss: 1.3309 - val_accuracy: 0.6084\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4125 - accuracy: 0.8083 - val_loss: 1.3415 - val_accuracy: 0.6070\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4035 - accuracy: 0.8111 - val_loss: 1.3315 - val_accuracy: 0.6009\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4159 - accuracy: 0.8089 - val_loss: 1.3013 - val_accuracy: 0.6163\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4021 - accuracy: 0.8132 - val_loss: 1.3562 - val_accuracy: 0.5976\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4137 - accuracy: 0.8089 - val_loss: 1.3292 - val_accuracy: 0.6027\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4099 - accuracy: 0.8101 - val_loss: 1.3943 - val_accuracy: 0.6163\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4047 - accuracy: 0.8097 - val_loss: 1.3422 - val_accuracy: 0.6228\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4048 - accuracy: 0.8107 - val_loss: 1.3807 - val_accuracy: 0.6077\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4051 - accuracy: 0.8137 - val_loss: 1.4694 - val_accuracy: 0.6016\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4018 - accuracy: 0.8173 - val_loss: 1.3922 - val_accuracy: 0.6109\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3983 - accuracy: 0.8152 - val_loss: 1.3698 - val_accuracy: 0.6077\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3977 - accuracy: 0.8180 - val_loss: 1.3985 - val_accuracy: 0.6106\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4016 - accuracy: 0.8114 - val_loss: 1.4077 - val_accuracy: 0.6081\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4051 - accuracy: 0.8121 - val_loss: 1.4139 - val_accuracy: 0.6188\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3986 - accuracy: 0.8178 - val_loss: 1.4228 - val_accuracy: 0.6124\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3925 - accuracy: 0.8182 - val_loss: 1.4299 - val_accuracy: 0.6160\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3922 - accuracy: 0.8177 - val_loss: 1.4788 - val_accuracy: 0.6009\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3918 - accuracy: 0.8187 - val_loss: 1.4285 - val_accuracy: 0.6063\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 1s 2ms/step - loss: 0.3906 - accuracy: 0.8205 - val_loss: 1.4312 - val_accuracy: 0.6066\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3905 - accuracy: 0.8174 - val_loss: 1.4401 - val_accuracy: 0.6113\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3918 - accuracy: 0.8163 - val_loss: 1.4235 - val_accuracy: 0.5994\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3843 - accuracy: 0.8235 - val_loss: 1.4260 - val_accuracy: 0.6037\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3928 - accuracy: 0.8174 - val_loss: 1.4996 - val_accuracy: 0.6066\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3892 - accuracy: 0.8172 - val_loss: 1.4647 - val_accuracy: 0.6073\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3961 - accuracy: 0.8202 - val_loss: 1.4367 - val_accuracy: 0.6117\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3864 - accuracy: 0.8173 - val_loss: 1.4508 - val_accuracy: 0.6023\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3883 - accuracy: 0.8189 - val_loss: 1.4716 - val_accuracy: 0.6052\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3808 - accuracy: 0.8234 - val_loss: 1.4994 - val_accuracy: 0.6138\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3779 - accuracy: 0.8225 - val_loss: 1.5623 - val_accuracy: 0.6048\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3784 - accuracy: 0.8229 - val_loss: 1.5255 - val_accuracy: 0.6052\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3838 - accuracy: 0.8182 - val_loss: 1.4423 - val_accuracy: 0.6091\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3821 - accuracy: 0.8241 - val_loss: 1.5077 - val_accuracy: 0.6041\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3818 - accuracy: 0.8223 - val_loss: 1.4714 - val_accuracy: 0.6027\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3764 - accuracy: 0.8243 - val_loss: 1.5236 - val_accuracy: 0.6034\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3823 - accuracy: 0.8241 - val_loss: 1.5210 - val_accuracy: 0.6048\n",
            "History for model 2: <keras.src.callbacks.History object at 0x7b094d10a860>\n",
            "Trial 3: Number of layers = 3, Number of neurons per layer = [90, 100, 110]\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.8187 - accuracy: 0.6074 - val_loss: 0.7889 - val_accuracy: 0.6178\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7602 - accuracy: 0.6393 - val_loss: 0.7608 - val_accuracy: 0.6289\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7435 - accuracy: 0.6465 - val_loss: 0.7668 - val_accuracy: 0.6235\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7316 - accuracy: 0.6521 - val_loss: 0.7549 - val_accuracy: 0.6487\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.7209 - accuracy: 0.6572 - val_loss: 0.7624 - val_accuracy: 0.6311\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7114 - accuracy: 0.6648 - val_loss: 0.7630 - val_accuracy: 0.6375\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6993 - accuracy: 0.6714 - val_loss: 0.7581 - val_accuracy: 0.6437\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6931 - accuracy: 0.6748 - val_loss: 0.7503 - val_accuracy: 0.6444\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6838 - accuracy: 0.6806 - val_loss: 0.7566 - val_accuracy: 0.6357\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6680 - accuracy: 0.6874 - val_loss: 0.7493 - val_accuracy: 0.6419\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6901 - val_loss: 0.7708 - val_accuracy: 0.6447\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6948 - val_loss: 0.7891 - val_accuracy: 0.6347\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6956 - val_loss: 0.7670 - val_accuracy: 0.6365\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6346 - accuracy: 0.7059 - val_loss: 0.7764 - val_accuracy: 0.6365\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.7079 - val_loss: 0.7910 - val_accuracy: 0.6318\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6148 - accuracy: 0.7127 - val_loss: 0.8471 - val_accuracy: 0.6314\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6124 - accuracy: 0.7157 - val_loss: 0.7949 - val_accuracy: 0.6429\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6003 - accuracy: 0.7220 - val_loss: 0.7907 - val_accuracy: 0.6368\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5915 - accuracy: 0.7283 - val_loss: 0.8439 - val_accuracy: 0.6350\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5861 - accuracy: 0.7300 - val_loss: 0.8510 - val_accuracy: 0.6311\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5768 - accuracy: 0.7342 - val_loss: 0.8421 - val_accuracy: 0.6451\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5700 - accuracy: 0.7387 - val_loss: 0.8446 - val_accuracy: 0.6307\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5618 - accuracy: 0.7364 - val_loss: 0.8543 - val_accuracy: 0.6350\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5547 - accuracy: 0.7418 - val_loss: 0.8776 - val_accuracy: 0.6361\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5474 - accuracy: 0.7485 - val_loss: 0.8711 - val_accuracy: 0.6411\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5426 - accuracy: 0.7474 - val_loss: 0.8964 - val_accuracy: 0.6357\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5378 - accuracy: 0.7474 - val_loss: 0.9433 - val_accuracy: 0.6196\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5325 - accuracy: 0.7553 - val_loss: 0.9807 - val_accuracy: 0.6124\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5254 - accuracy: 0.7515 - val_loss: 0.9479 - val_accuracy: 0.6332\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5119 - accuracy: 0.7583 - val_loss: 0.9284 - val_accuracy: 0.6332\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5098 - accuracy: 0.7638 - val_loss: 0.9602 - val_accuracy: 0.6192\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5126 - accuracy: 0.7633 - val_loss: 1.0230 - val_accuracy: 0.6321\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4993 - accuracy: 0.7671 - val_loss: 1.0176 - val_accuracy: 0.6343\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4973 - accuracy: 0.7693 - val_loss: 1.0123 - val_accuracy: 0.6354\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4954 - accuracy: 0.7667 - val_loss: 1.0825 - val_accuracy: 0.6383\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4797 - accuracy: 0.7781 - val_loss: 1.0651 - val_accuracy: 0.6232\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4759 - accuracy: 0.7753 - val_loss: 1.0680 - val_accuracy: 0.6275\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4761 - accuracy: 0.7809 - val_loss: 1.0723 - val_accuracy: 0.6199\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4696 - accuracy: 0.7857 - val_loss: 1.1091 - val_accuracy: 0.6181\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4634 - accuracy: 0.7829 - val_loss: 1.1232 - val_accuracy: 0.6318\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4646 - accuracy: 0.7875 - val_loss: 1.1293 - val_accuracy: 0.6242\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4486 - accuracy: 0.7891 - val_loss: 1.1806 - val_accuracy: 0.6009\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4501 - accuracy: 0.7877 - val_loss: 1.2065 - val_accuracy: 0.6127\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4451 - accuracy: 0.7914 - val_loss: 1.1881 - val_accuracy: 0.6239\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4427 - accuracy: 0.7928 - val_loss: 1.1957 - val_accuracy: 0.6264\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4407 - accuracy: 0.7952 - val_loss: 1.1675 - val_accuracy: 0.6178\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4410 - accuracy: 0.7926 - val_loss: 1.1532 - val_accuracy: 0.6228\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4306 - accuracy: 0.7967 - val_loss: 1.2398 - val_accuracy: 0.6170\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4200 - accuracy: 0.8024 - val_loss: 1.2502 - val_accuracy: 0.6224\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4190 - accuracy: 0.8075 - val_loss: 1.3497 - val_accuracy: 0.6134\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4103 - accuracy: 0.8064 - val_loss: 1.3277 - val_accuracy: 0.6188\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4119 - accuracy: 0.8050 - val_loss: 1.3503 - val_accuracy: 0.6023\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4136 - accuracy: 0.8093 - val_loss: 1.3450 - val_accuracy: 0.6149\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4149 - accuracy: 0.8072 - val_loss: 1.3397 - val_accuracy: 0.6106\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4100 - accuracy: 0.8069 - val_loss: 1.3520 - val_accuracy: 0.6174\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4004 - accuracy: 0.8101 - val_loss: 1.4474 - val_accuracy: 0.6199\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3982 - accuracy: 0.8108 - val_loss: 1.4563 - val_accuracy: 0.6070\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3920 - accuracy: 0.8126 - val_loss: 1.4539 - val_accuracy: 0.6167\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3950 - accuracy: 0.8127 - val_loss: 1.4838 - val_accuracy: 0.6152\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3864 - accuracy: 0.8156 - val_loss: 1.4280 - val_accuracy: 0.6134\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3858 - accuracy: 0.8164 - val_loss: 1.5051 - val_accuracy: 0.5962\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3777 - accuracy: 0.8202 - val_loss: 1.5294 - val_accuracy: 0.6016\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3817 - accuracy: 0.8175 - val_loss: 1.4850 - val_accuracy: 0.6228\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3723 - accuracy: 0.8230 - val_loss: 1.4676 - val_accuracy: 0.6009\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3650 - accuracy: 0.8276 - val_loss: 1.5781 - val_accuracy: 0.6145\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3704 - accuracy: 0.8262 - val_loss: 1.5657 - val_accuracy: 0.6134\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3688 - accuracy: 0.8249 - val_loss: 1.5380 - val_accuracy: 0.5933\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3660 - accuracy: 0.8241 - val_loss: 1.5503 - val_accuracy: 0.6059\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3664 - accuracy: 0.8258 - val_loss: 1.4823 - val_accuracy: 0.6052\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3579 - accuracy: 0.8259 - val_loss: 1.6624 - val_accuracy: 0.6120\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3485 - accuracy: 0.8323 - val_loss: 1.6573 - val_accuracy: 0.6106\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3545 - accuracy: 0.8295 - val_loss: 1.6712 - val_accuracy: 0.6073\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3497 - accuracy: 0.8306 - val_loss: 1.6884 - val_accuracy: 0.6117\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3505 - accuracy: 0.8311 - val_loss: 1.7140 - val_accuracy: 0.6012\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3449 - accuracy: 0.8339 - val_loss: 1.7798 - val_accuracy: 0.6224\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3558 - accuracy: 0.8282 - val_loss: 1.7480 - val_accuracy: 0.5973\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3429 - accuracy: 0.8362 - val_loss: 1.7898 - val_accuracy: 0.6228\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3542 - accuracy: 0.8353 - val_loss: 1.7065 - val_accuracy: 0.6138\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3435 - accuracy: 0.8370 - val_loss: 1.6627 - val_accuracy: 0.5983\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3389 - accuracy: 0.8378 - val_loss: 1.7044 - val_accuracy: 0.5948\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3322 - accuracy: 0.8396 - val_loss: 1.7873 - val_accuracy: 0.6037\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3318 - accuracy: 0.8351 - val_loss: 1.7950 - val_accuracy: 0.6034\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3468 - accuracy: 0.8365 - val_loss: 1.6489 - val_accuracy: 0.6073\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3376 - accuracy: 0.8384 - val_loss: 1.6270 - val_accuracy: 0.5876\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3198 - accuracy: 0.8451 - val_loss: 1.8737 - val_accuracy: 0.6109\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3322 - accuracy: 0.8394 - val_loss: 1.7239 - val_accuracy: 0.6009\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3313 - accuracy: 0.8412 - val_loss: 1.8082 - val_accuracy: 0.5983\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3214 - accuracy: 0.8444 - val_loss: 1.8158 - val_accuracy: 0.6023\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3193 - accuracy: 0.8465 - val_loss: 1.7806 - val_accuracy: 0.6138\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3137 - accuracy: 0.8482 - val_loss: 1.8901 - val_accuracy: 0.6063\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3196 - accuracy: 0.8443 - val_loss: 1.8134 - val_accuracy: 0.6120\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3083 - accuracy: 0.8481 - val_loss: 1.9320 - val_accuracy: 0.6117\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3154 - accuracy: 0.8457 - val_loss: 1.8972 - val_accuracy: 0.6073\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3078 - accuracy: 0.8469 - val_loss: 1.9397 - val_accuracy: 0.6095\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3250 - accuracy: 0.8461 - val_loss: 1.8641 - val_accuracy: 0.6027\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3079 - accuracy: 0.8505 - val_loss: 1.9182 - val_accuracy: 0.6059\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3158 - accuracy: 0.8469 - val_loss: 1.8377 - val_accuracy: 0.6099\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3051 - accuracy: 0.8489 - val_loss: 2.0431 - val_accuracy: 0.6027\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3039 - accuracy: 0.8531 - val_loss: 2.1022 - val_accuracy: 0.6045\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3092 - accuracy: 0.8496 - val_loss: 1.8129 - val_accuracy: 0.6059\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3014 - accuracy: 0.8523 - val_loss: 1.8742 - val_accuracy: 0.5965\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2992 - accuracy: 0.8532 - val_loss: 1.9826 - val_accuracy: 0.6023\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3038 - accuracy: 0.8513 - val_loss: 1.9072 - val_accuracy: 0.5994\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2953 - accuracy: 0.8581 - val_loss: 2.0339 - val_accuracy: 0.6034\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2995 - accuracy: 0.8543 - val_loss: 2.0463 - val_accuracy: 0.6131\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3077 - accuracy: 0.8504 - val_loss: 1.9864 - val_accuracy: 0.6041\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2958 - accuracy: 0.8553 - val_loss: 2.0531 - val_accuracy: 0.5976\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2898 - accuracy: 0.8581 - val_loss: 2.0861 - val_accuracy: 0.6009\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2907 - accuracy: 0.8581 - val_loss: 2.0516 - val_accuracy: 0.5933\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2891 - accuracy: 0.8590 - val_loss: 2.0578 - val_accuracy: 0.6117\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2887 - accuracy: 0.8550 - val_loss: 2.0320 - val_accuracy: 0.5998\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2851 - accuracy: 0.8592 - val_loss: 1.9978 - val_accuracy: 0.6027\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2869 - accuracy: 0.8572 - val_loss: 2.0622 - val_accuracy: 0.6030\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2865 - accuracy: 0.8593 - val_loss: 2.1275 - val_accuracy: 0.6019\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2876 - accuracy: 0.8598 - val_loss: 2.0922 - val_accuracy: 0.6045\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2915 - accuracy: 0.8600 - val_loss: 2.0335 - val_accuracy: 0.6001\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2855 - accuracy: 0.8608 - val_loss: 2.1257 - val_accuracy: 0.6077\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2723 - accuracy: 0.8629 - val_loss: 2.1223 - val_accuracy: 0.5962\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2792 - accuracy: 0.8612 - val_loss: 2.1527 - val_accuracy: 0.5965\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2923 - accuracy: 0.8576 - val_loss: 1.9603 - val_accuracy: 0.6099\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2782 - accuracy: 0.8614 - val_loss: 2.1783 - val_accuracy: 0.6088\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2759 - accuracy: 0.8626 - val_loss: 2.1535 - val_accuracy: 0.6073\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2717 - accuracy: 0.8646 - val_loss: 2.2575 - val_accuracy: 0.5969\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2753 - accuracy: 0.8619 - val_loss: 2.2606 - val_accuracy: 0.5894\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2674 - accuracy: 0.8628 - val_loss: 2.2377 - val_accuracy: 0.6009\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2635 - accuracy: 0.8686 - val_loss: 2.2673 - val_accuracy: 0.5944\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2669 - accuracy: 0.8655 - val_loss: 2.2275 - val_accuracy: 0.5879\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2947 - accuracy: 0.8576 - val_loss: 2.2836 - val_accuracy: 0.5955\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2837 - accuracy: 0.8620 - val_loss: 2.2229 - val_accuracy: 0.6048\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2732 - accuracy: 0.8675 - val_loss: 2.2169 - val_accuracy: 0.6052\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2748 - accuracy: 0.8664 - val_loss: 2.2948 - val_accuracy: 0.6041\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2669 - accuracy: 0.8675 - val_loss: 2.1931 - val_accuracy: 0.6081\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2633 - accuracy: 0.8650 - val_loss: 2.3574 - val_accuracy: 0.6027\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2750 - accuracy: 0.8686 - val_loss: 2.1937 - val_accuracy: 0.5922\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2598 - accuracy: 0.8697 - val_loss: 2.2392 - val_accuracy: 0.6016\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2606 - accuracy: 0.8708 - val_loss: 2.2958 - val_accuracy: 0.5969\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2673 - accuracy: 0.8661 - val_loss: 2.2968 - val_accuracy: 0.6016\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2512 - accuracy: 0.8712 - val_loss: 2.4798 - val_accuracy: 0.6009\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2614 - accuracy: 0.8667 - val_loss: 2.3938 - val_accuracy: 0.5912\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2632 - accuracy: 0.8688 - val_loss: 2.3344 - val_accuracy: 0.5994\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2625 - accuracy: 0.8706 - val_loss: 2.3339 - val_accuracy: 0.5955\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2735 - accuracy: 0.8647 - val_loss: 2.3784 - val_accuracy: 0.5904\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2561 - accuracy: 0.8727 - val_loss: 2.4022 - val_accuracy: 0.5965\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2483 - accuracy: 0.8738 - val_loss: 2.2983 - val_accuracy: 0.6084\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2699 - accuracy: 0.8684 - val_loss: 2.4439 - val_accuracy: 0.5904\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2541 - accuracy: 0.8706 - val_loss: 2.4365 - val_accuracy: 0.5919\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2445 - accuracy: 0.8745 - val_loss: 2.5813 - val_accuracy: 0.6041\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2683 - accuracy: 0.8661 - val_loss: 2.4279 - val_accuracy: 0.5904\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2372 - accuracy: 0.8751 - val_loss: 2.5191 - val_accuracy: 0.5983\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2604 - accuracy: 0.8698 - val_loss: 2.4639 - val_accuracy: 0.5969\n",
            "History for model 3: <keras.src.callbacks.History object at 0x7b094d0e4640>\n",
            "Trial 4: Number of layers = 3, Number of neurons per layer = [120, 130, 140]\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 3s 4ms/step - loss: 0.8152 - accuracy: 0.6099 - val_loss: 0.7728 - val_accuracy: 0.6361\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7585 - accuracy: 0.6394 - val_loss: 0.7573 - val_accuracy: 0.6375\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7422 - accuracy: 0.6501 - val_loss: 0.7561 - val_accuracy: 0.6375\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7285 - accuracy: 0.6557 - val_loss: 0.7560 - val_accuracy: 0.6354\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7157 - accuracy: 0.6600 - val_loss: 0.7543 - val_accuracy: 0.6397\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7041 - accuracy: 0.6677 - val_loss: 0.7590 - val_accuracy: 0.6390\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.6761 - val_loss: 0.7708 - val_accuracy: 0.6372\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6827 - accuracy: 0.6798 - val_loss: 0.7634 - val_accuracy: 0.6440\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6727 - accuracy: 0.6854 - val_loss: 0.7669 - val_accuracy: 0.6422\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6866 - val_loss: 0.7531 - val_accuracy: 0.6476\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6936 - val_loss: 0.7609 - val_accuracy: 0.6555\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6987 - val_loss: 0.7765 - val_accuracy: 0.6455\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.7039 - val_loss: 0.7922 - val_accuracy: 0.6257\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6293 - accuracy: 0.7066 - val_loss: 0.7779 - val_accuracy: 0.6437\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6177 - accuracy: 0.7102 - val_loss: 0.7897 - val_accuracy: 0.6379\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6057 - accuracy: 0.7148 - val_loss: 0.8345 - val_accuracy: 0.6501\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.6020 - accuracy: 0.7149 - val_loss: 0.8003 - val_accuracy: 0.6303\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5918 - accuracy: 0.7273 - val_loss: 0.8082 - val_accuracy: 0.6383\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5817 - accuracy: 0.7275 - val_loss: 0.8142 - val_accuracy: 0.6343\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5733 - accuracy: 0.7333 - val_loss: 0.8643 - val_accuracy: 0.6311\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5623 - accuracy: 0.7386 - val_loss: 0.8682 - val_accuracy: 0.6422\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5621 - accuracy: 0.7396 - val_loss: 0.8613 - val_accuracy: 0.6411\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5502 - accuracy: 0.7441 - val_loss: 0.8901 - val_accuracy: 0.6343\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5440 - accuracy: 0.7460 - val_loss: 0.8863 - val_accuracy: 0.6329\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5315 - accuracy: 0.7515 - val_loss: 0.8823 - val_accuracy: 0.6390\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5234 - accuracy: 0.7558 - val_loss: 0.9941 - val_accuracy: 0.6224\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5188 - accuracy: 0.7576 - val_loss: 0.9639 - val_accuracy: 0.6214\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5098 - accuracy: 0.7657 - val_loss: 0.9484 - val_accuracy: 0.6268\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5004 - accuracy: 0.7656 - val_loss: 1.0003 - val_accuracy: 0.6296\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4943 - accuracy: 0.7680 - val_loss: 1.0233 - val_accuracy: 0.6321\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4910 - accuracy: 0.7732 - val_loss: 1.0120 - val_accuracy: 0.6300\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4842 - accuracy: 0.7739 - val_loss: 1.0283 - val_accuracy: 0.6289\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4750 - accuracy: 0.7759 - val_loss: 1.0803 - val_accuracy: 0.6311\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4713 - accuracy: 0.7785 - val_loss: 1.0578 - val_accuracy: 0.6311\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4691 - accuracy: 0.7826 - val_loss: 1.0980 - val_accuracy: 0.6178\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4583 - accuracy: 0.7835 - val_loss: 1.1231 - val_accuracy: 0.6268\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4520 - accuracy: 0.7887 - val_loss: 1.1323 - val_accuracy: 0.6260\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4422 - accuracy: 0.7923 - val_loss: 1.1736 - val_accuracy: 0.6163\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4463 - accuracy: 0.7890 - val_loss: 1.1589 - val_accuracy: 0.6142\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4383 - accuracy: 0.7947 - val_loss: 1.2074 - val_accuracy: 0.6170\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4361 - accuracy: 0.7956 - val_loss: 1.1895 - val_accuracy: 0.6181\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4277 - accuracy: 0.7940 - val_loss: 1.2525 - val_accuracy: 0.6253\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4199 - accuracy: 0.7987 - val_loss: 1.2505 - val_accuracy: 0.6253\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4198 - accuracy: 0.8011 - val_loss: 1.2336 - val_accuracy: 0.6325\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4117 - accuracy: 0.8033 - val_loss: 1.3177 - val_accuracy: 0.6275\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4157 - accuracy: 0.8047 - val_loss: 1.2696 - val_accuracy: 0.6185\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4146 - accuracy: 0.8036 - val_loss: 1.2637 - val_accuracy: 0.6206\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4013 - accuracy: 0.8102 - val_loss: 1.3177 - val_accuracy: 0.6134\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3986 - accuracy: 0.8087 - val_loss: 1.3675 - val_accuracy: 0.6206\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3877 - accuracy: 0.8143 - val_loss: 1.3811 - val_accuracy: 0.6181\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3889 - accuracy: 0.8170 - val_loss: 1.4320 - val_accuracy: 0.6117\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3897 - accuracy: 0.8112 - val_loss: 1.4128 - val_accuracy: 0.6138\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3754 - accuracy: 0.8198 - val_loss: 1.4199 - val_accuracy: 0.6199\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3884 - accuracy: 0.8132 - val_loss: 1.4559 - val_accuracy: 0.6163\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3786 - accuracy: 0.8174 - val_loss: 1.4892 - val_accuracy: 0.6070\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3777 - accuracy: 0.8194 - val_loss: 1.4514 - val_accuracy: 0.6095\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3620 - accuracy: 0.8215 - val_loss: 1.5802 - val_accuracy: 0.6120\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3584 - accuracy: 0.8278 - val_loss: 1.5912 - val_accuracy: 0.6185\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3603 - accuracy: 0.8284 - val_loss: 1.5417 - val_accuracy: 0.6117\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3559 - accuracy: 0.8300 - val_loss: 1.5914 - val_accuracy: 0.6009\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3562 - accuracy: 0.8262 - val_loss: 1.5983 - val_accuracy: 0.6127\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3510 - accuracy: 0.8278 - val_loss: 1.6030 - val_accuracy: 0.6001\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3470 - accuracy: 0.8332 - val_loss: 1.5534 - val_accuracy: 0.6127\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3458 - accuracy: 0.8338 - val_loss: 1.6926 - val_accuracy: 0.6149\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3508 - accuracy: 0.8345 - val_loss: 1.7513 - val_accuracy: 0.6077\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3440 - accuracy: 0.8336 - val_loss: 1.7549 - val_accuracy: 0.6059\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3426 - accuracy: 0.8367 - val_loss: 1.6428 - val_accuracy: 0.6037\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3379 - accuracy: 0.8360 - val_loss: 1.6718 - val_accuracy: 0.6091\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3299 - accuracy: 0.8356 - val_loss: 1.7285 - val_accuracy: 0.6019\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3316 - accuracy: 0.8425 - val_loss: 1.7964 - val_accuracy: 0.6034\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3384 - accuracy: 0.8356 - val_loss: 1.7127 - val_accuracy: 0.5976\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3307 - accuracy: 0.8398 - val_loss: 1.7926 - val_accuracy: 0.5980\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3191 - accuracy: 0.8468 - val_loss: 1.7396 - val_accuracy: 0.6106\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3265 - accuracy: 0.8410 - val_loss: 1.7977 - val_accuracy: 0.5965\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3134 - accuracy: 0.8482 - val_loss: 1.8627 - val_accuracy: 0.6134\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3199 - accuracy: 0.8482 - val_loss: 1.8389 - val_accuracy: 0.6099\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3044 - accuracy: 0.8479 - val_loss: 1.9315 - val_accuracy: 0.6077\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3104 - accuracy: 0.8481 - val_loss: 1.8725 - val_accuracy: 0.6027\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3065 - accuracy: 0.8499 - val_loss: 2.0224 - val_accuracy: 0.5976\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3139 - accuracy: 0.8460 - val_loss: 1.9279 - val_accuracy: 0.5915\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3115 - accuracy: 0.8493 - val_loss: 1.8334 - val_accuracy: 0.5897\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3053 - accuracy: 0.8512 - val_loss: 2.0086 - val_accuracy: 0.5994\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3108 - accuracy: 0.8454 - val_loss: 1.9474 - val_accuracy: 0.5955\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3007 - accuracy: 0.8518 - val_loss: 1.9696 - val_accuracy: 0.5948\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2952 - accuracy: 0.8538 - val_loss: 1.9778 - val_accuracy: 0.6019\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2878 - accuracy: 0.8590 - val_loss: 2.0445 - val_accuracy: 0.6059\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2895 - accuracy: 0.8584 - val_loss: 2.2227 - val_accuracy: 0.5879\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2859 - accuracy: 0.8575 - val_loss: 2.1914 - val_accuracy: 0.6088\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3007 - accuracy: 0.8528 - val_loss: 2.1267 - val_accuracy: 0.5955\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2970 - accuracy: 0.8551 - val_loss: 2.1920 - val_accuracy: 0.5973\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2867 - accuracy: 0.8639 - val_loss: 2.0723 - val_accuracy: 0.6012\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2897 - accuracy: 0.8580 - val_loss: 2.0528 - val_accuracy: 0.5955\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2979 - accuracy: 0.8524 - val_loss: 2.1809 - val_accuracy: 0.6027\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2827 - accuracy: 0.8573 - val_loss: 2.1226 - val_accuracy: 0.5991\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2763 - accuracy: 0.8590 - val_loss: 2.1675 - val_accuracy: 0.5915\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2795 - accuracy: 0.8618 - val_loss: 2.3645 - val_accuracy: 0.5937\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2793 - accuracy: 0.8589 - val_loss: 2.2747 - val_accuracy: 0.5994\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2725 - accuracy: 0.8635 - val_loss: 2.2586 - val_accuracy: 0.5832\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2864 - accuracy: 0.8621 - val_loss: 2.4112 - val_accuracy: 0.5955\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2682 - accuracy: 0.8652 - val_loss: 2.3662 - val_accuracy: 0.5897\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2788 - accuracy: 0.8610 - val_loss: 2.2822 - val_accuracy: 0.5843\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2599 - accuracy: 0.8695 - val_loss: 2.2908 - val_accuracy: 0.5930\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2710 - accuracy: 0.8655 - val_loss: 2.2755 - val_accuracy: 0.6059\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2667 - accuracy: 0.8654 - val_loss: 2.4185 - val_accuracy: 0.6084\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2737 - accuracy: 0.8625 - val_loss: 2.2471 - val_accuracy: 0.5930\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2633 - accuracy: 0.8711 - val_loss: 2.3822 - val_accuracy: 0.5915\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2736 - accuracy: 0.8650 - val_loss: 2.3203 - val_accuracy: 0.5933\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2801 - accuracy: 0.8645 - val_loss: 2.1028 - val_accuracy: 0.5886\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2544 - accuracy: 0.8686 - val_loss: 2.2628 - val_accuracy: 0.5858\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2511 - accuracy: 0.8708 - val_loss: 2.3485 - val_accuracy: 0.5976\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2669 - accuracy: 0.8656 - val_loss: 2.2438 - val_accuracy: 0.6019\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2662 - accuracy: 0.8674 - val_loss: 2.3465 - val_accuracy: 0.5904\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2733 - accuracy: 0.8679 - val_loss: 2.4171 - val_accuracy: 0.6063\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2486 - accuracy: 0.8735 - val_loss: 2.4482 - val_accuracy: 0.5904\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2442 - accuracy: 0.8750 - val_loss: 2.3166 - val_accuracy: 0.5948\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2514 - accuracy: 0.8767 - val_loss: 2.5330 - val_accuracy: 0.6027\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2500 - accuracy: 0.8732 - val_loss: 2.3544 - val_accuracy: 0.5843\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2696 - accuracy: 0.8670 - val_loss: 2.2527 - val_accuracy: 0.5940\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2665 - accuracy: 0.8685 - val_loss: 2.2183 - val_accuracy: 0.5872\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2584 - accuracy: 0.8704 - val_loss: 2.2792 - val_accuracy: 0.5926\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2466 - accuracy: 0.8722 - val_loss: 2.4301 - val_accuracy: 0.6001\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2405 - accuracy: 0.8796 - val_loss: 2.5967 - val_accuracy: 0.6030\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2660 - accuracy: 0.8672 - val_loss: 2.3997 - val_accuracy: 0.5886\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2434 - accuracy: 0.8778 - val_loss: 2.3988 - val_accuracy: 0.6012\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2461 - accuracy: 0.8784 - val_loss: 2.5010 - val_accuracy: 0.5912\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2570 - accuracy: 0.8724 - val_loss: 2.4310 - val_accuracy: 0.5912\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2473 - accuracy: 0.8741 - val_loss: 2.4339 - val_accuracy: 0.5861\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2343 - accuracy: 0.8835 - val_loss: 2.5570 - val_accuracy: 0.5897\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2386 - accuracy: 0.8786 - val_loss: 2.5660 - val_accuracy: 0.5904\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2269 - accuracy: 0.8798 - val_loss: 2.5623 - val_accuracy: 0.5818\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2266 - accuracy: 0.8803 - val_loss: 2.6315 - val_accuracy: 0.5955\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2508 - accuracy: 0.8732 - val_loss: 2.5831 - val_accuracy: 0.5994\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2502 - accuracy: 0.8741 - val_loss: 2.4631 - val_accuracy: 0.5847\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2520 - accuracy: 0.8758 - val_loss: 2.5067 - val_accuracy: 0.5930\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2487 - accuracy: 0.8769 - val_loss: 2.5910 - val_accuracy: 0.5814\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2367 - accuracy: 0.8796 - val_loss: 2.4421 - val_accuracy: 0.5908\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2327 - accuracy: 0.8783 - val_loss: 2.5406 - val_accuracy: 0.5973\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2332 - accuracy: 0.8817 - val_loss: 2.5670 - val_accuracy: 0.5976\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2310 - accuracy: 0.8818 - val_loss: 2.7069 - val_accuracy: 0.5930\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2426 - accuracy: 0.8806 - val_loss: 2.5832 - val_accuracy: 0.5890\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2379 - accuracy: 0.8773 - val_loss: 2.3879 - val_accuracy: 0.5775\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2390 - accuracy: 0.8770 - val_loss: 2.5739 - val_accuracy: 0.5987\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2314 - accuracy: 0.8820 - val_loss: 2.5800 - val_accuracy: 0.5951\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2204 - accuracy: 0.8851 - val_loss: 2.6174 - val_accuracy: 0.5922\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2242 - accuracy: 0.8833 - val_loss: 2.7299 - val_accuracy: 0.5915\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2287 - accuracy: 0.8839 - val_loss: 2.6025 - val_accuracy: 0.6001\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2229 - accuracy: 0.8857 - val_loss: 2.4888 - val_accuracy: 0.5908\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2310 - accuracy: 0.8789 - val_loss: 2.7270 - val_accuracy: 0.5922\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2154 - accuracy: 0.8831 - val_loss: 2.7863 - val_accuracy: 0.5926\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2518 - accuracy: 0.8762 - val_loss: 2.5291 - val_accuracy: 0.5991\n",
            "History for model 4: <keras.src.callbacks.History object at 0x7b094d09cc70>\n",
            "Trial 5: Number of layers = 3, Number of neurons per layer = [150, 160, 170]\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.8104 - accuracy: 0.6067 - val_loss: 0.7736 - val_accuracy: 0.6350\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7641 - accuracy: 0.6378 - val_loss: 0.7701 - val_accuracy: 0.6286\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7453 - accuracy: 0.6485 - val_loss: 0.7595 - val_accuracy: 0.6350\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7305 - accuracy: 0.6508 - val_loss: 0.7515 - val_accuracy: 0.6433\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7206 - accuracy: 0.6588 - val_loss: 0.7543 - val_accuracy: 0.6458\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.7064 - accuracy: 0.6630 - val_loss: 0.7590 - val_accuracy: 0.6437\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6973 - accuracy: 0.6715 - val_loss: 0.7477 - val_accuracy: 0.6512\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6872 - accuracy: 0.6753 - val_loss: 0.7899 - val_accuracy: 0.6372\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6766 - accuracy: 0.6801 - val_loss: 0.7544 - val_accuracy: 0.6422\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6695 - accuracy: 0.6876 - val_loss: 0.7643 - val_accuracy: 0.6408\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6935 - val_loss: 0.7771 - val_accuracy: 0.6393\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6933 - val_loss: 0.8039 - val_accuracy: 0.6329\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.7037 - val_loss: 0.7635 - val_accuracy: 0.6437\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.7091 - val_loss: 0.7929 - val_accuracy: 0.6444\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6159 - accuracy: 0.7097 - val_loss: 0.8035 - val_accuracy: 0.6347\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6139 - accuracy: 0.7149 - val_loss: 0.8094 - val_accuracy: 0.6537\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6030 - accuracy: 0.7197 - val_loss: 0.8248 - val_accuracy: 0.6375\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5894 - accuracy: 0.7212 - val_loss: 0.8681 - val_accuracy: 0.6339\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5841 - accuracy: 0.7294 - val_loss: 0.8356 - val_accuracy: 0.6303\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5731 - accuracy: 0.7333 - val_loss: 0.8869 - val_accuracy: 0.6188\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5658 - accuracy: 0.7337 - val_loss: 0.8752 - val_accuracy: 0.6458\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5567 - accuracy: 0.7401 - val_loss: 0.8508 - val_accuracy: 0.6296\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5495 - accuracy: 0.7457 - val_loss: 0.9095 - val_accuracy: 0.6365\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5387 - accuracy: 0.7508 - val_loss: 0.9635 - val_accuracy: 0.6325\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5338 - accuracy: 0.7533 - val_loss: 0.9585 - val_accuracy: 0.6422\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5279 - accuracy: 0.7514 - val_loss: 0.9332 - val_accuracy: 0.6303\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5153 - accuracy: 0.7589 - val_loss: 0.9681 - val_accuracy: 0.6314\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7638 - val_loss: 1.0086 - val_accuracy: 0.6235\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4983 - accuracy: 0.7657 - val_loss: 1.0226 - val_accuracy: 0.6404\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4960 - accuracy: 0.7668 - val_loss: 1.0351 - val_accuracy: 0.6275\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4886 - accuracy: 0.7661 - val_loss: 1.0553 - val_accuracy: 0.6325\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4844 - accuracy: 0.7720 - val_loss: 1.0777 - val_accuracy: 0.6307\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4819 - accuracy: 0.7734 - val_loss: 1.0331 - val_accuracy: 0.6429\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4726 - accuracy: 0.7771 - val_loss: 1.0565 - val_accuracy: 0.6224\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4649 - accuracy: 0.7802 - val_loss: 1.1196 - val_accuracy: 0.6235\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4562 - accuracy: 0.7831 - val_loss: 1.1160 - val_accuracy: 0.6311\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4434 - accuracy: 0.7914 - val_loss: 1.1596 - val_accuracy: 0.6293\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4472 - accuracy: 0.7884 - val_loss: 1.1565 - val_accuracy: 0.6221\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4429 - accuracy: 0.7874 - val_loss: 1.2639 - val_accuracy: 0.6325\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4363 - accuracy: 0.7939 - val_loss: 1.3111 - val_accuracy: 0.6289\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4295 - accuracy: 0.7945 - val_loss: 1.1990 - val_accuracy: 0.6268\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4246 - accuracy: 0.7947 - val_loss: 1.3083 - val_accuracy: 0.6239\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4281 - accuracy: 0.7991 - val_loss: 1.2511 - val_accuracy: 0.6307\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4201 - accuracy: 0.8018 - val_loss: 1.3059 - val_accuracy: 0.6210\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4059 - accuracy: 0.8047 - val_loss: 1.2382 - val_accuracy: 0.6242\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3979 - accuracy: 0.8086 - val_loss: 1.4315 - val_accuracy: 0.6149\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4025 - accuracy: 0.8065 - val_loss: 1.3352 - val_accuracy: 0.6250\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3925 - accuracy: 0.8068 - val_loss: 1.4939 - val_accuracy: 0.6170\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3981 - accuracy: 0.8094 - val_loss: 1.4858 - val_accuracy: 0.6260\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3843 - accuracy: 0.8109 - val_loss: 1.4718 - val_accuracy: 0.6303\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3830 - accuracy: 0.8134 - val_loss: 1.5260 - val_accuracy: 0.6271\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3859 - accuracy: 0.8145 - val_loss: 1.4349 - val_accuracy: 0.6138\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3802 - accuracy: 0.8208 - val_loss: 1.4453 - val_accuracy: 0.6127\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3746 - accuracy: 0.8227 - val_loss: 1.5583 - val_accuracy: 0.6124\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3745 - accuracy: 0.8199 - val_loss: 1.4214 - val_accuracy: 0.6188\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3654 - accuracy: 0.8197 - val_loss: 1.5413 - val_accuracy: 0.6127\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3676 - accuracy: 0.8266 - val_loss: 1.5572 - val_accuracy: 0.6134\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3704 - accuracy: 0.8187 - val_loss: 1.5069 - val_accuracy: 0.6117\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3563 - accuracy: 0.8294 - val_loss: 1.5509 - val_accuracy: 0.6181\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3477 - accuracy: 0.8289 - val_loss: 1.7361 - val_accuracy: 0.6131\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3367 - accuracy: 0.8306 - val_loss: 1.7161 - val_accuracy: 0.6138\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3406 - accuracy: 0.8321 - val_loss: 1.7548 - val_accuracy: 0.6045\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3461 - accuracy: 0.8310 - val_loss: 1.7397 - val_accuracy: 0.6178\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3482 - accuracy: 0.8302 - val_loss: 1.7300 - val_accuracy: 0.6210\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3350 - accuracy: 0.8344 - val_loss: 1.6963 - val_accuracy: 0.6174\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3402 - accuracy: 0.8323 - val_loss: 1.7226 - val_accuracy: 0.6084\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3265 - accuracy: 0.8390 - val_loss: 1.7628 - val_accuracy: 0.6109\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3215 - accuracy: 0.8428 - val_loss: 1.8211 - val_accuracy: 0.6156\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3340 - accuracy: 0.8377 - val_loss: 1.8251 - val_accuracy: 0.6163\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3261 - accuracy: 0.8382 - val_loss: 1.7892 - val_accuracy: 0.6088\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3137 - accuracy: 0.8436 - val_loss: 1.9364 - val_accuracy: 0.6077\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3096 - accuracy: 0.8450 - val_loss: 1.8820 - val_accuracy: 0.6066\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3227 - accuracy: 0.8419 - val_loss: 1.9291 - val_accuracy: 0.6073\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3261 - accuracy: 0.8413 - val_loss: 1.8698 - val_accuracy: 0.6102\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3079 - accuracy: 0.8474 - val_loss: 1.9345 - val_accuracy: 0.6099\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2972 - accuracy: 0.8510 - val_loss: 1.8652 - val_accuracy: 0.5965\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3086 - accuracy: 0.8497 - val_loss: 1.9361 - val_accuracy: 0.6095\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3158 - accuracy: 0.8459 - val_loss: 1.9358 - val_accuracy: 0.6070\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3103 - accuracy: 0.8454 - val_loss: 1.9912 - val_accuracy: 0.6084\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2917 - accuracy: 0.8546 - val_loss: 2.1618 - val_accuracy: 0.6037\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2869 - accuracy: 0.8537 - val_loss: 2.2233 - val_accuracy: 0.6142\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2866 - accuracy: 0.8558 - val_loss: 2.0744 - val_accuracy: 0.6088\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2911 - accuracy: 0.8573 - val_loss: 2.0762 - val_accuracy: 0.5951\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2994 - accuracy: 0.8522 - val_loss: 2.0638 - val_accuracy: 0.6192\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2937 - accuracy: 0.8562 - val_loss: 2.0182 - val_accuracy: 0.6027\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2909 - accuracy: 0.8542 - val_loss: 2.0904 - val_accuracy: 0.6113\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2810 - accuracy: 0.8593 - val_loss: 2.0949 - val_accuracy: 0.6045\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2716 - accuracy: 0.8600 - val_loss: 2.1390 - val_accuracy: 0.5908\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2728 - accuracy: 0.8606 - val_loss: 2.3196 - val_accuracy: 0.6005\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2822 - accuracy: 0.8588 - val_loss: 2.2596 - val_accuracy: 0.5865\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3041 - accuracy: 0.8564 - val_loss: 2.1606 - val_accuracy: 0.6102\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2783 - accuracy: 0.8579 - val_loss: 2.1411 - val_accuracy: 0.6041\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2624 - accuracy: 0.8628 - val_loss: 2.1591 - val_accuracy: 0.5976\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2650 - accuracy: 0.8633 - val_loss: 2.2357 - val_accuracy: 0.6016\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2854 - accuracy: 0.8564 - val_loss: 2.3047 - val_accuracy: 0.5980\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2813 - accuracy: 0.8608 - val_loss: 2.3038 - val_accuracy: 0.6023\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2720 - accuracy: 0.8626 - val_loss: 2.3160 - val_accuracy: 0.6066\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2683 - accuracy: 0.8632 - val_loss: 2.1306 - val_accuracy: 0.5919\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2643 - accuracy: 0.8639 - val_loss: 2.3385 - val_accuracy: 0.5965\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2510 - accuracy: 0.8697 - val_loss: 2.4447 - val_accuracy: 0.6034\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2470 - accuracy: 0.8688 - val_loss: 2.4047 - val_accuracy: 0.5983\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2622 - accuracy: 0.8641 - val_loss: 2.4979 - val_accuracy: 0.6041\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2771 - accuracy: 0.8606 - val_loss: 2.1595 - val_accuracy: 0.6081\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2706 - accuracy: 0.8643 - val_loss: 2.1976 - val_accuracy: 0.5930\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2494 - accuracy: 0.8710 - val_loss: 2.5746 - val_accuracy: 0.6037\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2554 - accuracy: 0.8710 - val_loss: 2.4694 - val_accuracy: 0.5976\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2648 - accuracy: 0.8687 - val_loss: 2.3141 - val_accuracy: 0.6088\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2506 - accuracy: 0.8705 - val_loss: 2.4830 - val_accuracy: 0.6106\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2340 - accuracy: 0.8750 - val_loss: 2.4633 - val_accuracy: 0.6052\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2398 - accuracy: 0.8773 - val_loss: 2.5408 - val_accuracy: 0.6081\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2519 - accuracy: 0.8697 - val_loss: 2.5732 - val_accuracy: 0.6030\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2499 - accuracy: 0.8745 - val_loss: 2.6546 - val_accuracy: 0.5965\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2552 - accuracy: 0.8723 - val_loss: 2.4797 - val_accuracy: 0.5987\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2421 - accuracy: 0.8741 - val_loss: 2.4221 - val_accuracy: 0.6005\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2370 - accuracy: 0.8768 - val_loss: 2.5494 - val_accuracy: 0.6034\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2543 - accuracy: 0.8694 - val_loss: 2.5660 - val_accuracy: 0.5980\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2479 - accuracy: 0.8708 - val_loss: 2.4718 - val_accuracy: 0.5908\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2498 - accuracy: 0.8712 - val_loss: 2.5203 - val_accuracy: 0.5901\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2483 - accuracy: 0.8753 - val_loss: 2.4199 - val_accuracy: 0.5948\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2562 - accuracy: 0.8725 - val_loss: 2.4712 - val_accuracy: 0.5951\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2353 - accuracy: 0.8766 - val_loss: 2.7042 - val_accuracy: 0.5897\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2400 - accuracy: 0.8788 - val_loss: 2.4617 - val_accuracy: 0.5987\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2436 - accuracy: 0.8768 - val_loss: 2.5766 - val_accuracy: 0.5965\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2267 - accuracy: 0.8835 - val_loss: 2.7452 - val_accuracy: 0.5890\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2342 - accuracy: 0.8787 - val_loss: 2.6573 - val_accuracy: 0.6041\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2207 - accuracy: 0.8833 - val_loss: 2.6734 - val_accuracy: 0.5948\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2440 - accuracy: 0.8757 - val_loss: 2.6766 - val_accuracy: 0.6023\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2503 - accuracy: 0.8751 - val_loss: 2.5535 - val_accuracy: 0.5980\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2388 - accuracy: 0.8784 - val_loss: 2.6994 - val_accuracy: 0.5933\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2308 - accuracy: 0.8800 - val_loss: 2.7476 - val_accuracy: 0.5948\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2237 - accuracy: 0.8837 - val_loss: 2.6482 - val_accuracy: 0.5930\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2312 - accuracy: 0.8788 - val_loss: 2.7714 - val_accuracy: 0.5937\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2159 - accuracy: 0.8862 - val_loss: 2.9626 - val_accuracy: 0.5922\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2173 - accuracy: 0.8840 - val_loss: 2.7019 - val_accuracy: 0.5991\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2315 - accuracy: 0.8832 - val_loss: 2.7988 - val_accuracy: 0.5912\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2291 - accuracy: 0.8841 - val_loss: 2.8016 - val_accuracy: 0.5930\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2452 - accuracy: 0.8769 - val_loss: 2.8293 - val_accuracy: 0.5958\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2342 - accuracy: 0.8788 - val_loss: 2.8351 - val_accuracy: 0.6005\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2120 - accuracy: 0.8886 - val_loss: 2.9928 - val_accuracy: 0.5886\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2172 - accuracy: 0.8894 - val_loss: 2.8172 - val_accuracy: 0.5868\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2162 - accuracy: 0.8852 - val_loss: 2.8694 - val_accuracy: 0.5904\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2455 - accuracy: 0.8776 - val_loss: 2.5241 - val_accuracy: 0.5944\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2321 - accuracy: 0.8825 - val_loss: 2.9169 - val_accuracy: 0.5890\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2202 - accuracy: 0.8866 - val_loss: 2.9576 - val_accuracy: 0.5854\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2203 - accuracy: 0.8857 - val_loss: 2.7052 - val_accuracy: 0.5915\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2162 - accuracy: 0.8859 - val_loss: 2.8714 - val_accuracy: 0.5922\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2044 - accuracy: 0.8910 - val_loss: 2.9295 - val_accuracy: 0.5897\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2210 - accuracy: 0.8857 - val_loss: 2.9987 - val_accuracy: 0.5973\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2166 - accuracy: 0.8844 - val_loss: 2.8352 - val_accuracy: 0.6009\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2213 - accuracy: 0.8830 - val_loss: 2.6980 - val_accuracy: 0.5879\n",
            "History for model 5: <keras.src.callbacks.History object at 0x7b093d4a9600>\n",
            "Trial 6: Number of layers = 3, Number of neurons per layer = [180, 190, 200]\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.8103 - accuracy: 0.6103 - val_loss: 0.7812 - val_accuracy: 0.6336\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7612 - accuracy: 0.6371 - val_loss: 0.7737 - val_accuracy: 0.6365\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7435 - accuracy: 0.6506 - val_loss: 0.7535 - val_accuracy: 0.6379\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7290 - accuracy: 0.6579 - val_loss: 0.7470 - val_accuracy: 0.6404\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7150 - accuracy: 0.6627 - val_loss: 0.7538 - val_accuracy: 0.6462\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7057 - accuracy: 0.6680 - val_loss: 0.7455 - val_accuracy: 0.6447\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6919 - accuracy: 0.6778 - val_loss: 0.7421 - val_accuracy: 0.6516\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6833 - accuracy: 0.6793 - val_loss: 0.7409 - val_accuracy: 0.6469\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6705 - accuracy: 0.6914 - val_loss: 0.7903 - val_accuracy: 0.6282\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6621 - accuracy: 0.6897 - val_loss: 0.7763 - val_accuracy: 0.6329\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6521 - accuracy: 0.6988 - val_loss: 0.7643 - val_accuracy: 0.6390\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6433 - accuracy: 0.7046 - val_loss: 0.7628 - val_accuracy: 0.6566\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.7105 - val_loss: 0.7953 - val_accuracy: 0.6264\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.7112 - val_loss: 0.7864 - val_accuracy: 0.6296\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6071 - accuracy: 0.7214 - val_loss: 0.8268 - val_accuracy: 0.6303\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6008 - accuracy: 0.7201 - val_loss: 0.8383 - val_accuracy: 0.6332\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5923 - accuracy: 0.7260 - val_loss: 0.8511 - val_accuracy: 0.6390\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5795 - accuracy: 0.7314 - val_loss: 0.8701 - val_accuracy: 0.6487\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5685 - accuracy: 0.7346 - val_loss: 0.8407 - val_accuracy: 0.6350\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5558 - accuracy: 0.7406 - val_loss: 0.8911 - val_accuracy: 0.6354\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5509 - accuracy: 0.7464 - val_loss: 0.8935 - val_accuracy: 0.6332\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5474 - accuracy: 0.7481 - val_loss: 0.9207 - val_accuracy: 0.6411\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5306 - accuracy: 0.7500 - val_loss: 0.9258 - val_accuracy: 0.6368\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5195 - accuracy: 0.7575 - val_loss: 0.9495 - val_accuracy: 0.6498\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5073 - accuracy: 0.7642 - val_loss: 0.9942 - val_accuracy: 0.6257\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5004 - accuracy: 0.7646 - val_loss: 0.9352 - val_accuracy: 0.6329\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4940 - accuracy: 0.7643 - val_loss: 0.9773 - val_accuracy: 0.6375\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4992 - accuracy: 0.7661 - val_loss: 0.9868 - val_accuracy: 0.6339\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4806 - accuracy: 0.7705 - val_loss: 1.0427 - val_accuracy: 0.6375\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4722 - accuracy: 0.7790 - val_loss: 1.0233 - val_accuracy: 0.6210\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4631 - accuracy: 0.7813 - val_loss: 1.1032 - val_accuracy: 0.6253\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4511 - accuracy: 0.7878 - val_loss: 1.1756 - val_accuracy: 0.6250\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4472 - accuracy: 0.7868 - val_loss: 1.2285 - val_accuracy: 0.6228\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4391 - accuracy: 0.7907 - val_loss: 1.2101 - val_accuracy: 0.6278\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4418 - accuracy: 0.7920 - val_loss: 1.1734 - val_accuracy: 0.6242\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4259 - accuracy: 0.7962 - val_loss: 1.1110 - val_accuracy: 0.6214\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4161 - accuracy: 0.7986 - val_loss: 1.3013 - val_accuracy: 0.6152\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4099 - accuracy: 0.8015 - val_loss: 1.3219 - val_accuracy: 0.6142\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4222 - accuracy: 0.7980 - val_loss: 1.2328 - val_accuracy: 0.6199\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4157 - accuracy: 0.8002 - val_loss: 1.3793 - val_accuracy: 0.6235\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3938 - accuracy: 0.8087 - val_loss: 1.3908 - val_accuracy: 0.6030\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4097 - accuracy: 0.8038 - val_loss: 1.3589 - val_accuracy: 0.6120\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3881 - accuracy: 0.8138 - val_loss: 1.3672 - val_accuracy: 0.6170\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3800 - accuracy: 0.8148 - val_loss: 1.3270 - val_accuracy: 0.6199\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3816 - accuracy: 0.8132 - val_loss: 1.4616 - val_accuracy: 0.6106\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3738 - accuracy: 0.8184 - val_loss: 1.4671 - val_accuracy: 0.6106\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3704 - accuracy: 0.8202 - val_loss: 1.4700 - val_accuracy: 0.6246\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3703 - accuracy: 0.8207 - val_loss: 1.4670 - val_accuracy: 0.6196\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3581 - accuracy: 0.8268 - val_loss: 1.5002 - val_accuracy: 0.6109\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3483 - accuracy: 0.8298 - val_loss: 1.5707 - val_accuracy: 0.6156\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3455 - accuracy: 0.8314 - val_loss: 1.5957 - val_accuracy: 0.6289\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3517 - accuracy: 0.8285 - val_loss: 1.6540 - val_accuracy: 0.6152\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3474 - accuracy: 0.8277 - val_loss: 1.6820 - val_accuracy: 0.6149\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3463 - accuracy: 0.8294 - val_loss: 1.6154 - val_accuracy: 0.6109\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3540 - accuracy: 0.8324 - val_loss: 1.5761 - val_accuracy: 0.6138\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3367 - accuracy: 0.8354 - val_loss: 1.7178 - val_accuracy: 0.6091\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3342 - accuracy: 0.8348 - val_loss: 1.6063 - val_accuracy: 0.6167\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3176 - accuracy: 0.8438 - val_loss: 1.8210 - val_accuracy: 0.6048\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3290 - accuracy: 0.8380 - val_loss: 1.7377 - val_accuracy: 0.6113\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3217 - accuracy: 0.8390 - val_loss: 1.8149 - val_accuracy: 0.6012\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3105 - accuracy: 0.8432 - val_loss: 1.8666 - val_accuracy: 0.5998\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3179 - accuracy: 0.8437 - val_loss: 1.8480 - val_accuracy: 0.6073\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3158 - accuracy: 0.8439 - val_loss: 1.7629 - val_accuracy: 0.6001\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3122 - accuracy: 0.8466 - val_loss: 1.7463 - val_accuracy: 0.6027\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3188 - accuracy: 0.8447 - val_loss: 1.9200 - val_accuracy: 0.6131\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3026 - accuracy: 0.8463 - val_loss: 2.0147 - val_accuracy: 0.6109\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2882 - accuracy: 0.8549 - val_loss: 2.0911 - val_accuracy: 0.6145\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2921 - accuracy: 0.8541 - val_loss: 2.0226 - val_accuracy: 0.6145\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3109 - accuracy: 0.8481 - val_loss: 1.9159 - val_accuracy: 0.6106\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3068 - accuracy: 0.8477 - val_loss: 1.8922 - val_accuracy: 0.5908\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3009 - accuracy: 0.8544 - val_loss: 1.9494 - val_accuracy: 0.5904\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2846 - accuracy: 0.8562 - val_loss: 2.0032 - val_accuracy: 0.6037\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2826 - accuracy: 0.8573 - val_loss: 2.0628 - val_accuracy: 0.6095\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3071 - accuracy: 0.8548 - val_loss: 1.9094 - val_accuracy: 0.6088\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2926 - accuracy: 0.8591 - val_loss: 2.1162 - val_accuracy: 0.6066\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2797 - accuracy: 0.8607 - val_loss: 2.2369 - val_accuracy: 0.6027\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2724 - accuracy: 0.8601 - val_loss: 2.3731 - val_accuracy: 0.5998\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2729 - accuracy: 0.8615 - val_loss: 2.0888 - val_accuracy: 0.6023\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2799 - accuracy: 0.8590 - val_loss: 1.9519 - val_accuracy: 0.6037\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2719 - accuracy: 0.8640 - val_loss: 2.2225 - val_accuracy: 0.5983\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2781 - accuracy: 0.8596 - val_loss: 2.2045 - val_accuracy: 0.6084\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2827 - accuracy: 0.8605 - val_loss: 2.1514 - val_accuracy: 0.5933\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2580 - accuracy: 0.8691 - val_loss: 2.2280 - val_accuracy: 0.5944\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2805 - accuracy: 0.8633 - val_loss: 2.1626 - val_accuracy: 0.6041\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2563 - accuracy: 0.8724 - val_loss: 2.1990 - val_accuracy: 0.5865\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2699 - accuracy: 0.8631 - val_loss: 2.1613 - val_accuracy: 0.5937\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2515 - accuracy: 0.8698 - val_loss: 2.3619 - val_accuracy: 0.5976\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2724 - accuracy: 0.8627 - val_loss: 2.3302 - val_accuracy: 0.5991\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2624 - accuracy: 0.8702 - val_loss: 2.1870 - val_accuracy: 0.5987\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2623 - accuracy: 0.8667 - val_loss: 2.2053 - val_accuracy: 0.5890\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2583 - accuracy: 0.8652 - val_loss: 2.3702 - val_accuracy: 0.6027\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2574 - accuracy: 0.8677 - val_loss: 2.3268 - val_accuracy: 0.6019\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2373 - accuracy: 0.8770 - val_loss: 2.3184 - val_accuracy: 0.5940\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2496 - accuracy: 0.8709 - val_loss: 2.3706 - val_accuracy: 0.6012\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2604 - accuracy: 0.8669 - val_loss: 2.4668 - val_accuracy: 0.5944\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2454 - accuracy: 0.8723 - val_loss: 2.6839 - val_accuracy: 0.5973\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2550 - accuracy: 0.8682 - val_loss: 2.4051 - val_accuracy: 0.5991\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2692 - accuracy: 0.8681 - val_loss: 2.1618 - val_accuracy: 0.5915\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2497 - accuracy: 0.8714 - val_loss: 2.4237 - val_accuracy: 0.5901\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2448 - accuracy: 0.8747 - val_loss: 2.4677 - val_accuracy: 0.5912\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2372 - accuracy: 0.8768 - val_loss: 2.6089 - val_accuracy: 0.5994\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2355 - accuracy: 0.8784 - val_loss: 2.5875 - val_accuracy: 0.6030\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2406 - accuracy: 0.8733 - val_loss: 2.4744 - val_accuracy: 0.5965\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2254 - accuracy: 0.8815 - val_loss: 2.6738 - val_accuracy: 0.5912\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2568 - accuracy: 0.8713 - val_loss: 2.4719 - val_accuracy: 0.5807\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2538 - accuracy: 0.8688 - val_loss: 2.6495 - val_accuracy: 0.5811\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2342 - accuracy: 0.8778 - val_loss: 2.7280 - val_accuracy: 0.5983\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2209 - accuracy: 0.8840 - val_loss: 2.7262 - val_accuracy: 0.6059\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2324 - accuracy: 0.8812 - val_loss: 2.5523 - val_accuracy: 0.5829\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2398 - accuracy: 0.8791 - val_loss: 2.6162 - val_accuracy: 0.5897\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2264 - accuracy: 0.8826 - val_loss: 2.5999 - val_accuracy: 0.5908\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2203 - accuracy: 0.8815 - val_loss: 2.5900 - val_accuracy: 0.5955\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2166 - accuracy: 0.8815 - val_loss: 2.7659 - val_accuracy: 0.5908\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2307 - accuracy: 0.8820 - val_loss: 2.7172 - val_accuracy: 0.5883\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2562 - accuracy: 0.8723 - val_loss: 2.5282 - val_accuracy: 0.5944\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2515 - accuracy: 0.8754 - val_loss: 2.7679 - val_accuracy: 0.5904\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2326 - accuracy: 0.8783 - val_loss: 2.6343 - val_accuracy: 0.5958\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2214 - accuracy: 0.8857 - val_loss: 2.8228 - val_accuracy: 0.5969\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2108 - accuracy: 0.8861 - val_loss: 2.5909 - val_accuracy: 0.5944\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2135 - accuracy: 0.8822 - val_loss: 2.7784 - val_accuracy: 0.5883\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2178 - accuracy: 0.8845 - val_loss: 2.6658 - val_accuracy: 0.5850\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2155 - accuracy: 0.8842 - val_loss: 2.9023 - val_accuracy: 0.5901\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2193 - accuracy: 0.8844 - val_loss: 2.8423 - val_accuracy: 0.5969\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2282 - accuracy: 0.8812 - val_loss: 3.0016 - val_accuracy: 0.5786\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2276 - accuracy: 0.8846 - val_loss: 2.6953 - val_accuracy: 0.5865\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2201 - accuracy: 0.8842 - val_loss: 2.8838 - val_accuracy: 0.5876\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2039 - accuracy: 0.8900 - val_loss: 3.0421 - val_accuracy: 0.5904\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2310 - accuracy: 0.8828 - val_loss: 2.7639 - val_accuracy: 0.5922\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2455 - accuracy: 0.8768 - val_loss: 2.7358 - val_accuracy: 0.5883\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2223 - accuracy: 0.8861 - val_loss: 2.7994 - val_accuracy: 0.5904\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2080 - accuracy: 0.8871 - val_loss: 2.9628 - val_accuracy: 0.5919\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2057 - accuracy: 0.8890 - val_loss: 3.0447 - val_accuracy: 0.5951\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2131 - accuracy: 0.8884 - val_loss: 2.7484 - val_accuracy: 0.5865\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2251 - accuracy: 0.8852 - val_loss: 2.8986 - val_accuracy: 0.5904\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2254 - accuracy: 0.8864 - val_loss: 2.8320 - val_accuracy: 0.5980\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2085 - accuracy: 0.8895 - val_loss: 3.0242 - val_accuracy: 0.6005\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.1958 - accuracy: 0.8926 - val_loss: 3.1901 - val_accuracy: 0.6077\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.1985 - accuracy: 0.8951 - val_loss: 3.2836 - val_accuracy: 0.5930\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2088 - accuracy: 0.8879 - val_loss: 2.7727 - val_accuracy: 0.5930\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2083 - accuracy: 0.8893 - val_loss: 3.0367 - val_accuracy: 0.5908\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2294 - accuracy: 0.8875 - val_loss: 2.7344 - val_accuracy: 0.5962\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2054 - accuracy: 0.8874 - val_loss: 2.9340 - val_accuracy: 0.6091\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.1911 - accuracy: 0.8963 - val_loss: 2.9733 - val_accuracy: 0.5973\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2144 - accuracy: 0.8908 - val_loss: 2.6162 - val_accuracy: 0.5937\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2208 - accuracy: 0.8869 - val_loss: 2.8952 - val_accuracy: 0.5912\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2024 - accuracy: 0.8924 - val_loss: 2.9459 - val_accuracy: 0.5836\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2013 - accuracy: 0.8932 - val_loss: 2.9165 - val_accuracy: 0.5944\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2098 - accuracy: 0.8884 - val_loss: 2.9023 - val_accuracy: 0.6001\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2040 - accuracy: 0.8913 - val_loss: 3.0991 - val_accuracy: 0.5969\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2095 - accuracy: 0.8929 - val_loss: 3.1750 - val_accuracy: 0.5969\n",
            "History for model 6: <keras.src.callbacks.History object at 0x7b093d2a80a0>\n",
            "Trial 7: Number of layers = 4, Number of neurons per layer = [60, 70, 80, 90]\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.8270 - accuracy: 0.5965 - val_loss: 0.7696 - val_accuracy: 0.6365\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7642 - accuracy: 0.6371 - val_loss: 0.7689 - val_accuracy: 0.6365\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7463 - accuracy: 0.6458 - val_loss: 0.7582 - val_accuracy: 0.6447\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7391 - accuracy: 0.6544 - val_loss: 0.7732 - val_accuracy: 0.6210\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7232 - accuracy: 0.6590 - val_loss: 0.7542 - val_accuracy: 0.6437\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7152 - accuracy: 0.6614 - val_loss: 0.7548 - val_accuracy: 0.6494\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7062 - accuracy: 0.6690 - val_loss: 0.7569 - val_accuracy: 0.6311\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6965 - accuracy: 0.6800 - val_loss: 0.7536 - val_accuracy: 0.6483\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6908 - accuracy: 0.6784 - val_loss: 0.7648 - val_accuracy: 0.6368\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6793 - accuracy: 0.6791 - val_loss: 0.7609 - val_accuracy: 0.6440\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6736 - accuracy: 0.6855 - val_loss: 0.7558 - val_accuracy: 0.6472\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6652 - accuracy: 0.6899 - val_loss: 0.7583 - val_accuracy: 0.6411\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6923 - val_loss: 0.7796 - val_accuracy: 0.6458\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6482 - accuracy: 0.6933 - val_loss: 0.7782 - val_accuracy: 0.6437\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6977 - val_loss: 0.8005 - val_accuracy: 0.6404\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6381 - accuracy: 0.7007 - val_loss: 0.7623 - val_accuracy: 0.6559\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.7078 - val_loss: 0.7767 - val_accuracy: 0.6401\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6217 - accuracy: 0.7114 - val_loss: 0.8180 - val_accuracy: 0.6490\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6160 - accuracy: 0.7088 - val_loss: 0.7903 - val_accuracy: 0.6386\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.6027 - accuracy: 0.7192 - val_loss: 0.8257 - val_accuracy: 0.6336\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6014 - accuracy: 0.7205 - val_loss: 0.8248 - val_accuracy: 0.6368\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5953 - accuracy: 0.7252 - val_loss: 0.8612 - val_accuracy: 0.6383\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5877 - accuracy: 0.7292 - val_loss: 0.8441 - val_accuracy: 0.6404\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5766 - accuracy: 0.7333 - val_loss: 0.8922 - val_accuracy: 0.6375\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5760 - accuracy: 0.7328 - val_loss: 0.8702 - val_accuracy: 0.6383\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5696 - accuracy: 0.7352 - val_loss: 0.8596 - val_accuracy: 0.6303\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5647 - accuracy: 0.7379 - val_loss: 0.8839 - val_accuracy: 0.6332\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5569 - accuracy: 0.7395 - val_loss: 0.9111 - val_accuracy: 0.6296\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5465 - accuracy: 0.7400 - val_loss: 0.9269 - val_accuracy: 0.6419\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5435 - accuracy: 0.7471 - val_loss: 0.8907 - val_accuracy: 0.6242\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5364 - accuracy: 0.7494 - val_loss: 0.8932 - val_accuracy: 0.6228\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5311 - accuracy: 0.7522 - val_loss: 0.9284 - val_accuracy: 0.6401\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5271 - accuracy: 0.7534 - val_loss: 0.9320 - val_accuracy: 0.6250\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5256 - accuracy: 0.7546 - val_loss: 0.9224 - val_accuracy: 0.6350\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5171 - accuracy: 0.7567 - val_loss: 1.0243 - val_accuracy: 0.6250\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5180 - accuracy: 0.7555 - val_loss: 1.0162 - val_accuracy: 0.6145\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5056 - accuracy: 0.7617 - val_loss: 1.0131 - val_accuracy: 0.6235\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5012 - accuracy: 0.7668 - val_loss: 1.0714 - val_accuracy: 0.6152\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4961 - accuracy: 0.7665 - val_loss: 1.0133 - val_accuracy: 0.6167\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5017 - accuracy: 0.7667 - val_loss: 1.0410 - val_accuracy: 0.6289\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4868 - accuracy: 0.7721 - val_loss: 1.1186 - val_accuracy: 0.6271\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4822 - accuracy: 0.7727 - val_loss: 1.0515 - val_accuracy: 0.6235\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4808 - accuracy: 0.7759 - val_loss: 1.0584 - val_accuracy: 0.6232\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4700 - accuracy: 0.7771 - val_loss: 1.1820 - val_accuracy: 0.6235\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4739 - accuracy: 0.7780 - val_loss: 1.0771 - val_accuracy: 0.6224\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4652 - accuracy: 0.7776 - val_loss: 1.1422 - val_accuracy: 0.6253\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4644 - accuracy: 0.7807 - val_loss: 1.1214 - val_accuracy: 0.6347\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4651 - accuracy: 0.7824 - val_loss: 1.1413 - val_accuracy: 0.6257\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4524 - accuracy: 0.7850 - val_loss: 1.1004 - val_accuracy: 0.6142\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4450 - accuracy: 0.7853 - val_loss: 1.1644 - val_accuracy: 0.6232\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4487 - accuracy: 0.7877 - val_loss: 1.1575 - val_accuracy: 0.6131\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4481 - accuracy: 0.7876 - val_loss: 1.1769 - val_accuracy: 0.6271\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4497 - accuracy: 0.7913 - val_loss: 1.1654 - val_accuracy: 0.6156\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4475 - accuracy: 0.7891 - val_loss: 1.2141 - val_accuracy: 0.6109\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4373 - accuracy: 0.7928 - val_loss: 1.1679 - val_accuracy: 0.6217\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4249 - accuracy: 0.7952 - val_loss: 1.2666 - val_accuracy: 0.6027\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4261 - accuracy: 0.8005 - val_loss: 1.2424 - val_accuracy: 0.6152\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4229 - accuracy: 0.8004 - val_loss: 1.2745 - val_accuracy: 0.6221\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4222 - accuracy: 0.7997 - val_loss: 1.2925 - val_accuracy: 0.6134\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4146 - accuracy: 0.8024 - val_loss: 1.3850 - val_accuracy: 0.6163\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4174 - accuracy: 0.8032 - val_loss: 1.2793 - val_accuracy: 0.6102\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4166 - accuracy: 0.8007 - val_loss: 1.2714 - val_accuracy: 0.5983\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4058 - accuracy: 0.8115 - val_loss: 1.3600 - val_accuracy: 0.6088\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4121 - accuracy: 0.8032 - val_loss: 1.3603 - val_accuracy: 0.6203\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3981 - accuracy: 0.8079 - val_loss: 1.3756 - val_accuracy: 0.6099\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4036 - accuracy: 0.8142 - val_loss: 1.3371 - val_accuracy: 0.6156\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4168 - accuracy: 0.8014 - val_loss: 1.3852 - val_accuracy: 0.6192\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3998 - accuracy: 0.8133 - val_loss: 1.4721 - val_accuracy: 0.6073\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3940 - accuracy: 0.8129 - val_loss: 1.4876 - val_accuracy: 0.6030\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3940 - accuracy: 0.8107 - val_loss: 1.3930 - val_accuracy: 0.6001\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3915 - accuracy: 0.8125 - val_loss: 1.4745 - val_accuracy: 0.6152\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3838 - accuracy: 0.8194 - val_loss: 1.4334 - val_accuracy: 0.6081\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3886 - accuracy: 0.8149 - val_loss: 1.4228 - val_accuracy: 0.6246\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3876 - accuracy: 0.8166 - val_loss: 1.4687 - val_accuracy: 0.6206\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3760 - accuracy: 0.8189 - val_loss: 1.4199 - val_accuracy: 0.6124\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3811 - accuracy: 0.8203 - val_loss: 1.5326 - val_accuracy: 0.6055\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3829 - accuracy: 0.8207 - val_loss: 1.4922 - val_accuracy: 0.6084\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3774 - accuracy: 0.8201 - val_loss: 1.5309 - val_accuracy: 0.6142\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3641 - accuracy: 0.8232 - val_loss: 1.5470 - val_accuracy: 0.6070\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3688 - accuracy: 0.8243 - val_loss: 1.5771 - val_accuracy: 0.6109\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3655 - accuracy: 0.8234 - val_loss: 1.6144 - val_accuracy: 0.6077\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3652 - accuracy: 0.8234 - val_loss: 1.5590 - val_accuracy: 0.6134\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3632 - accuracy: 0.8283 - val_loss: 1.6995 - val_accuracy: 0.6048\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3770 - accuracy: 0.8215 - val_loss: 1.7186 - val_accuracy: 0.6134\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3613 - accuracy: 0.8287 - val_loss: 1.6811 - val_accuracy: 0.6055\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3610 - accuracy: 0.8225 - val_loss: 1.5632 - val_accuracy: 0.6009\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3581 - accuracy: 0.8306 - val_loss: 1.7083 - val_accuracy: 0.6091\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3627 - accuracy: 0.8245 - val_loss: 1.5871 - val_accuracy: 0.6077\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3477 - accuracy: 0.8332 - val_loss: 1.7211 - val_accuracy: 0.6199\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3445 - accuracy: 0.8316 - val_loss: 1.6554 - val_accuracy: 0.6156\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3605 - accuracy: 0.8267 - val_loss: 1.6576 - val_accuracy: 0.6037\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3536 - accuracy: 0.8314 - val_loss: 1.5481 - val_accuracy: 0.6084\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3438 - accuracy: 0.8347 - val_loss: 1.7105 - val_accuracy: 0.6084\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3406 - accuracy: 0.8331 - val_loss: 1.6595 - val_accuracy: 0.5998\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3358 - accuracy: 0.8367 - val_loss: 1.7224 - val_accuracy: 0.6188\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3381 - accuracy: 0.8339 - val_loss: 1.8073 - val_accuracy: 0.6081\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3344 - accuracy: 0.8371 - val_loss: 1.8048 - val_accuracy: 0.6052\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3376 - accuracy: 0.8393 - val_loss: 1.6687 - val_accuracy: 0.6030\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3313 - accuracy: 0.8430 - val_loss: 1.9245 - val_accuracy: 0.6034\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3464 - accuracy: 0.8346 - val_loss: 1.6418 - val_accuracy: 0.6009\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3377 - accuracy: 0.8389 - val_loss: 1.7533 - val_accuracy: 0.6145\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3252 - accuracy: 0.8384 - val_loss: 1.8080 - val_accuracy: 0.5991\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3164 - accuracy: 0.8440 - val_loss: 1.9369 - val_accuracy: 0.6045\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3266 - accuracy: 0.8405 - val_loss: 1.7410 - val_accuracy: 0.6077\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3302 - accuracy: 0.8404 - val_loss: 1.7946 - val_accuracy: 0.6016\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3289 - accuracy: 0.8435 - val_loss: 1.6953 - val_accuracy: 0.6070\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3251 - accuracy: 0.8422 - val_loss: 1.8562 - val_accuracy: 0.6023\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3392 - accuracy: 0.8423 - val_loss: 1.7990 - val_accuracy: 0.6016\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3262 - accuracy: 0.8451 - val_loss: 1.9080 - val_accuracy: 0.5955\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3156 - accuracy: 0.8452 - val_loss: 1.9747 - val_accuracy: 0.5987\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3114 - accuracy: 0.8474 - val_loss: 2.1245 - val_accuracy: 0.5840\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3203 - accuracy: 0.8422 - val_loss: 1.9740 - val_accuracy: 0.6091\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3099 - accuracy: 0.8441 - val_loss: 1.9262 - val_accuracy: 0.5976\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3128 - accuracy: 0.8467 - val_loss: 1.9757 - val_accuracy: 0.5890\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3112 - accuracy: 0.8466 - val_loss: 2.0636 - val_accuracy: 0.6063\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3109 - accuracy: 0.8478 - val_loss: 2.0151 - val_accuracy: 0.6005\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3173 - accuracy: 0.8471 - val_loss: 2.0646 - val_accuracy: 0.5948\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3268 - accuracy: 0.8422 - val_loss: 2.0329 - val_accuracy: 0.5940\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3141 - accuracy: 0.8489 - val_loss: 1.9393 - val_accuracy: 0.6034\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3080 - accuracy: 0.8494 - val_loss: 1.9818 - val_accuracy: 0.5926\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3024 - accuracy: 0.8491 - val_loss: 2.1491 - val_accuracy: 0.5904\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2975 - accuracy: 0.8529 - val_loss: 2.0260 - val_accuracy: 0.5937\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3092 - accuracy: 0.8456 - val_loss: 1.9674 - val_accuracy: 0.6063\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2950 - accuracy: 0.8543 - val_loss: 2.1803 - val_accuracy: 0.6045\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2981 - accuracy: 0.8545 - val_loss: 2.1642 - val_accuracy: 0.5987\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3150 - accuracy: 0.8474 - val_loss: 1.9916 - val_accuracy: 0.5980\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3116 - accuracy: 0.8488 - val_loss: 2.1395 - val_accuracy: 0.5944\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2902 - accuracy: 0.8552 - val_loss: 2.1311 - val_accuracy: 0.5940\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2909 - accuracy: 0.8567 - val_loss: 1.9203 - val_accuracy: 0.5926\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2993 - accuracy: 0.8561 - val_loss: 2.0799 - val_accuracy: 0.6019\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3090 - accuracy: 0.8499 - val_loss: 2.0220 - val_accuracy: 0.5897\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3131 - accuracy: 0.8483 - val_loss: 1.9995 - val_accuracy: 0.5904\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3042 - accuracy: 0.8528 - val_loss: 2.0735 - val_accuracy: 0.6030\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2833 - accuracy: 0.8586 - val_loss: 2.2191 - val_accuracy: 0.5894\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2787 - accuracy: 0.8601 - val_loss: 2.1694 - val_accuracy: 0.6081\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2873 - accuracy: 0.8621 - val_loss: 2.1945 - val_accuracy: 0.6117\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2880 - accuracy: 0.8566 - val_loss: 2.2382 - val_accuracy: 0.6088\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2828 - accuracy: 0.8602 - val_loss: 2.1059 - val_accuracy: 0.6005\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2895 - accuracy: 0.8552 - val_loss: 2.2594 - val_accuracy: 0.5908\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2897 - accuracy: 0.8606 - val_loss: 2.2495 - val_accuracy: 0.5980\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2841 - accuracy: 0.8574 - val_loss: 2.3014 - val_accuracy: 0.6005\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2958 - accuracy: 0.8520 - val_loss: 2.2387 - val_accuracy: 0.5930\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2851 - accuracy: 0.8604 - val_loss: 2.3309 - val_accuracy: 0.5915\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2786 - accuracy: 0.8605 - val_loss: 2.2708 - val_accuracy: 0.5872\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2869 - accuracy: 0.8605 - val_loss: 2.3022 - val_accuracy: 0.5908\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2954 - accuracy: 0.8568 - val_loss: 2.0979 - val_accuracy: 0.6016\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2761 - accuracy: 0.8620 - val_loss: 2.2639 - val_accuracy: 0.5962\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2753 - accuracy: 0.8616 - val_loss: 2.3238 - val_accuracy: 0.5940\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2728 - accuracy: 0.8637 - val_loss: 2.4170 - val_accuracy: 0.5865\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2720 - accuracy: 0.8646 - val_loss: 2.2680 - val_accuracy: 0.6005\n",
            "History for model 7: <keras.src.callbacks.History object at 0x7b093d094e20>\n",
            "Trial 8: Number of layers = 4, Number of neurons per layer = [100, 110, 120, 130]\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.8140 - accuracy: 0.6125 - val_loss: 0.7780 - val_accuracy: 0.6242\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7648 - accuracy: 0.6389 - val_loss: 0.7635 - val_accuracy: 0.6383\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7469 - accuracy: 0.6455 - val_loss: 0.7571 - val_accuracy: 0.6264\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7321 - accuracy: 0.6574 - val_loss: 0.7687 - val_accuracy: 0.6379\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.7232 - accuracy: 0.6570 - val_loss: 0.7693 - val_accuracy: 0.6437\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7110 - accuracy: 0.6665 - val_loss: 0.7541 - val_accuracy: 0.6472\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7037 - accuracy: 0.6676 - val_loss: 0.7537 - val_accuracy: 0.6415\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6937 - accuracy: 0.6722 - val_loss: 0.7649 - val_accuracy: 0.6487\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6832 - accuracy: 0.6798 - val_loss: 0.7649 - val_accuracy: 0.6429\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6721 - accuracy: 0.6846 - val_loss: 0.7528 - val_accuracy: 0.6444\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6630 - accuracy: 0.6843 - val_loss: 0.7492 - val_accuracy: 0.6440\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6547 - accuracy: 0.6933 - val_loss: 0.7715 - val_accuracy: 0.6548\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6450 - accuracy: 0.6924 - val_loss: 0.7660 - val_accuracy: 0.6455\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6370 - accuracy: 0.6991 - val_loss: 0.7763 - val_accuracy: 0.6501\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.7065 - val_loss: 0.7857 - val_accuracy: 0.6534\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6159 - accuracy: 0.7122 - val_loss: 0.7920 - val_accuracy: 0.6397\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6057 - accuracy: 0.7186 - val_loss: 0.7980 - val_accuracy: 0.6440\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6002 - accuracy: 0.7164 - val_loss: 0.8232 - val_accuracy: 0.6411\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5940 - accuracy: 0.7240 - val_loss: 0.8278 - val_accuracy: 0.6286\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5841 - accuracy: 0.7277 - val_loss: 0.8833 - val_accuracy: 0.6455\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5746 - accuracy: 0.7336 - val_loss: 0.8355 - val_accuracy: 0.6408\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5650 - accuracy: 0.7355 - val_loss: 0.8855 - val_accuracy: 0.6386\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5621 - accuracy: 0.7400 - val_loss: 0.8546 - val_accuracy: 0.6433\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5531 - accuracy: 0.7422 - val_loss: 0.9414 - val_accuracy: 0.6303\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5477 - accuracy: 0.7441 - val_loss: 0.9177 - val_accuracy: 0.6404\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5401 - accuracy: 0.7457 - val_loss: 0.9306 - val_accuracy: 0.6336\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5325 - accuracy: 0.7492 - val_loss: 0.9443 - val_accuracy: 0.6354\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5280 - accuracy: 0.7524 - val_loss: 0.9769 - val_accuracy: 0.6300\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5227 - accuracy: 0.7554 - val_loss: 0.9582 - val_accuracy: 0.6433\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.5166 - accuracy: 0.7638 - val_loss: 0.9712 - val_accuracy: 0.6303\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5077 - accuracy: 0.7594 - val_loss: 0.9864 - val_accuracy: 0.6361\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4953 - accuracy: 0.7639 - val_loss: 1.0963 - val_accuracy: 0.6275\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4938 - accuracy: 0.7673 - val_loss: 1.0159 - val_accuracy: 0.6232\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5012 - accuracy: 0.7623 - val_loss: 0.9811 - val_accuracy: 0.6325\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4870 - accuracy: 0.7730 - val_loss: 1.0734 - val_accuracy: 0.6282\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4730 - accuracy: 0.7771 - val_loss: 1.1069 - val_accuracy: 0.6221\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4705 - accuracy: 0.7773 - val_loss: 1.1545 - val_accuracy: 0.6332\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4701 - accuracy: 0.7769 - val_loss: 1.0454 - val_accuracy: 0.6350\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4705 - accuracy: 0.7761 - val_loss: 1.2193 - val_accuracy: 0.6246\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4564 - accuracy: 0.7828 - val_loss: 1.1382 - val_accuracy: 0.6303\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4469 - accuracy: 0.7835 - val_loss: 1.2213 - val_accuracy: 0.6268\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4565 - accuracy: 0.7864 - val_loss: 1.2218 - val_accuracy: 0.6102\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4482 - accuracy: 0.7885 - val_loss: 1.2696 - val_accuracy: 0.6296\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4363 - accuracy: 0.7919 - val_loss: 1.2998 - val_accuracy: 0.6286\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4367 - accuracy: 0.7937 - val_loss: 1.2767 - val_accuracy: 0.6278\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4316 - accuracy: 0.7913 - val_loss: 1.2799 - val_accuracy: 0.6264\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4262 - accuracy: 0.7953 - val_loss: 1.2828 - val_accuracy: 0.6253\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4246 - accuracy: 0.7961 - val_loss: 1.2783 - val_accuracy: 0.6235\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4190 - accuracy: 0.8004 - val_loss: 1.3185 - val_accuracy: 0.6361\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.4168 - accuracy: 0.8010 - val_loss: 1.3482 - val_accuracy: 0.6199\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4233 - accuracy: 0.8015 - val_loss: 1.3252 - val_accuracy: 0.6206\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4082 - accuracy: 0.8054 - val_loss: 1.4004 - val_accuracy: 0.6181\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3950 - accuracy: 0.8101 - val_loss: 1.4456 - val_accuracy: 0.6188\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4049 - accuracy: 0.8038 - val_loss: 1.4053 - val_accuracy: 0.6070\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3960 - accuracy: 0.8066 - val_loss: 1.5317 - val_accuracy: 0.6185\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3905 - accuracy: 0.8130 - val_loss: 1.4901 - val_accuracy: 0.6278\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3841 - accuracy: 0.8177 - val_loss: 1.6335 - val_accuracy: 0.6196\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3874 - accuracy: 0.8154 - val_loss: 1.4780 - val_accuracy: 0.6181\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3810 - accuracy: 0.8108 - val_loss: 1.5123 - val_accuracy: 0.6181\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3862 - accuracy: 0.8138 - val_loss: 1.4452 - val_accuracy: 0.6196\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3659 - accuracy: 0.8205 - val_loss: 1.5028 - val_accuracy: 0.6156\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3779 - accuracy: 0.8149 - val_loss: 1.5675 - val_accuracy: 0.6145\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3714 - accuracy: 0.8180 - val_loss: 1.4445 - val_accuracy: 0.6203\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3616 - accuracy: 0.8201 - val_loss: 1.5225 - val_accuracy: 0.6170\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3659 - accuracy: 0.8200 - val_loss: 1.6307 - val_accuracy: 0.6099\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3724 - accuracy: 0.8196 - val_loss: 1.4612 - val_accuracy: 0.6203\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3587 - accuracy: 0.8222 - val_loss: 1.6087 - val_accuracy: 0.6268\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3544 - accuracy: 0.8306 - val_loss: 1.7109 - val_accuracy: 0.6138\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3523 - accuracy: 0.8252 - val_loss: 1.6849 - val_accuracy: 0.6120\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3515 - accuracy: 0.8255 - val_loss: 1.7174 - val_accuracy: 0.6152\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3505 - accuracy: 0.8303 - val_loss: 1.7785 - val_accuracy: 0.6192\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3455 - accuracy: 0.8283 - val_loss: 1.7293 - val_accuracy: 0.6199\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3469 - accuracy: 0.8319 - val_loss: 1.6903 - val_accuracy: 0.6160\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3344 - accuracy: 0.8333 - val_loss: 1.7455 - val_accuracy: 0.6138\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3379 - accuracy: 0.8332 - val_loss: 1.7088 - val_accuracy: 0.6113\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3267 - accuracy: 0.8377 - val_loss: 1.9759 - val_accuracy: 0.6091\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3348 - accuracy: 0.8340 - val_loss: 2.0273 - val_accuracy: 0.6037\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3282 - accuracy: 0.8391 - val_loss: 1.8195 - val_accuracy: 0.6131\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3391 - accuracy: 0.8332 - val_loss: 1.9791 - val_accuracy: 0.6109\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3376 - accuracy: 0.8356 - val_loss: 1.7893 - val_accuracy: 0.6142\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3329 - accuracy: 0.8348 - val_loss: 1.8086 - val_accuracy: 0.6009\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3158 - accuracy: 0.8437 - val_loss: 1.9392 - val_accuracy: 0.5987\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3184 - accuracy: 0.8392 - val_loss: 1.9866 - val_accuracy: 0.5930\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3105 - accuracy: 0.8429 - val_loss: 1.9779 - val_accuracy: 0.6012\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3207 - accuracy: 0.8463 - val_loss: 2.1815 - val_accuracy: 0.6106\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3165 - accuracy: 0.8424 - val_loss: 1.9852 - val_accuracy: 0.6099\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3113 - accuracy: 0.8432 - val_loss: 1.8706 - val_accuracy: 0.6037\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.3107 - accuracy: 0.8468 - val_loss: 2.0011 - val_accuracy: 0.6134\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3210 - accuracy: 0.8443 - val_loss: 1.9839 - val_accuracy: 0.5998\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3152 - accuracy: 0.8438 - val_loss: 2.0540 - val_accuracy: 0.6041\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3104 - accuracy: 0.8451 - val_loss: 1.8822 - val_accuracy: 0.6052\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3055 - accuracy: 0.8494 - val_loss: 1.9050 - val_accuracy: 0.6063\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3056 - accuracy: 0.8533 - val_loss: 1.8195 - val_accuracy: 0.6059\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2939 - accuracy: 0.8509 - val_loss: 2.0572 - val_accuracy: 0.5998\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2889 - accuracy: 0.8492 - val_loss: 2.1624 - val_accuracy: 0.6052\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2931 - accuracy: 0.8532 - val_loss: 2.0846 - val_accuracy: 0.6099\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2969 - accuracy: 0.8492 - val_loss: 2.0193 - val_accuracy: 0.5976\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3113 - accuracy: 0.8466 - val_loss: 1.8785 - val_accuracy: 0.5983\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3007 - accuracy: 0.8485 - val_loss: 2.0080 - val_accuracy: 0.6045\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3178 - accuracy: 0.8474 - val_loss: 1.8389 - val_accuracy: 0.6073\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2801 - accuracy: 0.8585 - val_loss: 2.1983 - val_accuracy: 0.6081\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2804 - accuracy: 0.8597 - val_loss: 2.1010 - val_accuracy: 0.6023\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2815 - accuracy: 0.8588 - val_loss: 2.0947 - val_accuracy: 0.5991\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2913 - accuracy: 0.8551 - val_loss: 2.2591 - val_accuracy: 0.6005\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2893 - accuracy: 0.8572 - val_loss: 2.1047 - val_accuracy: 0.6001\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2834 - accuracy: 0.8554 - val_loss: 2.0708 - val_accuracy: 0.5944\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2800 - accuracy: 0.8598 - val_loss: 2.1982 - val_accuracy: 0.6048\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2856 - accuracy: 0.8545 - val_loss: 2.2612 - val_accuracy: 0.5930\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2773 - accuracy: 0.8622 - val_loss: 2.1030 - val_accuracy: 0.6041\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2694 - accuracy: 0.8615 - val_loss: 2.3012 - val_accuracy: 0.5897\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2697 - accuracy: 0.8587 - val_loss: 2.1718 - val_accuracy: 0.6041\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2865 - accuracy: 0.8599 - val_loss: 2.1942 - val_accuracy: 0.5912\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2803 - accuracy: 0.8602 - val_loss: 2.0682 - val_accuracy: 0.5983\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2757 - accuracy: 0.8638 - val_loss: 2.3704 - val_accuracy: 0.6127\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2834 - accuracy: 0.8566 - val_loss: 2.2815 - val_accuracy: 0.6034\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2745 - accuracy: 0.8612 - val_loss: 2.1675 - val_accuracy: 0.6027\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2689 - accuracy: 0.8594 - val_loss: 2.2348 - val_accuracy: 0.5926\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2664 - accuracy: 0.8670 - val_loss: 2.3937 - val_accuracy: 0.6099\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2782 - accuracy: 0.8604 - val_loss: 2.3293 - val_accuracy: 0.6023\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2693 - accuracy: 0.8620 - val_loss: 2.3788 - val_accuracy: 0.5940\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2673 - accuracy: 0.8661 - val_loss: 2.2252 - val_accuracy: 0.5965\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2646 - accuracy: 0.8657 - val_loss: 2.3363 - val_accuracy: 0.6016\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2641 - accuracy: 0.8661 - val_loss: 2.4678 - val_accuracy: 0.5944\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2565 - accuracy: 0.8701 - val_loss: 2.4281 - val_accuracy: 0.5926\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2557 - accuracy: 0.8672 - val_loss: 2.4279 - val_accuracy: 0.5994\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2632 - accuracy: 0.8710 - val_loss: 2.4310 - val_accuracy: 0.5937\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2528 - accuracy: 0.8688 - val_loss: 2.4490 - val_accuracy: 0.6001\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 1s 3ms/step - loss: 0.2658 - accuracy: 0.8626 - val_loss: 2.3443 - val_accuracy: 0.6041\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2549 - accuracy: 0.8722 - val_loss: 2.5466 - val_accuracy: 0.6066\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2569 - accuracy: 0.8689 - val_loss: 2.6037 - val_accuracy: 0.6117\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2505 - accuracy: 0.8683 - val_loss: 2.5965 - val_accuracy: 0.5879\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2732 - accuracy: 0.8651 - val_loss: 2.3949 - val_accuracy: 0.5969\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2650 - accuracy: 0.8716 - val_loss: 2.4624 - val_accuracy: 0.5973\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2524 - accuracy: 0.8693 - val_loss: 2.5755 - val_accuracy: 0.5868\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2588 - accuracy: 0.8738 - val_loss: 2.3970 - val_accuracy: 0.5951\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2623 - accuracy: 0.8715 - val_loss: 2.2179 - val_accuracy: 0.5847\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2510 - accuracy: 0.8759 - val_loss: 2.6119 - val_accuracy: 0.5951\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2447 - accuracy: 0.8747 - val_loss: 2.4855 - val_accuracy: 0.5994\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2381 - accuracy: 0.8761 - val_loss: 2.7618 - val_accuracy: 0.5872\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2608 - accuracy: 0.8700 - val_loss: 2.5901 - val_accuracy: 0.6073\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2451 - accuracy: 0.8742 - val_loss: 2.5171 - val_accuracy: 0.5890\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2437 - accuracy: 0.8752 - val_loss: 2.7953 - val_accuracy: 0.5965\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2328 - accuracy: 0.8773 - val_loss: 2.7715 - val_accuracy: 0.5926\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2478 - accuracy: 0.8738 - val_loss: 2.5719 - val_accuracy: 0.5937\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2519 - accuracy: 0.8715 - val_loss: 2.7016 - val_accuracy: 0.5983\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2317 - accuracy: 0.8811 - val_loss: 2.5763 - val_accuracy: 0.5904\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2497 - accuracy: 0.8756 - val_loss: 2.4617 - val_accuracy: 0.5933\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2354 - accuracy: 0.8765 - val_loss: 2.7985 - val_accuracy: 0.5980\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2417 - accuracy: 0.8753 - val_loss: 2.5558 - val_accuracy: 0.5922\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2635 - accuracy: 0.8725 - val_loss: 2.4386 - val_accuracy: 0.5886\n",
            "History for model 8: <keras.src.callbacks.History object at 0x7b0939ac3520>\n",
            "Trial 9: Number of layers = 4, Number of neurons per layer = [140, 150, 160, 170]\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 4s 8ms/step - loss: 0.8192 - accuracy: 0.6082 - val_loss: 0.7778 - val_accuracy: 0.6275\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7648 - accuracy: 0.6387 - val_loss: 0.7579 - val_accuracy: 0.6343\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.7502 - accuracy: 0.6455 - val_loss: 0.7715 - val_accuracy: 0.6447\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7357 - accuracy: 0.6505 - val_loss: 0.7743 - val_accuracy: 0.6311\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7238 - accuracy: 0.6574 - val_loss: 0.7533 - val_accuracy: 0.6372\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7123 - accuracy: 0.6653 - val_loss: 0.7577 - val_accuracy: 0.6354\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7005 - accuracy: 0.6714 - val_loss: 0.7585 - val_accuracy: 0.6458\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6931 - accuracy: 0.6727 - val_loss: 0.7476 - val_accuracy: 0.6530\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.6813 - accuracy: 0.6802 - val_loss: 0.7642 - val_accuracy: 0.6429\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6713 - accuracy: 0.6834 - val_loss: 0.7588 - val_accuracy: 0.6476\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6607 - accuracy: 0.6913 - val_loss: 0.7951 - val_accuracy: 0.6383\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6474 - accuracy: 0.6995 - val_loss: 0.7706 - val_accuracy: 0.6476\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6431 - accuracy: 0.6983 - val_loss: 0.8075 - val_accuracy: 0.6415\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6322 - accuracy: 0.6990 - val_loss: 0.7710 - val_accuracy: 0.6368\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6188 - accuracy: 0.7115 - val_loss: 0.7976 - val_accuracy: 0.6490\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6103 - accuracy: 0.7140 - val_loss: 0.8367 - val_accuracy: 0.6357\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6046 - accuracy: 0.7161 - val_loss: 0.7943 - val_accuracy: 0.6455\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5957 - accuracy: 0.7181 - val_loss: 0.7967 - val_accuracy: 0.6419\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5869 - accuracy: 0.7273 - val_loss: 0.8167 - val_accuracy: 0.6437\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5844 - accuracy: 0.7276 - val_loss: 0.8334 - val_accuracy: 0.6433\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5656 - accuracy: 0.7367 - val_loss: 0.8646 - val_accuracy: 0.6357\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5613 - accuracy: 0.7394 - val_loss: 0.8620 - val_accuracy: 0.6368\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5469 - accuracy: 0.7446 - val_loss: 0.8746 - val_accuracy: 0.6329\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5426 - accuracy: 0.7485 - val_loss: 0.8742 - val_accuracy: 0.6426\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5342 - accuracy: 0.7487 - val_loss: 0.9418 - val_accuracy: 0.6390\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5214 - accuracy: 0.7596 - val_loss: 0.9519 - val_accuracy: 0.6347\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5199 - accuracy: 0.7566 - val_loss: 0.9107 - val_accuracy: 0.6289\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5139 - accuracy: 0.7533 - val_loss: 0.9957 - val_accuracy: 0.6375\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5095 - accuracy: 0.7598 - val_loss: 0.9940 - val_accuracy: 0.6311\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5046 - accuracy: 0.7658 - val_loss: 1.0096 - val_accuracy: 0.6343\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4923 - accuracy: 0.7712 - val_loss: 1.0323 - val_accuracy: 0.6354\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4810 - accuracy: 0.7729 - val_loss: 1.1335 - val_accuracy: 0.6350\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4787 - accuracy: 0.7711 - val_loss: 1.1376 - val_accuracy: 0.6339\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4706 - accuracy: 0.7813 - val_loss: 1.1054 - val_accuracy: 0.6404\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4754 - accuracy: 0.7745 - val_loss: 1.0171 - val_accuracy: 0.6383\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4718 - accuracy: 0.7766 - val_loss: 1.0782 - val_accuracy: 0.6214\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4611 - accuracy: 0.7811 - val_loss: 1.0643 - val_accuracy: 0.6329\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4358 - accuracy: 0.7882 - val_loss: 1.2313 - val_accuracy: 0.6325\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4440 - accuracy: 0.7895 - val_loss: 1.1942 - val_accuracy: 0.6271\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4341 - accuracy: 0.7957 - val_loss: 1.3190 - val_accuracy: 0.6235\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4223 - accuracy: 0.7931 - val_loss: 1.2690 - val_accuracy: 0.6188\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4324 - accuracy: 0.7934 - val_loss: 1.2901 - val_accuracy: 0.6303\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4237 - accuracy: 0.7962 - val_loss: 1.2327 - val_accuracy: 0.6145\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4191 - accuracy: 0.7988 - val_loss: 1.3206 - val_accuracy: 0.6343\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4090 - accuracy: 0.8021 - val_loss: 1.3519 - val_accuracy: 0.6117\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4090 - accuracy: 0.8033 - val_loss: 1.4008 - val_accuracy: 0.6178\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4025 - accuracy: 0.8060 - val_loss: 1.3752 - val_accuracy: 0.6149\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4000 - accuracy: 0.8076 - val_loss: 1.4365 - val_accuracy: 0.6286\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3939 - accuracy: 0.8100 - val_loss: 1.5118 - val_accuracy: 0.6102\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3821 - accuracy: 0.8141 - val_loss: 1.6313 - val_accuracy: 0.6152\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3843 - accuracy: 0.8172 - val_loss: 1.5274 - val_accuracy: 0.6149\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3823 - accuracy: 0.8167 - val_loss: 1.4745 - val_accuracy: 0.6235\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3902 - accuracy: 0.8136 - val_loss: 1.5287 - val_accuracy: 0.6188\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3824 - accuracy: 0.8094 - val_loss: 1.4592 - val_accuracy: 0.6178\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3753 - accuracy: 0.8168 - val_loss: 1.6241 - val_accuracy: 0.6188\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3660 - accuracy: 0.8201 - val_loss: 1.6899 - val_accuracy: 0.6214\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3612 - accuracy: 0.8255 - val_loss: 1.5619 - val_accuracy: 0.6059\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3703 - accuracy: 0.8212 - val_loss: 1.5950 - val_accuracy: 0.6152\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3770 - accuracy: 0.8148 - val_loss: 1.4721 - val_accuracy: 0.6178\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3525 - accuracy: 0.8244 - val_loss: 1.6644 - val_accuracy: 0.6052\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3485 - accuracy: 0.8263 - val_loss: 1.7111 - val_accuracy: 0.6070\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3470 - accuracy: 0.8272 - val_loss: 1.5511 - val_accuracy: 0.6149\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3376 - accuracy: 0.8323 - val_loss: 1.7680 - val_accuracy: 0.6037\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3278 - accuracy: 0.8361 - val_loss: 1.8519 - val_accuracy: 0.6203\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3334 - accuracy: 0.8350 - val_loss: 1.7347 - val_accuracy: 0.6102\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3438 - accuracy: 0.8310 - val_loss: 1.8004 - val_accuracy: 0.6214\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3494 - accuracy: 0.8296 - val_loss: 1.7287 - val_accuracy: 0.6088\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3247 - accuracy: 0.8367 - val_loss: 1.8503 - val_accuracy: 0.6160\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3199 - accuracy: 0.8381 - val_loss: 1.8307 - val_accuracy: 0.6077\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3346 - accuracy: 0.8350 - val_loss: 1.7471 - val_accuracy: 0.6077\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3281 - accuracy: 0.8364 - val_loss: 1.8072 - val_accuracy: 0.6109\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3185 - accuracy: 0.8390 - val_loss: 1.8603 - val_accuracy: 0.6059\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.3135 - accuracy: 0.8414 - val_loss: 1.9601 - val_accuracy: 0.6077\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3148 - accuracy: 0.8427 - val_loss: 1.8916 - val_accuracy: 0.6217\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3044 - accuracy: 0.8453 - val_loss: 2.0275 - val_accuracy: 0.6113\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3147 - accuracy: 0.8409 - val_loss: 1.9422 - val_accuracy: 0.5958\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3052 - accuracy: 0.8467 - val_loss: 2.0853 - val_accuracy: 0.6070\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3109 - accuracy: 0.8456 - val_loss: 1.9907 - val_accuracy: 0.6156\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3172 - accuracy: 0.8480 - val_loss: 1.9764 - val_accuracy: 0.6109\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2969 - accuracy: 0.8490 - val_loss: 2.1700 - val_accuracy: 0.6131\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2971 - accuracy: 0.8491 - val_loss: 2.1452 - val_accuracy: 0.6181\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3021 - accuracy: 0.8449 - val_loss: 2.1417 - val_accuracy: 0.6163\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3091 - accuracy: 0.8501 - val_loss: 2.1150 - val_accuracy: 0.6041\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2925 - accuracy: 0.8491 - val_loss: 2.0878 - val_accuracy: 0.6095\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2729 - accuracy: 0.8584 - val_loss: 2.4212 - val_accuracy: 0.6063\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2992 - accuracy: 0.8499 - val_loss: 1.9813 - val_accuracy: 0.6099\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2769 - accuracy: 0.8572 - val_loss: 2.1557 - val_accuracy: 0.5962\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2866 - accuracy: 0.8568 - val_loss: 2.1274 - val_accuracy: 0.5987\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2843 - accuracy: 0.8572 - val_loss: 2.2986 - val_accuracy: 0.6077\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2910 - accuracy: 0.8531 - val_loss: 1.9520 - val_accuracy: 0.6070\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2865 - accuracy: 0.8544 - val_loss: 2.3102 - val_accuracy: 0.6070\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2881 - accuracy: 0.8554 - val_loss: 2.0488 - val_accuracy: 0.6052\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2649 - accuracy: 0.8622 - val_loss: 2.6586 - val_accuracy: 0.6019\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2648 - accuracy: 0.8622 - val_loss: 2.4663 - val_accuracy: 0.5955\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2824 - accuracy: 0.8567 - val_loss: 2.1731 - val_accuracy: 0.5922\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2927 - accuracy: 0.8553 - val_loss: 2.1161 - val_accuracy: 0.5965\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2734 - accuracy: 0.8594 - val_loss: 2.2956 - val_accuracy: 0.6019\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2661 - accuracy: 0.8600 - val_loss: 2.4064 - val_accuracy: 0.6113\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2630 - accuracy: 0.8619 - val_loss: 2.1220 - val_accuracy: 0.6095\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2684 - accuracy: 0.8642 - val_loss: 2.2813 - val_accuracy: 0.6156\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2623 - accuracy: 0.8626 - val_loss: 2.5667 - val_accuracy: 0.6070\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2773 - accuracy: 0.8641 - val_loss: 2.2797 - val_accuracy: 0.5991\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2693 - accuracy: 0.8644 - val_loss: 2.4773 - val_accuracy: 0.6081\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2752 - accuracy: 0.8602 - val_loss: 2.2092 - val_accuracy: 0.5922\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2630 - accuracy: 0.8671 - val_loss: 2.1621 - val_accuracy: 0.5973\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2505 - accuracy: 0.8703 - val_loss: 2.4915 - val_accuracy: 0.5969\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2468 - accuracy: 0.8720 - val_loss: 2.6640 - val_accuracy: 0.6041\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2618 - accuracy: 0.8671 - val_loss: 2.4133 - val_accuracy: 0.5969\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2678 - accuracy: 0.8647 - val_loss: 2.4589 - val_accuracy: 0.5965\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2500 - accuracy: 0.8716 - val_loss: 2.2776 - val_accuracy: 0.6016\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2514 - accuracy: 0.8709 - val_loss: 2.2894 - val_accuracy: 0.5958\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2575 - accuracy: 0.8673 - val_loss: 2.3451 - val_accuracy: 0.5987\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2558 - accuracy: 0.8681 - val_loss: 2.6110 - val_accuracy: 0.6073\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2463 - accuracy: 0.8711 - val_loss: 2.5259 - val_accuracy: 0.5951\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2465 - accuracy: 0.8727 - val_loss: 2.5052 - val_accuracy: 0.6095\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2619 - accuracy: 0.8673 - val_loss: 2.3142 - val_accuracy: 0.5807\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2561 - accuracy: 0.8673 - val_loss: 2.4814 - val_accuracy: 0.5919\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2422 - accuracy: 0.8736 - val_loss: 2.6817 - val_accuracy: 0.6052\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2351 - accuracy: 0.8756 - val_loss: 2.7629 - val_accuracy: 0.5933\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2387 - accuracy: 0.8748 - val_loss: 2.5519 - val_accuracy: 0.6063\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2520 - accuracy: 0.8727 - val_loss: 2.5697 - val_accuracy: 0.5983\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2430 - accuracy: 0.8750 - val_loss: 2.7753 - val_accuracy: 0.6012\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2480 - accuracy: 0.8757 - val_loss: 2.5980 - val_accuracy: 0.5955\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2337 - accuracy: 0.8762 - val_loss: 2.7702 - val_accuracy: 0.5994\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2332 - accuracy: 0.8797 - val_loss: 2.6820 - val_accuracy: 0.6012\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2539 - accuracy: 0.8730 - val_loss: 2.3652 - val_accuracy: 0.5847\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2337 - accuracy: 0.8812 - val_loss: 2.5798 - val_accuracy: 0.5850\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2425 - accuracy: 0.8744 - val_loss: 2.6596 - val_accuracy: 0.5926\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2286 - accuracy: 0.8789 - val_loss: 2.6889 - val_accuracy: 0.5948\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2210 - accuracy: 0.8837 - val_loss: 2.9489 - val_accuracy: 0.5822\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2321 - accuracy: 0.8828 - val_loss: 2.6040 - val_accuracy: 0.6001\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2395 - accuracy: 0.8765 - val_loss: 2.6368 - val_accuracy: 0.5904\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2127 - accuracy: 0.8878 - val_loss: 2.9041 - val_accuracy: 0.5872\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2474 - accuracy: 0.8768 - val_loss: 2.3630 - val_accuracy: 0.6005\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2390 - accuracy: 0.8782 - val_loss: 2.5728 - val_accuracy: 0.5886\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2309 - accuracy: 0.8808 - val_loss: 2.6461 - val_accuracy: 0.5919\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2428 - accuracy: 0.8775 - val_loss: 2.4871 - val_accuracy: 0.5890\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2376 - accuracy: 0.8772 - val_loss: 2.4122 - val_accuracy: 0.6001\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2185 - accuracy: 0.8833 - val_loss: 2.7254 - val_accuracy: 0.6016\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2224 - accuracy: 0.8832 - val_loss: 2.6003 - val_accuracy: 0.6001\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2138 - accuracy: 0.8858 - val_loss: 2.8611 - val_accuracy: 0.5865\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2110 - accuracy: 0.8867 - val_loss: 2.8894 - val_accuracy: 0.6034\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2193 - accuracy: 0.8838 - val_loss: 2.9349 - val_accuracy: 0.5847\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2455 - accuracy: 0.8741 - val_loss: 2.7952 - val_accuracy: 0.6001\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2243 - accuracy: 0.8832 - val_loss: 2.8295 - val_accuracy: 0.5973\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2151 - accuracy: 0.8865 - val_loss: 2.7172 - val_accuracy: 0.5796\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2232 - accuracy: 0.8836 - val_loss: 2.7180 - val_accuracy: 0.5858\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2362 - accuracy: 0.8775 - val_loss: 2.8173 - val_accuracy: 0.5879\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2197 - accuracy: 0.8833 - val_loss: 2.9004 - val_accuracy: 0.5998\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.2246 - accuracy: 0.8823 - val_loss: 2.6928 - val_accuracy: 0.5987\n",
            "History for model 9: <keras.src.callbacks.History object at 0x7b09399ac190>\n",
            "Trial 10: Number of layers = 4, Number of neurons per layer = [180, 190, 200, 210]\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 3s 6ms/step - loss: 0.8201 - accuracy: 0.6002 - val_loss: 0.7689 - val_accuracy: 0.6411\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7676 - accuracy: 0.6331 - val_loss: 0.7669 - val_accuracy: 0.6278\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7495 - accuracy: 0.6437 - val_loss: 0.7601 - val_accuracy: 0.6458\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7403 - accuracy: 0.6522 - val_loss: 0.7518 - val_accuracy: 0.6437\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7248 - accuracy: 0.6599 - val_loss: 0.7579 - val_accuracy: 0.6455\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7166 - accuracy: 0.6616 - val_loss: 0.7655 - val_accuracy: 0.6415\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7038 - accuracy: 0.6691 - val_loss: 0.7611 - val_accuracy: 0.6404\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6927 - accuracy: 0.6700 - val_loss: 0.7701 - val_accuracy: 0.6537\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6791 - accuracy: 0.6772 - val_loss: 0.7620 - val_accuracy: 0.6440\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6748 - accuracy: 0.6801 - val_loss: 0.7605 - val_accuracy: 0.6444\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6627 - accuracy: 0.6882 - val_loss: 0.7651 - val_accuracy: 0.6526\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6483 - accuracy: 0.6964 - val_loss: 0.7925 - val_accuracy: 0.6433\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6378 - accuracy: 0.7024 - val_loss: 0.7899 - val_accuracy: 0.6483\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6310 - accuracy: 0.7076 - val_loss: 0.7799 - val_accuracy: 0.6447\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6177 - accuracy: 0.7115 - val_loss: 0.8004 - val_accuracy: 0.6494\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6086 - accuracy: 0.7154 - val_loss: 0.8140 - val_accuracy: 0.6372\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5987 - accuracy: 0.7201 - val_loss: 0.8541 - val_accuracy: 0.6296\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5908 - accuracy: 0.7213 - val_loss: 0.8907 - val_accuracy: 0.6401\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5776 - accuracy: 0.7311 - val_loss: 0.8740 - val_accuracy: 0.6350\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5658 - accuracy: 0.7376 - val_loss: 0.9057 - val_accuracy: 0.6242\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5659 - accuracy: 0.7307 - val_loss: 0.8864 - val_accuracy: 0.6393\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5534 - accuracy: 0.7392 - val_loss: 0.9562 - val_accuracy: 0.6487\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5425 - accuracy: 0.7452 - val_loss: 0.9610 - val_accuracy: 0.6498\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5350 - accuracy: 0.7522 - val_loss: 0.9642 - val_accuracy: 0.6386\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5338 - accuracy: 0.7502 - val_loss: 0.9244 - val_accuracy: 0.6206\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5123 - accuracy: 0.7593 - val_loss: 0.9931 - val_accuracy: 0.6419\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5048 - accuracy: 0.7576 - val_loss: 1.0543 - val_accuracy: 0.6336\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5020 - accuracy: 0.7643 - val_loss: 1.0027 - val_accuracy: 0.6264\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4968 - accuracy: 0.7677 - val_loss: 1.1356 - val_accuracy: 0.6375\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4841 - accuracy: 0.7721 - val_loss: 1.0618 - val_accuracy: 0.6343\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4859 - accuracy: 0.7731 - val_loss: 1.1234 - val_accuracy: 0.6257\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4756 - accuracy: 0.7754 - val_loss: 1.1110 - val_accuracy: 0.6293\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4667 - accuracy: 0.7781 - val_loss: 1.1814 - val_accuracy: 0.6192\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4623 - accuracy: 0.7838 - val_loss: 1.2183 - val_accuracy: 0.6303\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4536 - accuracy: 0.7872 - val_loss: 1.1421 - val_accuracy: 0.6235\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4588 - accuracy: 0.7872 - val_loss: 1.2261 - val_accuracy: 0.6221\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4434 - accuracy: 0.7910 - val_loss: 1.3355 - val_accuracy: 0.6257\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4278 - accuracy: 0.7961 - val_loss: 1.2965 - val_accuracy: 0.6235\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4415 - accuracy: 0.7914 - val_loss: 1.2163 - val_accuracy: 0.6307\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4222 - accuracy: 0.7977 - val_loss: 1.3369 - val_accuracy: 0.6289\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4249 - accuracy: 0.7949 - val_loss: 1.3443 - val_accuracy: 0.6232\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4096 - accuracy: 0.8027 - val_loss: 1.5042 - val_accuracy: 0.6188\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4241 - accuracy: 0.8019 - val_loss: 1.2743 - val_accuracy: 0.6149\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4071 - accuracy: 0.8061 - val_loss: 1.4270 - val_accuracy: 0.6196\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3936 - accuracy: 0.8072 - val_loss: 1.4301 - val_accuracy: 0.6246\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4020 - accuracy: 0.8067 - val_loss: 1.4409 - val_accuracy: 0.6224\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3998 - accuracy: 0.8083 - val_loss: 1.3917 - val_accuracy: 0.6239\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3919 - accuracy: 0.8148 - val_loss: 1.4586 - val_accuracy: 0.6156\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3791 - accuracy: 0.8155 - val_loss: 1.4688 - val_accuracy: 0.6199\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3822 - accuracy: 0.8181 - val_loss: 1.5701 - val_accuracy: 0.6239\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3863 - accuracy: 0.8114 - val_loss: 1.5539 - val_accuracy: 0.6192\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3686 - accuracy: 0.8230 - val_loss: 1.7209 - val_accuracy: 0.6253\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.3636 - accuracy: 0.8218 - val_loss: 1.6063 - val_accuracy: 0.6163\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3656 - accuracy: 0.8239 - val_loss: 1.7786 - val_accuracy: 0.6178\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3584 - accuracy: 0.8263 - val_loss: 1.7270 - val_accuracy: 0.6127\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3686 - accuracy: 0.8224 - val_loss: 1.5496 - val_accuracy: 0.6192\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3499 - accuracy: 0.8300 - val_loss: 1.7533 - val_accuracy: 0.6088\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3531 - accuracy: 0.8253 - val_loss: 1.8182 - val_accuracy: 0.6113\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3539 - accuracy: 0.8287 - val_loss: 1.8099 - val_accuracy: 0.6055\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3407 - accuracy: 0.8301 - val_loss: 1.9232 - val_accuracy: 0.6138\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3386 - accuracy: 0.8346 - val_loss: 1.8207 - val_accuracy: 0.6077\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3399 - accuracy: 0.8345 - val_loss: 1.7967 - val_accuracy: 0.6052\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3383 - accuracy: 0.8324 - val_loss: 1.7951 - val_accuracy: 0.6120\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3293 - accuracy: 0.8390 - val_loss: 1.9759 - val_accuracy: 0.6073\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.3176 - accuracy: 0.8427 - val_loss: 1.9650 - val_accuracy: 0.5930\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3272 - accuracy: 0.8388 - val_loss: 1.9388 - val_accuracy: 0.6070\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3441 - accuracy: 0.8345 - val_loss: 1.8960 - val_accuracy: 0.6095\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3163 - accuracy: 0.8484 - val_loss: 1.8754 - val_accuracy: 0.6203\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3230 - accuracy: 0.8417 - val_loss: 2.0207 - val_accuracy: 0.6073\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3149 - accuracy: 0.8440 - val_loss: 1.8873 - val_accuracy: 0.5998\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3158 - accuracy: 0.8451 - val_loss: 2.0557 - val_accuracy: 0.6030\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3054 - accuracy: 0.8477 - val_loss: 2.2222 - val_accuracy: 0.5973\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3033 - accuracy: 0.8510 - val_loss: 2.0927 - val_accuracy: 0.5958\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3037 - accuracy: 0.8477 - val_loss: 2.5380 - val_accuracy: 0.6009\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3094 - accuracy: 0.8484 - val_loss: 2.1920 - val_accuracy: 0.5969\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3075 - accuracy: 0.8476 - val_loss: 2.0890 - val_accuracy: 0.6109\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3106 - accuracy: 0.8486 - val_loss: 2.1206 - val_accuracy: 0.6030\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3108 - accuracy: 0.8473 - val_loss: 2.0057 - val_accuracy: 0.5937\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2893 - accuracy: 0.8554 - val_loss: 2.1839 - val_accuracy: 0.5958\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2958 - accuracy: 0.8554 - val_loss: 2.3046 - val_accuracy: 0.6063\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2815 - accuracy: 0.8633 - val_loss: 2.4112 - val_accuracy: 0.5980\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2945 - accuracy: 0.8525 - val_loss: 2.3400 - val_accuracy: 0.5951\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2920 - accuracy: 0.8538 - val_loss: 2.2980 - val_accuracy: 0.5926\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2825 - accuracy: 0.8606 - val_loss: 2.3325 - val_accuracy: 0.5919\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2730 - accuracy: 0.8603 - val_loss: 2.2636 - val_accuracy: 0.6138\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2769 - accuracy: 0.8614 - val_loss: 2.3690 - val_accuracy: 0.5919\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2791 - accuracy: 0.8589 - val_loss: 2.3437 - val_accuracy: 0.6027\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2730 - accuracy: 0.8607 - val_loss: 2.5256 - val_accuracy: 0.5872\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2871 - accuracy: 0.8597 - val_loss: 2.2959 - val_accuracy: 0.5991\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2684 - accuracy: 0.8667 - val_loss: 2.5960 - val_accuracy: 0.5933\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2748 - accuracy: 0.8597 - val_loss: 2.4002 - val_accuracy: 0.6001\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2860 - accuracy: 0.8583 - val_loss: 2.3773 - val_accuracy: 0.6070\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2799 - accuracy: 0.8616 - val_loss: 2.3623 - val_accuracy: 0.6012\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2510 - accuracy: 0.8705 - val_loss: 2.6387 - val_accuracy: 0.6070\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2560 - accuracy: 0.8703 - val_loss: 2.6206 - val_accuracy: 0.6027\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2547 - accuracy: 0.8702 - val_loss: 2.8050 - val_accuracy: 0.6034\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2562 - accuracy: 0.8665 - val_loss: 2.8300 - val_accuracy: 0.5987\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2689 - accuracy: 0.8661 - val_loss: 2.5855 - val_accuracy: 0.5998\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2671 - accuracy: 0.8721 - val_loss: 2.5595 - val_accuracy: 0.6045\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2732 - accuracy: 0.8635 - val_loss: 2.4247 - val_accuracy: 0.6016\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2600 - accuracy: 0.8704 - val_loss: 2.9340 - val_accuracy: 0.6012\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2554 - accuracy: 0.8715 - val_loss: 2.5548 - val_accuracy: 0.6041\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2693 - accuracy: 0.8669 - val_loss: 2.0216 - val_accuracy: 0.5922\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2535 - accuracy: 0.8674 - val_loss: 2.4473 - val_accuracy: 0.5969\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2578 - accuracy: 0.8692 - val_loss: 2.5401 - val_accuracy: 0.5868\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2378 - accuracy: 0.8767 - val_loss: 3.0233 - val_accuracy: 0.5958\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2522 - accuracy: 0.8741 - val_loss: 2.6764 - val_accuracy: 0.5912\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2512 - accuracy: 0.8705 - val_loss: 2.7063 - val_accuracy: 0.5894\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2359 - accuracy: 0.8784 - val_loss: 2.8431 - val_accuracy: 0.5904\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2387 - accuracy: 0.8768 - val_loss: 2.9649 - val_accuracy: 0.5796\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 3s 10ms/step - loss: 0.2564 - accuracy: 0.8706 - val_loss: 2.7423 - val_accuracy: 0.5804\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 3s 10ms/step - loss: 0.2426 - accuracy: 0.8768 - val_loss: 2.7437 - val_accuracy: 0.5868\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2381 - accuracy: 0.8765 - val_loss: 2.7114 - val_accuracy: 0.5930\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2374 - accuracy: 0.8750 - val_loss: 2.8464 - val_accuracy: 0.6117\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2501 - accuracy: 0.8746 - val_loss: 2.8058 - val_accuracy: 0.5987\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2292 - accuracy: 0.8771 - val_loss: 3.1472 - val_accuracy: 0.5973\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2342 - accuracy: 0.8787 - val_loss: 2.8468 - val_accuracy: 0.5894\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2303 - accuracy: 0.8782 - val_loss: 2.7869 - val_accuracy: 0.5854\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2555 - accuracy: 0.8760 - val_loss: 2.6676 - val_accuracy: 0.5865\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2335 - accuracy: 0.8814 - val_loss: 2.8394 - val_accuracy: 0.5958\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2431 - accuracy: 0.8807 - val_loss: 2.5465 - val_accuracy: 0.5940\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2433 - accuracy: 0.8784 - val_loss: 2.6224 - val_accuracy: 0.5782\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2243 - accuracy: 0.8830 - val_loss: 2.8710 - val_accuracy: 0.5948\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2252 - accuracy: 0.8830 - val_loss: 2.8209 - val_accuracy: 0.5890\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2331 - accuracy: 0.8799 - val_loss: 2.9883 - val_accuracy: 0.5930\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2179 - accuracy: 0.8854 - val_loss: 3.0612 - val_accuracy: 0.5915\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2166 - accuracy: 0.8811 - val_loss: 3.0624 - val_accuracy: 0.5951\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2335 - accuracy: 0.8794 - val_loss: 2.9488 - val_accuracy: 0.6019\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2402 - accuracy: 0.8775 - val_loss: 2.7471 - val_accuracy: 0.5908\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2420 - accuracy: 0.8811 - val_loss: 2.8922 - val_accuracy: 0.5987\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2449 - accuracy: 0.8806 - val_loss: 2.6228 - val_accuracy: 0.5868\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2299 - accuracy: 0.8817 - val_loss: 2.8483 - val_accuracy: 0.5962\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 3s 10ms/step - loss: 0.2156 - accuracy: 0.8866 - val_loss: 3.0922 - val_accuracy: 0.5858\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2091 - accuracy: 0.8883 - val_loss: 3.1968 - val_accuracy: 0.5976\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2162 - accuracy: 0.8889 - val_loss: 3.0429 - val_accuracy: 0.5876\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2263 - accuracy: 0.8812 - val_loss: 2.8991 - val_accuracy: 0.5757\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2318 - accuracy: 0.8841 - val_loss: 3.2815 - val_accuracy: 0.5919\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2249 - accuracy: 0.8844 - val_loss: 2.8518 - val_accuracy: 0.5886\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2104 - accuracy: 0.8854 - val_loss: 2.8637 - val_accuracy: 0.5915\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2221 - accuracy: 0.8866 - val_loss: 2.8767 - val_accuracy: 0.5937\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2191 - accuracy: 0.8853 - val_loss: 2.8092 - val_accuracy: 0.5858\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2181 - accuracy: 0.8854 - val_loss: 2.9954 - val_accuracy: 0.5987\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2218 - accuracy: 0.8861 - val_loss: 2.9668 - val_accuracy: 0.5915\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2237 - accuracy: 0.8852 - val_loss: 2.9111 - val_accuracy: 0.5904\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2343 - accuracy: 0.8827 - val_loss: 2.8156 - val_accuracy: 0.5868\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2128 - accuracy: 0.8891 - val_loss: 3.1761 - val_accuracy: 0.5840\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2082 - accuracy: 0.8892 - val_loss: 3.0886 - val_accuracy: 0.5908\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2114 - accuracy: 0.8901 - val_loss: 3.0164 - val_accuracy: 0.5840\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2167 - accuracy: 0.8887 - val_loss: 2.8661 - val_accuracy: 0.5829\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2149 - accuracy: 0.8903 - val_loss: 3.0368 - val_accuracy: 0.5829\n",
            "History for model 10: <keras.src.callbacks.History object at 0x7b093459cb50>\n",
            "Trial 11: Number of layers = 4, Number of neurons per layer = [50, 100, 150, 200]\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 4s 7ms/step - loss: 0.8166 - accuracy: 0.6110 - val_loss: 0.7732 - val_accuracy: 0.6300\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7627 - accuracy: 0.6348 - val_loss: 0.7861 - val_accuracy: 0.6311\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7474 - accuracy: 0.6437 - val_loss: 0.7521 - val_accuracy: 0.6433\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7348 - accuracy: 0.6505 - val_loss: 0.7491 - val_accuracy: 0.6426\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7243 - accuracy: 0.6587 - val_loss: 0.7730 - val_accuracy: 0.6296\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7147 - accuracy: 0.6622 - val_loss: 0.7498 - val_accuracy: 0.6469\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7035 - accuracy: 0.6662 - val_loss: 0.7566 - val_accuracy: 0.6451\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.6951 - accuracy: 0.6716 - val_loss: 0.7638 - val_accuracy: 0.6282\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.6792 - val_loss: 0.7644 - val_accuracy: 0.6347\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6800 - accuracy: 0.6762 - val_loss: 0.7912 - val_accuracy: 0.6329\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6717 - accuracy: 0.6846 - val_loss: 0.7772 - val_accuracy: 0.6458\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6635 - accuracy: 0.6887 - val_loss: 0.7564 - val_accuracy: 0.6505\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6565 - accuracy: 0.6881 - val_loss: 0.7690 - val_accuracy: 0.6440\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6456 - accuracy: 0.6956 - val_loss: 0.7870 - val_accuracy: 0.6429\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.6373 - accuracy: 0.7029 - val_loss: 0.7914 - val_accuracy: 0.6372\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6338 - accuracy: 0.7021 - val_loss: 0.7741 - val_accuracy: 0.6419\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6258 - accuracy: 0.7039 - val_loss: 0.8046 - val_accuracy: 0.6566\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6240 - accuracy: 0.7113 - val_loss: 0.7814 - val_accuracy: 0.6483\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6074 - accuracy: 0.7168 - val_loss: 0.8036 - val_accuracy: 0.6386\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6050 - accuracy: 0.7184 - val_loss: 0.8355 - val_accuracy: 0.6379\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6013 - accuracy: 0.7188 - val_loss: 0.8233 - val_accuracy: 0.6508\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5934 - accuracy: 0.7238 - val_loss: 0.8396 - val_accuracy: 0.6372\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5818 - accuracy: 0.7247 - val_loss: 0.8257 - val_accuracy: 0.6401\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5777 - accuracy: 0.7268 - val_loss: 0.8843 - val_accuracy: 0.6318\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5745 - accuracy: 0.7318 - val_loss: 0.8705 - val_accuracy: 0.6401\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5634 - accuracy: 0.7411 - val_loss: 0.9132 - val_accuracy: 0.6286\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5595 - accuracy: 0.7370 - val_loss: 0.8970 - val_accuracy: 0.6343\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5548 - accuracy: 0.7401 - val_loss: 0.8614 - val_accuracy: 0.6455\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5508 - accuracy: 0.7431 - val_loss: 0.9633 - val_accuracy: 0.6307\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5443 - accuracy: 0.7475 - val_loss: 0.9232 - val_accuracy: 0.6242\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5393 - accuracy: 0.7464 - val_loss: 0.9869 - val_accuracy: 0.6336\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5345 - accuracy: 0.7523 - val_loss: 0.9681 - val_accuracy: 0.6343\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5264 - accuracy: 0.7533 - val_loss: 0.9957 - val_accuracy: 0.6408\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5274 - accuracy: 0.7553 - val_loss: 1.0148 - val_accuracy: 0.6336\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5202 - accuracy: 0.7536 - val_loss: 1.0061 - val_accuracy: 0.6379\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5060 - accuracy: 0.7639 - val_loss: 1.0479 - val_accuracy: 0.6282\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5067 - accuracy: 0.7651 - val_loss: 1.0163 - val_accuracy: 0.6242\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5012 - accuracy: 0.7621 - val_loss: 1.0655 - val_accuracy: 0.6339\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5018 - accuracy: 0.7648 - val_loss: 1.0539 - val_accuracy: 0.6250\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4904 - accuracy: 0.7698 - val_loss: 1.0886 - val_accuracy: 0.6282\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4955 - accuracy: 0.7674 - val_loss: 1.0933 - val_accuracy: 0.6163\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4848 - accuracy: 0.7689 - val_loss: 1.0893 - val_accuracy: 0.6185\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4854 - accuracy: 0.7714 - val_loss: 1.0915 - val_accuracy: 0.6268\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4723 - accuracy: 0.7790 - val_loss: 1.2498 - val_accuracy: 0.6203\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4721 - accuracy: 0.7750 - val_loss: 1.1987 - val_accuracy: 0.6134\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4671 - accuracy: 0.7768 - val_loss: 1.0941 - val_accuracy: 0.6196\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4678 - accuracy: 0.7817 - val_loss: 1.1798 - val_accuracy: 0.6289\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4641 - accuracy: 0.7812 - val_loss: 1.1488 - val_accuracy: 0.6235\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4571 - accuracy: 0.7824 - val_loss: 1.2437 - val_accuracy: 0.6206\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4522 - accuracy: 0.7831 - val_loss: 1.2481 - val_accuracy: 0.6232\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4421 - accuracy: 0.7885 - val_loss: 1.2507 - val_accuracy: 0.6264\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4429 - accuracy: 0.7894 - val_loss: 1.3733 - val_accuracy: 0.6242\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4411 - accuracy: 0.7907 - val_loss: 1.3153 - val_accuracy: 0.6228\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4365 - accuracy: 0.7943 - val_loss: 1.3033 - val_accuracy: 0.6156\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4313 - accuracy: 0.7963 - val_loss: 1.2653 - val_accuracy: 0.6156\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4265 - accuracy: 0.7972 - val_loss: 1.3789 - val_accuracy: 0.6232\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4310 - accuracy: 0.7951 - val_loss: 1.4504 - val_accuracy: 0.6077\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4273 - accuracy: 0.7970 - val_loss: 1.4305 - val_accuracy: 0.6124\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4201 - accuracy: 0.8001 - val_loss: 1.4759 - val_accuracy: 0.6163\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4115 - accuracy: 0.8049 - val_loss: 1.4250 - val_accuracy: 0.6203\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4104 - accuracy: 0.8049 - val_loss: 1.5211 - val_accuracy: 0.6120\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4071 - accuracy: 0.8071 - val_loss: 1.5500 - val_accuracy: 0.6278\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4113 - accuracy: 0.8053 - val_loss: 1.4652 - val_accuracy: 0.6091\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4056 - accuracy: 0.8062 - val_loss: 1.4413 - val_accuracy: 0.6109\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4023 - accuracy: 0.8087 - val_loss: 1.4409 - val_accuracy: 0.6142\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3938 - accuracy: 0.8105 - val_loss: 1.6341 - val_accuracy: 0.6170\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3857 - accuracy: 0.8149 - val_loss: 1.5873 - val_accuracy: 0.6214\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3917 - accuracy: 0.8107 - val_loss: 1.5958 - val_accuracy: 0.6152\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3952 - accuracy: 0.8086 - val_loss: 1.5765 - val_accuracy: 0.6203\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3921 - accuracy: 0.8154 - val_loss: 1.6861 - val_accuracy: 0.6174\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3850 - accuracy: 0.8200 - val_loss: 1.5873 - val_accuracy: 0.6070\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3805 - accuracy: 0.8172 - val_loss: 1.7783 - val_accuracy: 0.6188\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3709 - accuracy: 0.8227 - val_loss: 1.7146 - val_accuracy: 0.6077\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3689 - accuracy: 0.8241 - val_loss: 1.7160 - val_accuracy: 0.6077\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3785 - accuracy: 0.8157 - val_loss: 1.7498 - val_accuracy: 0.6077\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3814 - accuracy: 0.8239 - val_loss: 1.7088 - val_accuracy: 0.6117\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3724 - accuracy: 0.8188 - val_loss: 1.7803 - val_accuracy: 0.6070\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3614 - accuracy: 0.8265 - val_loss: 1.8244 - val_accuracy: 0.5969\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3522 - accuracy: 0.8276 - val_loss: 1.8281 - val_accuracy: 0.6073\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3507 - accuracy: 0.8295 - val_loss: 1.6630 - val_accuracy: 0.6185\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3595 - accuracy: 0.8243 - val_loss: 1.7953 - val_accuracy: 0.6052\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3501 - accuracy: 0.8319 - val_loss: 1.8537 - val_accuracy: 0.6199\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3460 - accuracy: 0.8335 - val_loss: 1.8445 - val_accuracy: 0.6041\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3560 - accuracy: 0.8269 - val_loss: 1.7705 - val_accuracy: 0.6185\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3422 - accuracy: 0.8326 - val_loss: 1.8920 - val_accuracy: 0.6145\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3715 - accuracy: 0.8274 - val_loss: 1.8998 - val_accuracy: 0.6134\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3543 - accuracy: 0.8322 - val_loss: 1.8534 - val_accuracy: 0.6052\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3287 - accuracy: 0.8380 - val_loss: 1.9156 - val_accuracy: 0.6012\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3273 - accuracy: 0.8403 - val_loss: 2.0581 - val_accuracy: 0.6063\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3470 - accuracy: 0.8324 - val_loss: 1.9090 - val_accuracy: 0.6052\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3332 - accuracy: 0.8378 - val_loss: 2.0273 - val_accuracy: 0.6117\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3303 - accuracy: 0.8369 - val_loss: 2.1396 - val_accuracy: 0.5994\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3279 - accuracy: 0.8419 - val_loss: 2.0489 - val_accuracy: 0.6181\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3352 - accuracy: 0.8394 - val_loss: 2.0210 - val_accuracy: 0.6113\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3184 - accuracy: 0.8455 - val_loss: 2.0540 - val_accuracy: 0.6037\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3107 - accuracy: 0.8447 - val_loss: 2.0025 - val_accuracy: 0.6156\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3451 - accuracy: 0.8390 - val_loss: 1.8254 - val_accuracy: 0.6027\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3238 - accuracy: 0.8430 - val_loss: 2.0640 - val_accuracy: 0.5983\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3104 - accuracy: 0.8448 - val_loss: 2.0883 - val_accuracy: 0.5994\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3185 - accuracy: 0.8448 - val_loss: 1.9422 - val_accuracy: 0.6106\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3389 - accuracy: 0.8424 - val_loss: 1.9029 - val_accuracy: 0.6001\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3098 - accuracy: 0.8484 - val_loss: 1.9982 - val_accuracy: 0.6052\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3015 - accuracy: 0.8491 - val_loss: 2.2184 - val_accuracy: 0.6106\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3025 - accuracy: 0.8505 - val_loss: 2.1680 - val_accuracy: 0.6005\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3225 - accuracy: 0.8441 - val_loss: 2.1968 - val_accuracy: 0.5937\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3179 - accuracy: 0.8446 - val_loss: 2.1706 - val_accuracy: 0.6066\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3030 - accuracy: 0.8509 - val_loss: 2.1875 - val_accuracy: 0.6081\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2891 - accuracy: 0.8554 - val_loss: 2.2695 - val_accuracy: 0.6131\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2926 - accuracy: 0.8536 - val_loss: 2.1508 - val_accuracy: 0.5976\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3007 - accuracy: 0.8515 - val_loss: 2.2486 - val_accuracy: 0.6041\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3043 - accuracy: 0.8469 - val_loss: 2.1635 - val_accuracy: 0.5922\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2986 - accuracy: 0.8556 - val_loss: 2.3750 - val_accuracy: 0.5965\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2989 - accuracy: 0.8526 - val_loss: 2.2668 - val_accuracy: 0.5919\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2818 - accuracy: 0.8611 - val_loss: 2.4287 - val_accuracy: 0.6091\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2888 - accuracy: 0.8567 - val_loss: 2.2764 - val_accuracy: 0.5969\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2809 - accuracy: 0.8611 - val_loss: 2.5203 - val_accuracy: 0.5962\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2964 - accuracy: 0.8532 - val_loss: 2.5150 - val_accuracy: 0.6016\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2912 - accuracy: 0.8597 - val_loss: 2.2534 - val_accuracy: 0.5994\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2902 - accuracy: 0.8597 - val_loss: 2.4783 - val_accuracy: 0.6030\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2942 - accuracy: 0.8556 - val_loss: 2.2464 - val_accuracy: 0.5958\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.2827 - accuracy: 0.8589 - val_loss: 2.3861 - val_accuracy: 0.6019\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2850 - accuracy: 0.8620 - val_loss: 2.4327 - val_accuracy: 0.5962\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2762 - accuracy: 0.8628 - val_loss: 2.3569 - val_accuracy: 0.5915\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2801 - accuracy: 0.8635 - val_loss: 2.4329 - val_accuracy: 0.5991\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2745 - accuracy: 0.8624 - val_loss: 2.6608 - val_accuracy: 0.6045\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2736 - accuracy: 0.8631 - val_loss: 2.3564 - val_accuracy: 0.6037\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2677 - accuracy: 0.8665 - val_loss: 2.6807 - val_accuracy: 0.5933\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2656 - accuracy: 0.8636 - val_loss: 2.4452 - val_accuracy: 0.5912\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2770 - accuracy: 0.8579 - val_loss: 2.8017 - val_accuracy: 0.5998\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2859 - accuracy: 0.8596 - val_loss: 2.4713 - val_accuracy: 0.5962\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2734 - accuracy: 0.8663 - val_loss: 2.5289 - val_accuracy: 0.5994\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2544 - accuracy: 0.8707 - val_loss: 2.7074 - val_accuracy: 0.5944\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2579 - accuracy: 0.8718 - val_loss: 2.6519 - val_accuracy: 0.5930\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2638 - accuracy: 0.8686 - val_loss: 2.9314 - val_accuracy: 0.5836\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3108 - accuracy: 0.8582 - val_loss: 2.4273 - val_accuracy: 0.5912\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2911 - accuracy: 0.8585 - val_loss: 2.3164 - val_accuracy: 0.5976\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2636 - accuracy: 0.8710 - val_loss: 2.4763 - val_accuracy: 0.5890\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2593 - accuracy: 0.8712 - val_loss: 2.4963 - val_accuracy: 0.5994\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2708 - accuracy: 0.8688 - val_loss: 2.4374 - val_accuracy: 0.5951\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2659 - accuracy: 0.8661 - val_loss: 2.5995 - val_accuracy: 0.5933\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2560 - accuracy: 0.8705 - val_loss: 2.7157 - val_accuracy: 0.5944\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2620 - accuracy: 0.8687 - val_loss: 2.7012 - val_accuracy: 0.5965\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2560 - accuracy: 0.8690 - val_loss: 2.7291 - val_accuracy: 0.5854\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2481 - accuracy: 0.8743 - val_loss: 2.7056 - val_accuracy: 0.6005\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2459 - accuracy: 0.8787 - val_loss: 2.6389 - val_accuracy: 0.5958\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2611 - accuracy: 0.8679 - val_loss: 2.6158 - val_accuracy: 0.5930\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2625 - accuracy: 0.8714 - val_loss: 2.5712 - val_accuracy: 0.6055\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2655 - accuracy: 0.8724 - val_loss: 2.5090 - val_accuracy: 0.5987\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2578 - accuracy: 0.8721 - val_loss: 2.6907 - val_accuracy: 0.6030\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2625 - accuracy: 0.8716 - val_loss: 2.4907 - val_accuracy: 0.5973\n",
            "History for model 11: <keras.src.callbacks.History object at 0x7b095e2a3be0>\n"
          ]
        }
      ],
      "source": [
        "# Define the models and their configurations\n",
        "model_configs = [\n",
        "    (init_neurons, [30, 40, 50]),\n",
        "    (init_neurons, [90, 100, 110]),\n",
        "    (init_neurons, [120, 130, 140]),\n",
        "    (init_neurons, [150, 160, 170]),\n",
        "    (init_neurons, [180, 190, 200]),\n",
        "    (init_neurons, [60, 70, 80, 90]),\n",
        "    (init_neurons, [100, 110, 120, 130]),\n",
        "    (init_neurons, [140, 150, 160, 170]),\n",
        "    (init_neurons, [180, 190, 200, 210]),\n",
        "    (init_neurons, [50, 100, 150, 200])\n",
        "]\n",
        "\n",
        "# Initialize lists to store models and histories\n",
        "models2 = []\n",
        "histories2 = []\n",
        "\n",
        "# Loop over model configurations\n",
        "for i, config in enumerate(model_configs, start=2):\n",
        "    # Extract the number of layers and neurons\n",
        "    num_layers = len(config[1])\n",
        "    neurons = config[1]\n",
        "\n",
        "    # Print the number of layers and neurons\n",
        "    print(f\"Trial {i}: Number of layers = {num_layers}, Number of neurons per layer = {neurons}\")\n",
        "\n",
        "    # Define the model\n",
        "    model_layers = [Dense(config[0], activation='relu', input_shape=input_shape)]\n",
        "    for neurons in config[1]:\n",
        "        model_layers.append(Dense(neurons, activation='relu'))\n",
        "    model_layers.append(Dense(3, activation='softmax'))\n",
        "\n",
        "    model = tf.keras.models.Sequential(model_layers)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model and store the history\n",
        "    history = model.fit(X_train, y_train_onehot, epochs=150, validation_data=(X_test, y_test_onehot))\n",
        "\n",
        "    # Append the model and its history to the lists\n",
        "    models2.append(model)\n",
        "    histories2.append(history)\n",
        "\n",
        "    # Print the history for reference\n",
        "    print(f\"History for model {i}:\", history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2npxFzSDfU0",
        "outputId": "fcf29787-a493-41f6-a9ee-2f315a5e9633"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5804 - accuracy: 0.7306 - val_loss: 0.9109 - val_accuracy: 0.6253\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5759 - accuracy: 0.7334 - val_loss: 0.9109 - val_accuracy: 0.6206\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5757 - accuracy: 0.7312 - val_loss: 0.9278 - val_accuracy: 0.6081\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5713 - accuracy: 0.7375 - val_loss: 0.9206 - val_accuracy: 0.6131\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5692 - accuracy: 0.7380 - val_loss: 0.9333 - val_accuracy: 0.6167\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5637 - accuracy: 0.7375 - val_loss: 0.9264 - val_accuracy: 0.6081\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5572 - accuracy: 0.7441 - val_loss: 0.9322 - val_accuracy: 0.6224\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5577 - accuracy: 0.7427 - val_loss: 0.9401 - val_accuracy: 0.6059\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5547 - accuracy: 0.7416 - val_loss: 0.9480 - val_accuracy: 0.6188\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5508 - accuracy: 0.7435 - val_loss: 0.9629 - val_accuracy: 0.6206\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5508 - accuracy: 0.7454 - val_loss: 0.9597 - val_accuracy: 0.6196\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5462 - accuracy: 0.7486 - val_loss: 0.9718 - val_accuracy: 0.6077\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5425 - accuracy: 0.7480 - val_loss: 0.9794 - val_accuracy: 0.6131\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5385 - accuracy: 0.7494 - val_loss: 0.9833 - val_accuracy: 0.6188\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5376 - accuracy: 0.7485 - val_loss: 0.9846 - val_accuracy: 0.6109\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5380 - accuracy: 0.7510 - val_loss: 0.9761 - val_accuracy: 0.6160\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5354 - accuracy: 0.7546 - val_loss: 1.0008 - val_accuracy: 0.6109\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5295 - accuracy: 0.7516 - val_loss: 1.0210 - val_accuracy: 0.6052\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5279 - accuracy: 0.7549 - val_loss: 1.0260 - val_accuracy: 0.6048\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5265 - accuracy: 0.7557 - val_loss: 1.0164 - val_accuracy: 0.6120\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5253 - accuracy: 0.7549 - val_loss: 1.0268 - val_accuracy: 0.6099\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5215 - accuracy: 0.7567 - val_loss: 1.0386 - val_accuracy: 0.6163\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5204 - accuracy: 0.7589 - val_loss: 1.0349 - val_accuracy: 0.6048\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5175 - accuracy: 0.7600 - val_loss: 1.0486 - val_accuracy: 0.6016\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5141 - accuracy: 0.7630 - val_loss: 1.0379 - val_accuracy: 0.6001\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5086 - accuracy: 0.7635 - val_loss: 1.1006 - val_accuracy: 0.6041\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5077 - accuracy: 0.7634 - val_loss: 1.0771 - val_accuracy: 0.6019\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5021 - accuracy: 0.7688 - val_loss: 1.0986 - val_accuracy: 0.6077\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5054 - accuracy: 0.7666 - val_loss: 1.0956 - val_accuracy: 0.6041\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5051 - accuracy: 0.7664 - val_loss: 1.0953 - val_accuracy: 0.6145\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5012 - accuracy: 0.7685 - val_loss: 1.1041 - val_accuracy: 0.5998\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5016 - accuracy: 0.7653 - val_loss: 1.1146 - val_accuracy: 0.6005\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4944 - accuracy: 0.7703 - val_loss: 1.1189 - val_accuracy: 0.6001\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4955 - accuracy: 0.7657 - val_loss: 1.1318 - val_accuracy: 0.6181\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4985 - accuracy: 0.7695 - val_loss: 1.1279 - val_accuracy: 0.6030\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4912 - accuracy: 0.7698 - val_loss: 1.1370 - val_accuracy: 0.6030\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4919 - accuracy: 0.7720 - val_loss: 1.1596 - val_accuracy: 0.5955\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4888 - accuracy: 0.7722 - val_loss: 1.1585 - val_accuracy: 0.6149\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4881 - accuracy: 0.7789 - val_loss: 1.1541 - val_accuracy: 0.6023\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4814 - accuracy: 0.7756 - val_loss: 1.1776 - val_accuracy: 0.6016\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4784 - accuracy: 0.7803 - val_loss: 1.1734 - val_accuracy: 0.6099\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4786 - accuracy: 0.7789 - val_loss: 1.1815 - val_accuracy: 0.6131\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4803 - accuracy: 0.7757 - val_loss: 1.1914 - val_accuracy: 0.6041\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4716 - accuracy: 0.7823 - val_loss: 1.2343 - val_accuracy: 0.6045\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4762 - accuracy: 0.7768 - val_loss: 1.2050 - val_accuracy: 0.6095\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4721 - accuracy: 0.7792 - val_loss: 1.2103 - val_accuracy: 0.6102\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4739 - accuracy: 0.7811 - val_loss: 1.2295 - val_accuracy: 0.6009\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4707 - accuracy: 0.7781 - val_loss: 1.2404 - val_accuracy: 0.5983\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4677 - accuracy: 0.7810 - val_loss: 1.2500 - val_accuracy: 0.6012\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4668 - accuracy: 0.7852 - val_loss: 1.2153 - val_accuracy: 0.6001\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4658 - accuracy: 0.7846 - val_loss: 1.2492 - val_accuracy: 0.6073\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4624 - accuracy: 0.7895 - val_loss: 1.2397 - val_accuracy: 0.5987\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4604 - accuracy: 0.7838 - val_loss: 1.2804 - val_accuracy: 0.6081\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4567 - accuracy: 0.7905 - val_loss: 1.2745 - val_accuracy: 0.6012\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4567 - accuracy: 0.7886 - val_loss: 1.2922 - val_accuracy: 0.5962\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4628 - accuracy: 0.7866 - val_loss: 1.2473 - val_accuracy: 0.6106\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4562 - accuracy: 0.7878 - val_loss: 1.2725 - val_accuracy: 0.5987\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4531 - accuracy: 0.7927 - val_loss: 1.2964 - val_accuracy: 0.5908\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4519 - accuracy: 0.7901 - val_loss: 1.2597 - val_accuracy: 0.5965\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4486 - accuracy: 0.7903 - val_loss: 1.3274 - val_accuracy: 0.5969\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4505 - accuracy: 0.7899 - val_loss: 1.2802 - val_accuracy: 0.5912\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4481 - accuracy: 0.7929 - val_loss: 1.3410 - val_accuracy: 0.6034\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4445 - accuracy: 0.7896 - val_loss: 1.3235 - val_accuracy: 0.6084\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4466 - accuracy: 0.7903 - val_loss: 1.3030 - val_accuracy: 0.5958\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4424 - accuracy: 0.7911 - val_loss: 1.3225 - val_accuracy: 0.5940\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4436 - accuracy: 0.7935 - val_loss: 1.3391 - val_accuracy: 0.5926\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4408 - accuracy: 0.7938 - val_loss: 1.3240 - val_accuracy: 0.5865\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4427 - accuracy: 0.7947 - val_loss: 1.3426 - val_accuracy: 0.5912\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4398 - accuracy: 0.7967 - val_loss: 1.3777 - val_accuracy: 0.6019\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4356 - accuracy: 0.7968 - val_loss: 1.3782 - val_accuracy: 0.5955\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4375 - accuracy: 0.7942 - val_loss: 1.3461 - val_accuracy: 0.6066\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4358 - accuracy: 0.7967 - val_loss: 1.3637 - val_accuracy: 0.5955\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4331 - accuracy: 0.7974 - val_loss: 1.3900 - val_accuracy: 0.5998\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4291 - accuracy: 0.8037 - val_loss: 1.3998 - val_accuracy: 0.6012\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4303 - accuracy: 0.8006 - val_loss: 1.3997 - val_accuracy: 0.6019\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4299 - accuracy: 0.7978 - val_loss: 1.3945 - val_accuracy: 0.6045\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4295 - accuracy: 0.7998 - val_loss: 1.3960 - val_accuracy: 0.5958\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4271 - accuracy: 0.8008 - val_loss: 1.4531 - val_accuracy: 0.6045\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4257 - accuracy: 0.8009 - val_loss: 1.4240 - val_accuracy: 0.5951\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4236 - accuracy: 0.8050 - val_loss: 1.4115 - val_accuracy: 0.6009\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4192 - accuracy: 0.8033 - val_loss: 1.4644 - val_accuracy: 0.5894\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4224 - accuracy: 0.7997 - val_loss: 1.4858 - val_accuracy: 0.5994\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4238 - accuracy: 0.8033 - val_loss: 1.4151 - val_accuracy: 0.6023\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4201 - accuracy: 0.8029 - val_loss: 1.4631 - val_accuracy: 0.6041\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4189 - accuracy: 0.8049 - val_loss: 1.4774 - val_accuracy: 0.5926\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4135 - accuracy: 0.8034 - val_loss: 1.5152 - val_accuracy: 0.5944\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4210 - accuracy: 0.8066 - val_loss: 1.4595 - val_accuracy: 0.5908\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4277 - accuracy: 0.8031 - val_loss: 1.4506 - val_accuracy: 0.5991\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4157 - accuracy: 0.8092 - val_loss: 1.4606 - val_accuracy: 0.5937\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4072 - accuracy: 0.8095 - val_loss: 1.4701 - val_accuracy: 0.5980\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4074 - accuracy: 0.8103 - val_loss: 1.4629 - val_accuracy: 0.5890\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4101 - accuracy: 0.8103 - val_loss: 1.4875 - val_accuracy: 0.5951\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4091 - accuracy: 0.8062 - val_loss: 1.5092 - val_accuracy: 0.5951\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4104 - accuracy: 0.8083 - val_loss: 1.4819 - val_accuracy: 0.5908\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4063 - accuracy: 0.8046 - val_loss: 1.5638 - val_accuracy: 0.5908\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4080 - accuracy: 0.8115 - val_loss: 1.5238 - val_accuracy: 0.5894\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3997 - accuracy: 0.8136 - val_loss: 1.5011 - val_accuracy: 0.5858\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4071 - accuracy: 0.8098 - val_loss: 1.5361 - val_accuracy: 0.5850\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4035 - accuracy: 0.8095 - val_loss: 1.5180 - val_accuracy: 0.5958\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4055 - accuracy: 0.8078 - val_loss: 1.5412 - val_accuracy: 0.5912\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3985 - accuracy: 0.8138 - val_loss: 1.5654 - val_accuracy: 0.5930\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4011 - accuracy: 0.8186 - val_loss: 1.5690 - val_accuracy: 0.5991\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3954 - accuracy: 0.8156 - val_loss: 1.5402 - val_accuracy: 0.5922\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3969 - accuracy: 0.8155 - val_loss: 1.5688 - val_accuracy: 0.6019\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3967 - accuracy: 0.8130 - val_loss: 1.5730 - val_accuracy: 0.5843\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4004 - accuracy: 0.8134 - val_loss: 1.6365 - val_accuracy: 0.5850\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3970 - accuracy: 0.8134 - val_loss: 1.6189 - val_accuracy: 0.5991\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3920 - accuracy: 0.8157 - val_loss: 1.5803 - val_accuracy: 0.5980\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3949 - accuracy: 0.8156 - val_loss: 1.6179 - val_accuracy: 0.5908\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3920 - accuracy: 0.8143 - val_loss: 1.5985 - val_accuracy: 0.5930\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3932 - accuracy: 0.8170 - val_loss: 1.6297 - val_accuracy: 0.5843\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3895 - accuracy: 0.8153 - val_loss: 1.6190 - val_accuracy: 0.5868\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3924 - accuracy: 0.8177 - val_loss: 1.5962 - val_accuracy: 0.5879\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3905 - accuracy: 0.8180 - val_loss: 1.5925 - val_accuracy: 0.5886\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3908 - accuracy: 0.8156 - val_loss: 1.5900 - val_accuracy: 0.5937\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3840 - accuracy: 0.8190 - val_loss: 1.6269 - val_accuracy: 0.5890\n",
            "History for model 9: <keras.src.callbacks.History object at 0x7e83fa5516f0>\n",
            "Trial 10: Number of layers = 3, Number of neurons per layer = 90\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 3s 6ms/step - loss: 0.8281 - accuracy: 0.6001 - val_loss: 0.7955 - val_accuracy: 0.6149\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7721 - accuracy: 0.6292 - val_loss: 0.7884 - val_accuracy: 0.6271\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7540 - accuracy: 0.6428 - val_loss: 0.7867 - val_accuracy: 0.6303\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7425 - accuracy: 0.6490 - val_loss: 0.7778 - val_accuracy: 0.6350\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7322 - accuracy: 0.6521 - val_loss: 0.7776 - val_accuracy: 0.6339\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7241 - accuracy: 0.6613 - val_loss: 0.7882 - val_accuracy: 0.6282\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7146 - accuracy: 0.6666 - val_loss: 0.7702 - val_accuracy: 0.6250\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.7051 - accuracy: 0.6733 - val_loss: 0.7898 - val_accuracy: 0.6152\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6991 - accuracy: 0.6708 - val_loss: 0.7958 - val_accuracy: 0.6293\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.6913 - accuracy: 0.6792 - val_loss: 0.7925 - val_accuracy: 0.6303\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6837 - accuracy: 0.6840 - val_loss: 0.7865 - val_accuracy: 0.6397\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6750 - accuracy: 0.6903 - val_loss: 0.8028 - val_accuracy: 0.6350\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6711 - accuracy: 0.6863 - val_loss: 0.8031 - val_accuracy: 0.6303\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6624 - accuracy: 0.6942 - val_loss: 0.7953 - val_accuracy: 0.6339\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6587 - accuracy: 0.6959 - val_loss: 0.7947 - val_accuracy: 0.6314\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6511 - accuracy: 0.6948 - val_loss: 0.8025 - val_accuracy: 0.6419\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6465 - accuracy: 0.6965 - val_loss: 0.8045 - val_accuracy: 0.6289\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6353 - accuracy: 0.7091 - val_loss: 0.8037 - val_accuracy: 0.6314\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6340 - accuracy: 0.7076 - val_loss: 0.8156 - val_accuracy: 0.6339\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6263 - accuracy: 0.7066 - val_loss: 0.8099 - val_accuracy: 0.6404\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6233 - accuracy: 0.7130 - val_loss: 0.8249 - val_accuracy: 0.6386\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6176 - accuracy: 0.7124 - val_loss: 0.8356 - val_accuracy: 0.6386\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6087 - accuracy: 0.7228 - val_loss: 0.8315 - val_accuracy: 0.6278\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6020 - accuracy: 0.7247 - val_loss: 0.8485 - val_accuracy: 0.6300\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6006 - accuracy: 0.7249 - val_loss: 0.8411 - val_accuracy: 0.6264\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5948 - accuracy: 0.7261 - val_loss: 0.8638 - val_accuracy: 0.6311\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5905 - accuracy: 0.7310 - val_loss: 0.8501 - val_accuracy: 0.6321\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5833 - accuracy: 0.7313 - val_loss: 0.8850 - val_accuracy: 0.6332\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5783 - accuracy: 0.7327 - val_loss: 0.8779 - val_accuracy: 0.6286\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5756 - accuracy: 0.7400 - val_loss: 0.8936 - val_accuracy: 0.6350\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5673 - accuracy: 0.7398 - val_loss: 0.9148 - val_accuracy: 0.6271\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5694 - accuracy: 0.7404 - val_loss: 0.9117 - val_accuracy: 0.6275\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5616 - accuracy: 0.7440 - val_loss: 0.8815 - val_accuracy: 0.6375\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5554 - accuracy: 0.7440 - val_loss: 0.9062 - val_accuracy: 0.6361\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5540 - accuracy: 0.7410 - val_loss: 0.9105 - val_accuracy: 0.6199\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5483 - accuracy: 0.7436 - val_loss: 0.9072 - val_accuracy: 0.6444\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5439 - accuracy: 0.7463 - val_loss: 0.9255 - val_accuracy: 0.6314\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5391 - accuracy: 0.7487 - val_loss: 0.9470 - val_accuracy: 0.6336\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5373 - accuracy: 0.7512 - val_loss: 0.9402 - val_accuracy: 0.6329\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5305 - accuracy: 0.7568 - val_loss: 0.9419 - val_accuracy: 0.6264\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5288 - accuracy: 0.7549 - val_loss: 0.9648 - val_accuracy: 0.6268\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5295 - accuracy: 0.7568 - val_loss: 0.9732 - val_accuracy: 0.6379\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5207 - accuracy: 0.7598 - val_loss: 0.9747 - val_accuracy: 0.6257\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5187 - accuracy: 0.7605 - val_loss: 0.9606 - val_accuracy: 0.6350\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5177 - accuracy: 0.7552 - val_loss: 0.9540 - val_accuracy: 0.6293\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5099 - accuracy: 0.7646 - val_loss: 0.9847 - val_accuracy: 0.6199\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5081 - accuracy: 0.7640 - val_loss: 1.0033 - val_accuracy: 0.6232\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5004 - accuracy: 0.7682 - val_loss: 1.0298 - val_accuracy: 0.6235\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4998 - accuracy: 0.7705 - val_loss: 0.9939 - val_accuracy: 0.6278\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4998 - accuracy: 0.7697 - val_loss: 1.0308 - val_accuracy: 0.6303\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4930 - accuracy: 0.7781 - val_loss: 1.0631 - val_accuracy: 0.6257\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4846 - accuracy: 0.7768 - val_loss: 1.0461 - val_accuracy: 0.6314\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4905 - accuracy: 0.7746 - val_loss: 1.0615 - val_accuracy: 0.6163\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4930 - accuracy: 0.7762 - val_loss: 1.0487 - val_accuracy: 0.6224\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4786 - accuracy: 0.7807 - val_loss: 1.0696 - val_accuracy: 0.6289\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4780 - accuracy: 0.7798 - val_loss: 1.1192 - val_accuracy: 0.6185\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4744 - accuracy: 0.7816 - val_loss: 1.0756 - val_accuracy: 0.6282\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4704 - accuracy: 0.7836 - val_loss: 1.1051 - val_accuracy: 0.6224\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4693 - accuracy: 0.7828 - val_loss: 1.1090 - val_accuracy: 0.6268\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4695 - accuracy: 0.7824 - val_loss: 1.1039 - val_accuracy: 0.6214\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4630 - accuracy: 0.7891 - val_loss: 1.1253 - val_accuracy: 0.6257\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4629 - accuracy: 0.7866 - val_loss: 1.1051 - val_accuracy: 0.6188\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4576 - accuracy: 0.7866 - val_loss: 1.1718 - val_accuracy: 0.6124\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4598 - accuracy: 0.7887 - val_loss: 1.1428 - val_accuracy: 0.6131\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4555 - accuracy: 0.7856 - val_loss: 1.1743 - val_accuracy: 0.6127\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4585 - accuracy: 0.7865 - val_loss: 1.1541 - val_accuracy: 0.6188\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4489 - accuracy: 0.7914 - val_loss: 1.1612 - val_accuracy: 0.6145\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4489 - accuracy: 0.7922 - val_loss: 1.1905 - val_accuracy: 0.6113\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4434 - accuracy: 0.7951 - val_loss: 1.1735 - val_accuracy: 0.6196\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4396 - accuracy: 0.7944 - val_loss: 1.1739 - val_accuracy: 0.6206\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4384 - accuracy: 0.7970 - val_loss: 1.1667 - val_accuracy: 0.6235\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4393 - accuracy: 0.7952 - val_loss: 1.1676 - val_accuracy: 0.6206\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4382 - accuracy: 0.7962 - val_loss: 1.2262 - val_accuracy: 0.6120\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4364 - accuracy: 0.7982 - val_loss: 1.2066 - val_accuracy: 0.6138\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4295 - accuracy: 0.8029 - val_loss: 1.2102 - val_accuracy: 0.6138\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4324 - accuracy: 0.7985 - val_loss: 1.2300 - val_accuracy: 0.6224\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4255 - accuracy: 0.8019 - val_loss: 1.2300 - val_accuracy: 0.6124\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4207 - accuracy: 0.8069 - val_loss: 1.2588 - val_accuracy: 0.6124\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4219 - accuracy: 0.8055 - val_loss: 1.2324 - val_accuracy: 0.6160\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4214 - accuracy: 0.8054 - val_loss: 1.2452 - val_accuracy: 0.6170\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4234 - accuracy: 0.8023 - val_loss: 1.2408 - val_accuracy: 0.6160\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4132 - accuracy: 0.8098 - val_loss: 1.2821 - val_accuracy: 0.6210\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4128 - accuracy: 0.8069 - val_loss: 1.2901 - val_accuracy: 0.6257\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4178 - accuracy: 0.8043 - val_loss: 1.2891 - val_accuracy: 0.6124\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4153 - accuracy: 0.8043 - val_loss: 1.2823 - val_accuracy: 0.6102\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4084 - accuracy: 0.8104 - val_loss: 1.2667 - val_accuracy: 0.6185\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4055 - accuracy: 0.8080 - val_loss: 1.2914 - val_accuracy: 0.6152\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4010 - accuracy: 0.8097 - val_loss: 1.3546 - val_accuracy: 0.6037\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4036 - accuracy: 0.8133 - val_loss: 1.3327 - val_accuracy: 0.6095\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4001 - accuracy: 0.8098 - val_loss: 1.3669 - val_accuracy: 0.6170\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3977 - accuracy: 0.8127 - val_loss: 1.3862 - val_accuracy: 0.6124\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3945 - accuracy: 0.8163 - val_loss: 1.3940 - val_accuracy: 0.6099\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3989 - accuracy: 0.8159 - val_loss: 1.3921 - val_accuracy: 0.6091\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3954 - accuracy: 0.8101 - val_loss: 1.4051 - val_accuracy: 0.6185\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3974 - accuracy: 0.8136 - val_loss: 1.3960 - val_accuracy: 0.6174\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3954 - accuracy: 0.8181 - val_loss: 1.4097 - val_accuracy: 0.6088\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3874 - accuracy: 0.8198 - val_loss: 1.4020 - val_accuracy: 0.6124\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3858 - accuracy: 0.8183 - val_loss: 1.4375 - val_accuracy: 0.6149\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3864 - accuracy: 0.8194 - val_loss: 1.4621 - val_accuracy: 0.6034\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3955 - accuracy: 0.8136 - val_loss: 1.4303 - val_accuracy: 0.6091\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3873 - accuracy: 0.8179 - val_loss: 1.4451 - val_accuracy: 0.6073\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3770 - accuracy: 0.8221 - val_loss: 1.4783 - val_accuracy: 0.6156\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3764 - accuracy: 0.8257 - val_loss: 1.4439 - val_accuracy: 0.6041\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3799 - accuracy: 0.8218 - val_loss: 1.4397 - val_accuracy: 0.6163\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3751 - accuracy: 0.8241 - val_loss: 1.5010 - val_accuracy: 0.6127\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3755 - accuracy: 0.8251 - val_loss: 1.4694 - val_accuracy: 0.6037\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3741 - accuracy: 0.8249 - val_loss: 1.4687 - val_accuracy: 0.6009\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3806 - accuracy: 0.8209 - val_loss: 1.5119 - val_accuracy: 0.6152\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3776 - accuracy: 0.8225 - val_loss: 1.4662 - val_accuracy: 0.6081\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3749 - accuracy: 0.8243 - val_loss: 1.5147 - val_accuracy: 0.6005\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3721 - accuracy: 0.8286 - val_loss: 1.4991 - val_accuracy: 0.6152\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3684 - accuracy: 0.8266 - val_loss: 1.5354 - val_accuracy: 0.5991\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3623 - accuracy: 0.8270 - val_loss: 1.5398 - val_accuracy: 0.6120\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3664 - accuracy: 0.8278 - val_loss: 1.5499 - val_accuracy: 0.6041\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3620 - accuracy: 0.8294 - val_loss: 1.5292 - val_accuracy: 0.6131\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3682 - accuracy: 0.8270 - val_loss: 1.5379 - val_accuracy: 0.6048\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3669 - accuracy: 0.8291 - val_loss: 1.5432 - val_accuracy: 0.5919\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3656 - accuracy: 0.8256 - val_loss: 1.5870 - val_accuracy: 0.5994\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3583 - accuracy: 0.8307 - val_loss: 1.5789 - val_accuracy: 0.6034\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3529 - accuracy: 0.8327 - val_loss: 1.6009 - val_accuracy: 0.6210\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3546 - accuracy: 0.8331 - val_loss: 1.5874 - val_accuracy: 0.6077\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3574 - accuracy: 0.8287 - val_loss: 1.6144 - val_accuracy: 0.5951\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3581 - accuracy: 0.8283 - val_loss: 1.5939 - val_accuracy: 0.5976\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3615 - accuracy: 0.8290 - val_loss: 1.6292 - val_accuracy: 0.6030\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3579 - accuracy: 0.8296 - val_loss: 1.5990 - val_accuracy: 0.5998\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3536 - accuracy: 0.8333 - val_loss: 1.6047 - val_accuracy: 0.6037\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3439 - accuracy: 0.8377 - val_loss: 1.6846 - val_accuracy: 0.6037\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3504 - accuracy: 0.8338 - val_loss: 1.6879 - val_accuracy: 0.6027\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3513 - accuracy: 0.8345 - val_loss: 1.6617 - val_accuracy: 0.6077\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3471 - accuracy: 0.8336 - val_loss: 1.7214 - val_accuracy: 0.6016\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3448 - accuracy: 0.8380 - val_loss: 1.6619 - val_accuracy: 0.6131\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3543 - accuracy: 0.8329 - val_loss: 1.7248 - val_accuracy: 0.6127\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3450 - accuracy: 0.8377 - val_loss: 1.6472 - val_accuracy: 0.6019\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3442 - accuracy: 0.8394 - val_loss: 1.6957 - val_accuracy: 0.6091\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3496 - accuracy: 0.8390 - val_loss: 1.6744 - val_accuracy: 0.6019\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3392 - accuracy: 0.8383 - val_loss: 1.6691 - val_accuracy: 0.6106\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3358 - accuracy: 0.8429 - val_loss: 1.7328 - val_accuracy: 0.5980\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3419 - accuracy: 0.8388 - val_loss: 1.7287 - val_accuracy: 0.5994\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3394 - accuracy: 0.8389 - val_loss: 1.7466 - val_accuracy: 0.6009\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3401 - accuracy: 0.8386 - val_loss: 1.6999 - val_accuracy: 0.6012\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3364 - accuracy: 0.8411 - val_loss: 1.7183 - val_accuracy: 0.5958\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3355 - accuracy: 0.8393 - val_loss: 1.6828 - val_accuracy: 0.6073\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3313 - accuracy: 0.8435 - val_loss: 1.7958 - val_accuracy: 0.6095\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3294 - accuracy: 0.8431 - val_loss: 1.7431 - val_accuracy: 0.6034\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3340 - accuracy: 0.8410 - val_loss: 1.7118 - val_accuracy: 0.6095\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3323 - accuracy: 0.8460 - val_loss: 1.7432 - val_accuracy: 0.6131\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3268 - accuracy: 0.8463 - val_loss: 1.7704 - val_accuracy: 0.6048\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3318 - accuracy: 0.8400 - val_loss: 1.8083 - val_accuracy: 0.6012\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3341 - accuracy: 0.8403 - val_loss: 1.7553 - val_accuracy: 0.6041\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3286 - accuracy: 0.8465 - val_loss: 1.8080 - val_accuracy: 0.6048\n",
            "History for model 10: <keras.src.callbacks.History object at 0x7e83fa4f0100>\n",
            "Trial 11: Number of layers = 3, Number of neurons per layer = 120\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 3s 5ms/step - loss: 0.8229 - accuracy: 0.6053 - val_loss: 0.8001 - val_accuracy: 0.6160\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7710 - accuracy: 0.6327 - val_loss: 0.7915 - val_accuracy: 0.6228\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7559 - accuracy: 0.6419 - val_loss: 0.7852 - val_accuracy: 0.6271\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7423 - accuracy: 0.6442 - val_loss: 0.7804 - val_accuracy: 0.6332\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7339 - accuracy: 0.6526 - val_loss: 0.7833 - val_accuracy: 0.6293\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7222 - accuracy: 0.6605 - val_loss: 0.7832 - val_accuracy: 0.6321\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7132 - accuracy: 0.6644 - val_loss: 0.7774 - val_accuracy: 0.6264\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7030 - accuracy: 0.6711 - val_loss: 0.7864 - val_accuracy: 0.6275\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6936 - accuracy: 0.6754 - val_loss: 0.7859 - val_accuracy: 0.6336\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6857 - accuracy: 0.6820 - val_loss: 0.7887 - val_accuracy: 0.6393\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6764 - accuracy: 0.6845 - val_loss: 0.7947 - val_accuracy: 0.6260\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.6689 - accuracy: 0.6894 - val_loss: 0.7997 - val_accuracy: 0.6339\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6919 - val_loss: 0.7937 - val_accuracy: 0.6268\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6549 - accuracy: 0.6917 - val_loss: 0.7979 - val_accuracy: 0.6347\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6452 - accuracy: 0.6962 - val_loss: 0.8051 - val_accuracy: 0.6379\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6410 - accuracy: 0.6990 - val_loss: 0.8120 - val_accuracy: 0.6242\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6290 - accuracy: 0.7096 - val_loss: 0.8168 - val_accuracy: 0.6426\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6236 - accuracy: 0.7084 - val_loss: 0.8189 - val_accuracy: 0.6311\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6181 - accuracy: 0.7113 - val_loss: 0.8311 - val_accuracy: 0.6339\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6111 - accuracy: 0.7162 - val_loss: 0.8453 - val_accuracy: 0.6300\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.6037 - accuracy: 0.7190 - val_loss: 0.8343 - val_accuracy: 0.6361\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5979 - accuracy: 0.7195 - val_loss: 0.8670 - val_accuracy: 0.6174\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5941 - accuracy: 0.7232 - val_loss: 0.8594 - val_accuracy: 0.6246\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5860 - accuracy: 0.7264 - val_loss: 0.8734 - val_accuracy: 0.6336\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5818 - accuracy: 0.7294 - val_loss: 0.9252 - val_accuracy: 0.6228\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5755 - accuracy: 0.7327 - val_loss: 0.8839 - val_accuracy: 0.6145\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5685 - accuracy: 0.7355 - val_loss: 0.8962 - val_accuracy: 0.6228\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5629 - accuracy: 0.7361 - val_loss: 0.8998 - val_accuracy: 0.6339\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5604 - accuracy: 0.7396 - val_loss: 0.9061 - val_accuracy: 0.6257\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5529 - accuracy: 0.7418 - val_loss: 0.9155 - val_accuracy: 0.6318\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5487 - accuracy: 0.7445 - val_loss: 0.9386 - val_accuracy: 0.6163\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5454 - accuracy: 0.7432 - val_loss: 0.9271 - val_accuracy: 0.6203\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5416 - accuracy: 0.7478 - val_loss: 0.9626 - val_accuracy: 0.6199\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5360 - accuracy: 0.7490 - val_loss: 0.9839 - val_accuracy: 0.6296\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5425 - accuracy: 0.7493 - val_loss: 0.9769 - val_accuracy: 0.6192\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5249 - accuracy: 0.7560 - val_loss: 1.0058 - val_accuracy: 0.6246\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5241 - accuracy: 0.7541 - val_loss: 0.9871 - val_accuracy: 0.6185\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5166 - accuracy: 0.7600 - val_loss: 1.0336 - val_accuracy: 0.6174\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5088 - accuracy: 0.7646 - val_loss: 1.0256 - val_accuracy: 0.6160\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5137 - accuracy: 0.7642 - val_loss: 1.0265 - val_accuracy: 0.6181\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5114 - accuracy: 0.7612 - val_loss: 1.0596 - val_accuracy: 0.6124\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5042 - accuracy: 0.7643 - val_loss: 1.0410 - val_accuracy: 0.6192\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.5035 - accuracy: 0.7634 - val_loss: 1.0153 - val_accuracy: 0.6199\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4945 - accuracy: 0.7696 - val_loss: 1.0570 - val_accuracy: 0.6163\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4920 - accuracy: 0.7683 - val_loss: 1.0948 - val_accuracy: 0.6192\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4872 - accuracy: 0.7732 - val_loss: 1.0663 - val_accuracy: 0.6163\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4846 - accuracy: 0.7711 - val_loss: 1.0654 - val_accuracy: 0.6206\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4849 - accuracy: 0.7739 - val_loss: 1.0914 - val_accuracy: 0.6185\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4769 - accuracy: 0.7777 - val_loss: 1.0975 - val_accuracy: 0.6174\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4808 - accuracy: 0.7748 - val_loss: 1.1399 - val_accuracy: 0.6156\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4742 - accuracy: 0.7789 - val_loss: 1.1455 - val_accuracy: 0.6131\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4699 - accuracy: 0.7816 - val_loss: 1.1221 - val_accuracy: 0.6185\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4703 - accuracy: 0.7813 - val_loss: 1.1933 - val_accuracy: 0.6278\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4642 - accuracy: 0.7825 - val_loss: 1.1794 - val_accuracy: 0.6289\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4585 - accuracy: 0.7838 - val_loss: 1.1680 - val_accuracy: 0.6066\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4524 - accuracy: 0.7866 - val_loss: 1.1704 - val_accuracy: 0.6120\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4576 - accuracy: 0.7844 - val_loss: 1.2265 - val_accuracy: 0.6124\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4531 - accuracy: 0.7860 - val_loss: 1.1974 - val_accuracy: 0.6106\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4497 - accuracy: 0.7896 - val_loss: 1.2278 - val_accuracy: 0.6145\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4415 - accuracy: 0.7946 - val_loss: 1.2312 - val_accuracy: 0.6181\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4385 - accuracy: 0.7943 - val_loss: 1.2734 - val_accuracy: 0.6063\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4403 - accuracy: 0.7911 - val_loss: 1.2408 - val_accuracy: 0.6081\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4313 - accuracy: 0.7983 - val_loss: 1.2547 - val_accuracy: 0.6052\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4311 - accuracy: 0.7979 - val_loss: 1.2996 - val_accuracy: 0.6152\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4402 - accuracy: 0.7915 - val_loss: 1.2411 - val_accuracy: 0.6095\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4328 - accuracy: 0.7978 - val_loss: 1.2817 - val_accuracy: 0.5958\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4229 - accuracy: 0.8005 - val_loss: 1.2757 - val_accuracy: 0.6095\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4221 - accuracy: 0.8003 - val_loss: 1.3146 - val_accuracy: 0.6156\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4285 - accuracy: 0.7992 - val_loss: 1.2803 - val_accuracy: 0.6048\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.4154 - accuracy: 0.8055 - val_loss: 1.3290 - val_accuracy: 0.6167\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4126 - accuracy: 0.8022 - val_loss: 1.3324 - val_accuracy: 0.6088\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4129 - accuracy: 0.8054 - val_loss: 1.3381 - val_accuracy: 0.6188\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4118 - accuracy: 0.8056 - val_loss: 1.3318 - val_accuracy: 0.6084\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4146 - accuracy: 0.8064 - val_loss: 1.3622 - val_accuracy: 0.6052\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4085 - accuracy: 0.8083 - val_loss: 1.3856 - val_accuracy: 0.5991\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4030 - accuracy: 0.8127 - val_loss: 1.4272 - val_accuracy: 0.6088\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3993 - accuracy: 0.8129 - val_loss: 1.3860 - val_accuracy: 0.6124\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4031 - accuracy: 0.8079 - val_loss: 1.3927 - val_accuracy: 0.6091\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4000 - accuracy: 0.8127 - val_loss: 1.3995 - val_accuracy: 0.5969\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.4003 - accuracy: 0.8137 - val_loss: 1.4271 - val_accuracy: 0.5994\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3937 - accuracy: 0.8162 - val_loss: 1.4442 - val_accuracy: 0.6016\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3958 - accuracy: 0.8146 - val_loss: 1.4245 - val_accuracy: 0.6127\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3912 - accuracy: 0.8171 - val_loss: 1.4366 - val_accuracy: 0.6063\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3875 - accuracy: 0.8154 - val_loss: 1.4477 - val_accuracy: 0.6102\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3858 - accuracy: 0.8167 - val_loss: 1.4690 - val_accuracy: 0.6145\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3866 - accuracy: 0.8140 - val_loss: 1.4514 - val_accuracy: 0.5991\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3860 - accuracy: 0.8154 - val_loss: 1.4759 - val_accuracy: 0.5973\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3804 - accuracy: 0.8179 - val_loss: 1.5394 - val_accuracy: 0.5976\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3840 - accuracy: 0.8148 - val_loss: 1.5247 - val_accuracy: 0.6134\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3732 - accuracy: 0.8221 - val_loss: 1.5404 - val_accuracy: 0.5958\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3800 - accuracy: 0.8201 - val_loss: 1.4791 - val_accuracy: 0.6045\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3709 - accuracy: 0.8205 - val_loss: 1.5248 - val_accuracy: 0.5915\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3716 - accuracy: 0.8241 - val_loss: 1.5681 - val_accuracy: 0.5969\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3795 - accuracy: 0.8230 - val_loss: 1.5438 - val_accuracy: 0.6005\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3657 - accuracy: 0.8256 - val_loss: 1.5821 - val_accuracy: 0.6016\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3629 - accuracy: 0.8277 - val_loss: 1.5855 - val_accuracy: 0.6063\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3609 - accuracy: 0.8284 - val_loss: 1.6186 - val_accuracy: 0.6037\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3666 - accuracy: 0.8244 - val_loss: 1.5900 - val_accuracy: 0.6023\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3616 - accuracy: 0.8273 - val_loss: 1.6350 - val_accuracy: 0.6088\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3584 - accuracy: 0.8242 - val_loss: 1.6035 - val_accuracy: 0.5987\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3628 - accuracy: 0.8265 - val_loss: 1.5429 - val_accuracy: 0.6041\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3547 - accuracy: 0.8325 - val_loss: 1.6384 - val_accuracy: 0.6027\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3576 - accuracy: 0.8302 - val_loss: 1.6274 - val_accuracy: 0.6145\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3625 - accuracy: 0.8297 - val_loss: 1.6045 - val_accuracy: 0.5962\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3557 - accuracy: 0.8305 - val_loss: 1.6837 - val_accuracy: 0.5976\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3512 - accuracy: 0.8321 - val_loss: 1.6537 - val_accuracy: 0.6045\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3528 - accuracy: 0.8278 - val_loss: 1.6568 - val_accuracy: 0.5915\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3541 - accuracy: 0.8306 - val_loss: 1.5937 - val_accuracy: 0.6106\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3465 - accuracy: 0.8322 - val_loss: 1.6630 - val_accuracy: 0.5958\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3469 - accuracy: 0.8350 - val_loss: 1.6449 - val_accuracy: 0.6059\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3541 - accuracy: 0.8322 - val_loss: 1.6753 - val_accuracy: 0.5908\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3409 - accuracy: 0.8360 - val_loss: 1.7200 - val_accuracy: 0.6016\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3357 - accuracy: 0.8367 - val_loss: 1.7476 - val_accuracy: 0.6052\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3316 - accuracy: 0.8401 - val_loss: 1.7225 - val_accuracy: 0.5951\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3392 - accuracy: 0.8355 - val_loss: 1.7606 - val_accuracy: 0.5962\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3430 - accuracy: 0.8347 - val_loss: 1.7171 - val_accuracy: 0.6016\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3361 - accuracy: 0.8379 - val_loss: 1.7280 - val_accuracy: 0.5987\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3369 - accuracy: 0.8410 - val_loss: 1.7980 - val_accuracy: 0.6095\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3418 - accuracy: 0.8386 - val_loss: 1.7808 - val_accuracy: 0.5987\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3360 - accuracy: 0.8391 - val_loss: 1.7548 - val_accuracy: 0.6012\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3324 - accuracy: 0.8394 - val_loss: 1.8199 - val_accuracy: 0.5955\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3316 - accuracy: 0.8399 - val_loss: 1.7598 - val_accuracy: 0.5915\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3277 - accuracy: 0.8403 - val_loss: 1.7475 - val_accuracy: 0.5937\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3345 - accuracy: 0.8377 - val_loss: 1.7817 - val_accuracy: 0.5958\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3283 - accuracy: 0.8430 - val_loss: 1.7692 - val_accuracy: 0.5976\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3280 - accuracy: 0.8386 - val_loss: 1.7647 - val_accuracy: 0.5965\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3226 - accuracy: 0.8420 - val_loss: 1.7602 - val_accuracy: 0.5991\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3280 - accuracy: 0.8448 - val_loss: 1.7471 - val_accuracy: 0.5908\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3256 - accuracy: 0.8467 - val_loss: 1.8320 - val_accuracy: 0.5872\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3266 - accuracy: 0.8388 - val_loss: 1.8811 - val_accuracy: 0.5998\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3217 - accuracy: 0.8455 - val_loss: 1.8560 - val_accuracy: 0.5937\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3235 - accuracy: 0.8424 - val_loss: 1.8174 - val_accuracy: 0.6027\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3091 - accuracy: 0.8533 - val_loss: 1.8982 - val_accuracy: 0.5901\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3184 - accuracy: 0.8473 - val_loss: 1.8490 - val_accuracy: 0.5872\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3163 - accuracy: 0.8444 - val_loss: 1.8496 - val_accuracy: 0.5958\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3198 - accuracy: 0.8481 - val_loss: 1.8442 - val_accuracy: 0.5983\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.3168 - accuracy: 0.8480 - val_loss: 1.9548 - val_accuracy: 0.5944\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3254 - accuracy: 0.8434 - val_loss: 1.8484 - val_accuracy: 0.6012\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3140 - accuracy: 0.8488 - val_loss: 1.9422 - val_accuracy: 0.6034\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3069 - accuracy: 0.8499 - val_loss: 1.9185 - val_accuracy: 0.5994\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3108 - accuracy: 0.8505 - val_loss: 1.8845 - val_accuracy: 0.5980\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3102 - accuracy: 0.8466 - val_loss: 1.9149 - val_accuracy: 0.5930\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3085 - accuracy: 0.8491 - val_loss: 1.9638 - val_accuracy: 0.6012\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3097 - accuracy: 0.8474 - val_loss: 1.9808 - val_accuracy: 0.5933\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3143 - accuracy: 0.8467 - val_loss: 1.9355 - val_accuracy: 0.5973\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.3043 - accuracy: 0.8539 - val_loss: 1.9995 - val_accuracy: 0.5868\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2986 - accuracy: 0.8564 - val_loss: 2.0393 - val_accuracy: 0.5944\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3102 - accuracy: 0.8464 - val_loss: 1.9856 - val_accuracy: 0.5858\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3111 - accuracy: 0.8481 - val_loss: 2.0040 - val_accuracy: 0.5876\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3115 - accuracy: 0.8480 - val_loss: 2.0793 - val_accuracy: 0.5948\n",
            "History for model 11: <keras.src.callbacks.History object at 0x7e83fa285750>\n",
            "Trial 12: Number of layers = 3, Number of neurons per layer = 150\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 3s 5ms/step - loss: 0.8203 - accuracy: 0.6108 - val_loss: 0.8024 - val_accuracy: 0.6131\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7692 - accuracy: 0.6322 - val_loss: 0.7919 - val_accuracy: 0.6314\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.7528 - accuracy: 0.6436 - val_loss: 0.7750 - val_accuracy: 0.6368\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 1s 4ms/step - loss: 0.7403 - accuracy: 0.6532 - val_loss: 0.7744 - val_accuracy: 0.6361\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7277 - accuracy: 0.6598 - val_loss: 0.7772 - val_accuracy: 0.6433\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7183 - accuracy: 0.6599 - val_loss: 0.7880 - val_accuracy: 0.6433\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7099 - accuracy: 0.6677 - val_loss: 0.7701 - val_accuracy: 0.6440\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6988 - accuracy: 0.6719 - val_loss: 0.7781 - val_accuracy: 0.6451\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6912 - accuracy: 0.6778 - val_loss: 0.7856 - val_accuracy: 0.6329\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6789 - accuracy: 0.6802 - val_loss: 0.7856 - val_accuracy: 0.6303\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6720 - accuracy: 0.6864 - val_loss: 0.7801 - val_accuracy: 0.6357\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6636 - accuracy: 0.6911 - val_loss: 0.7793 - val_accuracy: 0.6329\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6546 - accuracy: 0.6954 - val_loss: 0.7930 - val_accuracy: 0.6354\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6477 - accuracy: 0.6975 - val_loss: 0.7984 - val_accuracy: 0.6293\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6370 - accuracy: 0.7027 - val_loss: 0.7963 - val_accuracy: 0.6393\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6302 - accuracy: 0.7060 - val_loss: 0.8070 - val_accuracy: 0.6181\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6251 - accuracy: 0.7046 - val_loss: 0.8211 - val_accuracy: 0.6300\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6143 - accuracy: 0.7147 - val_loss: 0.8153 - val_accuracy: 0.6318\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6076 - accuracy: 0.7140 - val_loss: 0.8211 - val_accuracy: 0.6325\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5978 - accuracy: 0.7220 - val_loss: 0.8367 - val_accuracy: 0.6192\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5913 - accuracy: 0.7268 - val_loss: 0.8364 - val_accuracy: 0.6268\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5835 - accuracy: 0.7292 - val_loss: 0.8549 - val_accuracy: 0.6350\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5790 - accuracy: 0.7309 - val_loss: 0.8525 - val_accuracy: 0.6181\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5689 - accuracy: 0.7353 - val_loss: 0.8640 - val_accuracy: 0.6239\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5666 - accuracy: 0.7322 - val_loss: 0.8687 - val_accuracy: 0.6271\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5571 - accuracy: 0.7406 - val_loss: 0.8984 - val_accuracy: 0.6203\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5480 - accuracy: 0.7452 - val_loss: 0.9068 - val_accuracy: 0.6296\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5450 - accuracy: 0.7463 - val_loss: 0.8840 - val_accuracy: 0.6239\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5397 - accuracy: 0.7475 - val_loss: 0.9219 - val_accuracy: 0.6206\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5325 - accuracy: 0.7538 - val_loss: 0.9375 - val_accuracy: 0.6307\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5281 - accuracy: 0.7582 - val_loss: 0.9492 - val_accuracy: 0.6102\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5221 - accuracy: 0.7567 - val_loss: 0.9633 - val_accuracy: 0.6145\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5158 - accuracy: 0.7619 - val_loss: 0.9561 - val_accuracy: 0.6196\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5139 - accuracy: 0.7605 - val_loss: 0.9729 - val_accuracy: 0.6117\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5052 - accuracy: 0.7614 - val_loss: 0.9683 - val_accuracy: 0.6178\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5013 - accuracy: 0.7683 - val_loss: 0.9983 - val_accuracy: 0.6268\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4988 - accuracy: 0.7656 - val_loss: 0.9817 - val_accuracy: 0.6329\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4881 - accuracy: 0.7757 - val_loss: 1.0139 - val_accuracy: 0.6023\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4864 - accuracy: 0.7769 - val_loss: 1.0265 - val_accuracy: 0.6224\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4841 - accuracy: 0.7739 - val_loss: 1.0298 - val_accuracy: 0.6167\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4786 - accuracy: 0.7808 - val_loss: 1.0611 - val_accuracy: 0.6149\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4732 - accuracy: 0.7774 - val_loss: 1.0722 - val_accuracy: 0.6012\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4694 - accuracy: 0.7819 - val_loss: 1.0675 - val_accuracy: 0.6113\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4675 - accuracy: 0.7808 - val_loss: 1.1249 - val_accuracy: 0.6070\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4600 - accuracy: 0.7874 - val_loss: 1.0705 - val_accuracy: 0.6206\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4575 - accuracy: 0.7869 - val_loss: 1.1413 - val_accuracy: 0.6138\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4494 - accuracy: 0.7849 - val_loss: 1.1562 - val_accuracy: 0.6224\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4503 - accuracy: 0.7878 - val_loss: 1.1612 - val_accuracy: 0.6084\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4454 - accuracy: 0.7905 - val_loss: 1.1856 - val_accuracy: 0.6178\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4398 - accuracy: 0.7923 - val_loss: 1.1894 - val_accuracy: 0.6059\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4361 - accuracy: 0.7939 - val_loss: 1.2013 - val_accuracy: 0.6059\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4327 - accuracy: 0.7983 - val_loss: 1.2127 - val_accuracy: 0.6127\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4287 - accuracy: 0.7942 - val_loss: 1.2240 - val_accuracy: 0.6084\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4275 - accuracy: 0.8007 - val_loss: 1.1977 - val_accuracy: 0.6001\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4234 - accuracy: 0.8023 - val_loss: 1.2578 - val_accuracy: 0.6030\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4241 - accuracy: 0.8047 - val_loss: 1.2014 - val_accuracy: 0.6134\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4209 - accuracy: 0.8055 - val_loss: 1.2337 - val_accuracy: 0.6152\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4154 - accuracy: 0.8036 - val_loss: 1.2328 - val_accuracy: 0.6023\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4087 - accuracy: 0.8083 - val_loss: 1.2662 - val_accuracy: 0.6045\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4075 - accuracy: 0.8091 - val_loss: 1.2802 - val_accuracy: 0.6120\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4053 - accuracy: 0.8118 - val_loss: 1.2588 - val_accuracy: 0.6055\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4030 - accuracy: 0.8098 - val_loss: 1.3094 - val_accuracy: 0.5998\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3962 - accuracy: 0.8146 - val_loss: 1.3035 - val_accuracy: 0.6206\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3973 - accuracy: 0.8120 - val_loss: 1.3776 - val_accuracy: 0.5937\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3916 - accuracy: 0.8155 - val_loss: 1.3574 - val_accuracy: 0.6127\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3907 - accuracy: 0.8153 - val_loss: 1.3614 - val_accuracy: 0.6016\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3890 - accuracy: 0.8165 - val_loss: 1.3869 - val_accuracy: 0.6066\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3803 - accuracy: 0.8205 - val_loss: 1.4341 - val_accuracy: 0.5955\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3846 - accuracy: 0.8176 - val_loss: 1.3857 - val_accuracy: 0.5991\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3890 - accuracy: 0.8182 - val_loss: 1.3797 - val_accuracy: 0.5980\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3843 - accuracy: 0.8190 - val_loss: 1.3824 - val_accuracy: 0.6016\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3789 - accuracy: 0.8214 - val_loss: 1.4120 - val_accuracy: 0.6055\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3704 - accuracy: 0.8260 - val_loss: 1.4333 - val_accuracy: 0.6048\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3686 - accuracy: 0.8235 - val_loss: 1.4210 - val_accuracy: 0.5944\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3676 - accuracy: 0.8258 - val_loss: 1.4397 - val_accuracy: 0.6030\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3643 - accuracy: 0.8257 - val_loss: 1.4209 - val_accuracy: 0.5901\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3664 - accuracy: 0.8270 - val_loss: 1.4193 - val_accuracy: 0.5980\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3621 - accuracy: 0.8291 - val_loss: 1.4662 - val_accuracy: 0.6027\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3603 - accuracy: 0.8281 - val_loss: 1.5614 - val_accuracy: 0.5969\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3614 - accuracy: 0.8304 - val_loss: 1.5406 - val_accuracy: 0.6009\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3530 - accuracy: 0.8354 - val_loss: 1.5013 - val_accuracy: 0.6052\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3554 - accuracy: 0.8314 - val_loss: 1.5540 - val_accuracy: 0.5994\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3488 - accuracy: 0.8328 - val_loss: 1.5196 - val_accuracy: 0.6117\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3466 - accuracy: 0.8364 - val_loss: 1.5326 - val_accuracy: 0.6048\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3493 - accuracy: 0.8332 - val_loss: 1.5514 - val_accuracy: 0.6005\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3504 - accuracy: 0.8352 - val_loss: 1.6250 - val_accuracy: 0.5912\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3480 - accuracy: 0.8346 - val_loss: 1.5655 - val_accuracy: 0.5980\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3466 - accuracy: 0.8353 - val_loss: 1.6188 - val_accuracy: 0.6023\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3384 - accuracy: 0.8380 - val_loss: 1.5632 - val_accuracy: 0.5890\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3342 - accuracy: 0.8372 - val_loss: 1.6904 - val_accuracy: 0.6055\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3365 - accuracy: 0.8412 - val_loss: 1.6469 - val_accuracy: 0.5937\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3479 - accuracy: 0.8363 - val_loss: 1.6245 - val_accuracy: 0.6041\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3298 - accuracy: 0.8445 - val_loss: 1.6706 - val_accuracy: 0.5969\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3306 - accuracy: 0.8413 - val_loss: 1.6831 - val_accuracy: 0.5998\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3380 - accuracy: 0.8412 - val_loss: 1.7094 - val_accuracy: 0.5983\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3254 - accuracy: 0.8411 - val_loss: 1.6521 - val_accuracy: 0.5951\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3214 - accuracy: 0.8456 - val_loss: 1.6907 - val_accuracy: 0.5958\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3283 - accuracy: 0.8462 - val_loss: 1.6761 - val_accuracy: 0.5951\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3247 - accuracy: 0.8458 - val_loss: 1.7162 - val_accuracy: 0.5973\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3207 - accuracy: 0.8461 - val_loss: 1.7005 - val_accuracy: 0.5933\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3216 - accuracy: 0.8479 - val_loss: 1.7173 - val_accuracy: 0.6102\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3231 - accuracy: 0.8450 - val_loss: 1.7554 - val_accuracy: 0.5872\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3253 - accuracy: 0.8460 - val_loss: 1.7472 - val_accuracy: 0.5962\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3152 - accuracy: 0.8440 - val_loss: 1.7271 - val_accuracy: 0.6063\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3132 - accuracy: 0.8521 - val_loss: 1.7426 - val_accuracy: 0.6055\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3096 - accuracy: 0.8490 - val_loss: 1.7579 - val_accuracy: 0.5944\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3068 - accuracy: 0.8528 - val_loss: 1.7821 - val_accuracy: 0.6027\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3077 - accuracy: 0.8505 - val_loss: 1.7588 - val_accuracy: 0.6030\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3067 - accuracy: 0.8503 - val_loss: 1.8261 - val_accuracy: 0.6027\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3073 - accuracy: 0.8511 - val_loss: 1.8375 - val_accuracy: 0.5987\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3012 - accuracy: 0.8541 - val_loss: 1.7892 - val_accuracy: 0.6037\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3015 - accuracy: 0.8536 - val_loss: 1.8831 - val_accuracy: 0.5983\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3100 - accuracy: 0.8498 - val_loss: 1.8996 - val_accuracy: 0.5980\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3067 - accuracy: 0.8550 - val_loss: 1.8873 - val_accuracy: 0.5912\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3003 - accuracy: 0.8543 - val_loss: 1.9371 - val_accuracy: 0.5886\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3172 - accuracy: 0.8483 - val_loss: 1.8738 - val_accuracy: 0.5951\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3062 - accuracy: 0.8518 - val_loss: 1.8888 - val_accuracy: 0.6023\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3000 - accuracy: 0.8511 - val_loss: 1.8564 - val_accuracy: 0.5948\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2929 - accuracy: 0.8548 - val_loss: 1.8384 - val_accuracy: 0.5894\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2968 - accuracy: 0.8574 - val_loss: 1.9166 - val_accuracy: 0.5951\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2957 - accuracy: 0.8566 - val_loss: 1.9678 - val_accuracy: 0.5883\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2996 - accuracy: 0.8559 - val_loss: 1.8988 - val_accuracy: 0.6023\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2872 - accuracy: 0.8582 - val_loss: 1.9582 - val_accuracy: 0.5958\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2928 - accuracy: 0.8586 - val_loss: 1.8955 - val_accuracy: 0.5930\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2909 - accuracy: 0.8574 - val_loss: 1.9622 - val_accuracy: 0.5969\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2832 - accuracy: 0.8581 - val_loss: 1.9441 - val_accuracy: 0.5980\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2885 - accuracy: 0.8570 - val_loss: 1.9531 - val_accuracy: 0.5958\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2917 - accuracy: 0.8588 - val_loss: 1.8939 - val_accuracy: 0.5940\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2864 - accuracy: 0.8587 - val_loss: 2.0068 - val_accuracy: 0.5969\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3001 - accuracy: 0.8577 - val_loss: 1.9471 - val_accuracy: 0.5915\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2927 - accuracy: 0.8586 - val_loss: 1.9845 - val_accuracy: 0.5854\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2892 - accuracy: 0.8605 - val_loss: 1.9375 - val_accuracy: 0.5973\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2805 - accuracy: 0.8626 - val_loss: 2.0700 - val_accuracy: 0.5973\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2833 - accuracy: 0.8632 - val_loss: 2.0423 - val_accuracy: 0.5958\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2766 - accuracy: 0.8620 - val_loss: 2.0235 - val_accuracy: 0.5861\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2754 - accuracy: 0.8664 - val_loss: 2.0582 - val_accuracy: 0.5969\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2753 - accuracy: 0.8643 - val_loss: 2.0956 - val_accuracy: 0.5930\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2724 - accuracy: 0.8667 - val_loss: 2.0696 - val_accuracy: 0.5948\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2871 - accuracy: 0.8592 - val_loss: 2.0444 - val_accuracy: 0.5915\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2822 - accuracy: 0.8608 - val_loss: 2.0270 - val_accuracy: 0.5933\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2808 - accuracy: 0.8614 - val_loss: 2.0622 - val_accuracy: 0.5930\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2846 - accuracy: 0.8629 - val_loss: 2.0506 - val_accuracy: 0.5886\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2782 - accuracy: 0.8644 - val_loss: 2.0405 - val_accuracy: 0.5940\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2868 - accuracy: 0.8610 - val_loss: 2.0298 - val_accuracy: 0.5930\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2747 - accuracy: 0.8669 - val_loss: 2.0238 - val_accuracy: 0.5915\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2699 - accuracy: 0.8688 - val_loss: 2.0926 - val_accuracy: 0.6001\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2707 - accuracy: 0.8661 - val_loss: 2.0350 - val_accuracy: 0.5951\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2662 - accuracy: 0.8642 - val_loss: 2.0815 - val_accuracy: 0.5958\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2631 - accuracy: 0.8681 - val_loss: 2.0523 - val_accuracy: 0.5912\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2649 - accuracy: 0.8694 - val_loss: 2.1434 - val_accuracy: 0.5944\n",
            "History for model 12: <keras.src.callbacks.History object at 0x7e83fa037d90>\n",
            "Trial 13: Number of layers = 3, Number of neurons per layer = 180\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 3s 6ms/step - loss: 0.8250 - accuracy: 0.6072 - val_loss: 0.7873 - val_accuracy: 0.6214\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7697 - accuracy: 0.6307 - val_loss: 0.7894 - val_accuracy: 0.6228\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7571 - accuracy: 0.6419 - val_loss: 0.7935 - val_accuracy: 0.6160\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7422 - accuracy: 0.6471 - val_loss: 0.7819 - val_accuracy: 0.6332\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7310 - accuracy: 0.6539 - val_loss: 0.7671 - val_accuracy: 0.6357\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7200 - accuracy: 0.6575 - val_loss: 0.7843 - val_accuracy: 0.6357\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7076 - accuracy: 0.6668 - val_loss: 0.7752 - val_accuracy: 0.6332\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6989 - accuracy: 0.6717 - val_loss: 0.7896 - val_accuracy: 0.6343\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6890 - accuracy: 0.6787 - val_loss: 0.8028 - val_accuracy: 0.6300\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6811 - accuracy: 0.6798 - val_loss: 0.7889 - val_accuracy: 0.6386\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6702 - accuracy: 0.6868 - val_loss: 0.7815 - val_accuracy: 0.6357\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6639 - accuracy: 0.6889 - val_loss: 0.7976 - val_accuracy: 0.6339\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6551 - accuracy: 0.6906 - val_loss: 0.7999 - val_accuracy: 0.6386\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6452 - accuracy: 0.7001 - val_loss: 0.8089 - val_accuracy: 0.6365\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6373 - accuracy: 0.7006 - val_loss: 0.7997 - val_accuracy: 0.6368\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6293 - accuracy: 0.7042 - val_loss: 0.8138 - val_accuracy: 0.6268\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6223 - accuracy: 0.7133 - val_loss: 0.8281 - val_accuracy: 0.6332\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6170 - accuracy: 0.7132 - val_loss: 0.8227 - val_accuracy: 0.6307\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6063 - accuracy: 0.7180 - val_loss: 0.8489 - val_accuracy: 0.6286\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5987 - accuracy: 0.7213 - val_loss: 0.8575 - val_accuracy: 0.6332\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5885 - accuracy: 0.7269 - val_loss: 0.8474 - val_accuracy: 0.6250\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5856 - accuracy: 0.7264 - val_loss: 0.8641 - val_accuracy: 0.6329\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5762 - accuracy: 0.7318 - val_loss: 0.8817 - val_accuracy: 0.6160\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.5691 - accuracy: 0.7353 - val_loss: 0.8764 - val_accuracy: 0.6321\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5620 - accuracy: 0.7415 - val_loss: 0.9024 - val_accuracy: 0.6253\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5552 - accuracy: 0.7418 - val_loss: 0.9036 - val_accuracy: 0.6210\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5513 - accuracy: 0.7471 - val_loss: 0.9104 - val_accuracy: 0.6325\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5469 - accuracy: 0.7475 - val_loss: 0.9335 - val_accuracy: 0.6206\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5407 - accuracy: 0.7503 - val_loss: 0.9277 - val_accuracy: 0.6224\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5334 - accuracy: 0.7513 - val_loss: 0.9484 - val_accuracy: 0.6250\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5288 - accuracy: 0.7556 - val_loss: 0.9924 - val_accuracy: 0.6232\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5231 - accuracy: 0.7610 - val_loss: 0.9916 - val_accuracy: 0.6257\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5143 - accuracy: 0.7619 - val_loss: 1.0191 - val_accuracy: 0.6214\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5109 - accuracy: 0.7647 - val_loss: 0.9847 - val_accuracy: 0.6235\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5014 - accuracy: 0.7648 - val_loss: 1.0220 - val_accuracy: 0.6224\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4989 - accuracy: 0.7710 - val_loss: 1.0210 - val_accuracy: 0.6235\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4956 - accuracy: 0.7691 - val_loss: 1.0389 - val_accuracy: 0.6174\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4864 - accuracy: 0.7734 - val_loss: 1.0840 - val_accuracy: 0.6131\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4855 - accuracy: 0.7738 - val_loss: 1.0450 - val_accuracy: 0.6206\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4810 - accuracy: 0.7774 - val_loss: 1.0742 - val_accuracy: 0.6250\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4717 - accuracy: 0.7817 - val_loss: 1.0920 - val_accuracy: 0.6167\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4751 - accuracy: 0.7809 - val_loss: 1.0916 - val_accuracy: 0.6145\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4699 - accuracy: 0.7802 - val_loss: 1.0986 - val_accuracy: 0.6170\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4597 - accuracy: 0.7851 - val_loss: 1.1308 - val_accuracy: 0.6192\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4561 - accuracy: 0.7852 - val_loss: 1.1419 - val_accuracy: 0.6142\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4523 - accuracy: 0.7898 - val_loss: 1.1545 - val_accuracy: 0.6264\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4463 - accuracy: 0.7919 - val_loss: 1.1761 - val_accuracy: 0.6149\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4444 - accuracy: 0.7909 - val_loss: 1.2242 - val_accuracy: 0.6077\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4482 - accuracy: 0.7923 - val_loss: 1.1805 - val_accuracy: 0.6163\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4357 - accuracy: 0.7956 - val_loss: 1.2543 - val_accuracy: 0.6196\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4340 - accuracy: 0.7958 - val_loss: 1.1969 - val_accuracy: 0.6081\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4292 - accuracy: 0.7975 - val_loss: 1.2493 - val_accuracy: 0.6037\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4253 - accuracy: 0.7991 - val_loss: 1.2353 - val_accuracy: 0.6091\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4212 - accuracy: 0.8015 - val_loss: 1.2509 - val_accuracy: 0.6167\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4195 - accuracy: 0.8000 - val_loss: 1.2481 - val_accuracy: 0.6138\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4101 - accuracy: 0.8083 - val_loss: 1.2684 - val_accuracy: 0.6052\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4146 - accuracy: 0.8025 - val_loss: 1.2728 - val_accuracy: 0.6066\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4090 - accuracy: 0.8081 - val_loss: 1.2716 - val_accuracy: 0.6045\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4108 - accuracy: 0.8079 - val_loss: 1.3088 - val_accuracy: 0.6113\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4082 - accuracy: 0.8056 - val_loss: 1.2887 - val_accuracy: 0.6077\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3975 - accuracy: 0.8099 - val_loss: 1.3009 - val_accuracy: 0.6059\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3942 - accuracy: 0.8148 - val_loss: 1.3477 - val_accuracy: 0.6088\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3931 - accuracy: 0.8104 - val_loss: 1.3850 - val_accuracy: 0.6088\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3905 - accuracy: 0.8146 - val_loss: 1.3819 - val_accuracy: 0.6081\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3844 - accuracy: 0.8162 - val_loss: 1.3686 - val_accuracy: 0.6073\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3837 - accuracy: 0.8154 - val_loss: 1.4101 - val_accuracy: 0.6081\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3850 - accuracy: 0.8172 - val_loss: 1.3970 - val_accuracy: 0.6016\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3782 - accuracy: 0.8186 - val_loss: 1.3908 - val_accuracy: 0.6052\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3728 - accuracy: 0.8207 - val_loss: 1.3915 - val_accuracy: 0.6221\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3756 - accuracy: 0.8187 - val_loss: 1.3742 - val_accuracy: 0.6066\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3714 - accuracy: 0.8247 - val_loss: 1.4293 - val_accuracy: 0.5919\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3718 - accuracy: 0.8230 - val_loss: 1.4534 - val_accuracy: 0.5994\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3625 - accuracy: 0.8251 - val_loss: 1.4809 - val_accuracy: 0.6030\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3627 - accuracy: 0.8234 - val_loss: 1.5156 - val_accuracy: 0.6048\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3627 - accuracy: 0.8280 - val_loss: 1.4922 - val_accuracy: 0.6073\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3554 - accuracy: 0.8291 - val_loss: 1.5459 - val_accuracy: 0.6041\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3538 - accuracy: 0.8290 - val_loss: 1.4797 - val_accuracy: 0.5991\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3514 - accuracy: 0.8301 - val_loss: 1.4949 - val_accuracy: 0.6070\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3558 - accuracy: 0.8291 - val_loss: 1.5725 - val_accuracy: 0.6102\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3512 - accuracy: 0.8299 - val_loss: 1.6144 - val_accuracy: 0.5973\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3470 - accuracy: 0.8328 - val_loss: 1.6344 - val_accuracy: 0.6066\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3421 - accuracy: 0.8332 - val_loss: 1.5681 - val_accuracy: 0.6048\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3422 - accuracy: 0.8348 - val_loss: 1.6506 - val_accuracy: 0.5991\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3358 - accuracy: 0.8387 - val_loss: 1.6389 - val_accuracy: 0.5998\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3406 - accuracy: 0.8348 - val_loss: 1.6131 - val_accuracy: 0.6041\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3392 - accuracy: 0.8376 - val_loss: 1.6289 - val_accuracy: 0.5983\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3484 - accuracy: 0.8364 - val_loss: 1.5810 - val_accuracy: 0.6034\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3383 - accuracy: 0.8403 - val_loss: 1.6742 - val_accuracy: 0.6070\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3329 - accuracy: 0.8433 - val_loss: 1.6793 - val_accuracy: 0.6077\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3311 - accuracy: 0.8410 - val_loss: 1.6238 - val_accuracy: 0.5962\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3226 - accuracy: 0.8442 - val_loss: 1.6686 - val_accuracy: 0.5940\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3206 - accuracy: 0.8426 - val_loss: 1.6663 - val_accuracy: 0.6037\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3180 - accuracy: 0.8456 - val_loss: 1.7130 - val_accuracy: 0.6045\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3217 - accuracy: 0.8446 - val_loss: 1.7196 - val_accuracy: 0.5994\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3139 - accuracy: 0.8474 - val_loss: 1.7370 - val_accuracy: 0.6045\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3119 - accuracy: 0.8486 - val_loss: 1.7225 - val_accuracy: 0.5962\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3208 - accuracy: 0.8448 - val_loss: 1.6915 - val_accuracy: 0.5944\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3115 - accuracy: 0.8483 - val_loss: 1.7687 - val_accuracy: 0.6005\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3124 - accuracy: 0.8478 - val_loss: 1.7583 - val_accuracy: 0.6009\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3204 - accuracy: 0.8455 - val_loss: 1.7759 - val_accuracy: 0.5987\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3108 - accuracy: 0.8484 - val_loss: 1.7954 - val_accuracy: 0.6005\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3073 - accuracy: 0.8513 - val_loss: 1.8114 - val_accuracy: 0.6005\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3043 - accuracy: 0.8523 - val_loss: 1.8521 - val_accuracy: 0.5969\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3081 - accuracy: 0.8486 - val_loss: 1.7844 - val_accuracy: 0.6009\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3045 - accuracy: 0.8528 - val_loss: 1.8400 - val_accuracy: 0.5962\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3005 - accuracy: 0.8524 - val_loss: 1.8695 - val_accuracy: 0.5969\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3035 - accuracy: 0.8507 - val_loss: 1.7756 - val_accuracy: 0.5962\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3064 - accuracy: 0.8538 - val_loss: 1.8185 - val_accuracy: 0.5969\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3040 - accuracy: 0.8537 - val_loss: 1.9523 - val_accuracy: 0.5962\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2995 - accuracy: 0.8543 - val_loss: 1.8495 - val_accuracy: 0.5994\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2968 - accuracy: 0.8539 - val_loss: 1.9442 - val_accuracy: 0.5919\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2978 - accuracy: 0.8562 - val_loss: 1.8757 - val_accuracy: 0.5991\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3026 - accuracy: 0.8569 - val_loss: 1.9111 - val_accuracy: 0.5883\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2958 - accuracy: 0.8565 - val_loss: 1.8850 - val_accuracy: 0.5980\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2857 - accuracy: 0.8593 - val_loss: 1.8991 - val_accuracy: 0.5886\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2845 - accuracy: 0.8580 - val_loss: 1.8808 - val_accuracy: 0.5944\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2788 - accuracy: 0.8604 - val_loss: 1.9838 - val_accuracy: 0.5901\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2819 - accuracy: 0.8631 - val_loss: 1.9760 - val_accuracy: 0.5872\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2914 - accuracy: 0.8599 - val_loss: 2.0389 - val_accuracy: 0.5897\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2833 - accuracy: 0.8591 - val_loss: 2.0089 - val_accuracy: 0.5879\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2742 - accuracy: 0.8651 - val_loss: 2.0344 - val_accuracy: 0.5883\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2885 - accuracy: 0.8582 - val_loss: 1.9673 - val_accuracy: 0.5865\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2814 - accuracy: 0.8608 - val_loss: 2.0586 - val_accuracy: 0.5850\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2881 - accuracy: 0.8567 - val_loss: 2.0219 - val_accuracy: 0.5951\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2888 - accuracy: 0.8595 - val_loss: 1.9785 - val_accuracy: 0.5969\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2774 - accuracy: 0.8639 - val_loss: 2.1333 - val_accuracy: 0.5894\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2795 - accuracy: 0.8623 - val_loss: 1.9931 - val_accuracy: 0.5847\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2827 - accuracy: 0.8642 - val_loss: 1.9960 - val_accuracy: 0.5901\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2843 - accuracy: 0.8622 - val_loss: 2.0052 - val_accuracy: 0.5847\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2734 - accuracy: 0.8668 - val_loss: 1.9543 - val_accuracy: 0.5926\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2717 - accuracy: 0.8655 - val_loss: 2.0455 - val_accuracy: 0.5879\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2726 - accuracy: 0.8653 - val_loss: 2.1082 - val_accuracy: 0.5890\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2681 - accuracy: 0.8668 - val_loss: 2.0117 - val_accuracy: 0.5865\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2776 - accuracy: 0.8659 - val_loss: 2.1121 - val_accuracy: 0.5922\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2735 - accuracy: 0.8694 - val_loss: 2.0976 - val_accuracy: 0.5897\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2807 - accuracy: 0.8613 - val_loss: 2.0359 - val_accuracy: 0.5890\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2703 - accuracy: 0.8669 - val_loss: 2.1958 - val_accuracy: 0.5865\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2760 - accuracy: 0.8646 - val_loss: 2.0938 - val_accuracy: 0.5850\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2625 - accuracy: 0.8670 - val_loss: 2.1030 - val_accuracy: 0.5868\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2639 - accuracy: 0.8669 - val_loss: 2.1278 - val_accuracy: 0.5894\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2601 - accuracy: 0.8709 - val_loss: 2.1152 - val_accuracy: 0.5876\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2687 - accuracy: 0.8690 - val_loss: 2.1610 - val_accuracy: 0.5897\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2688 - accuracy: 0.8661 - val_loss: 2.1649 - val_accuracy: 0.5822\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2730 - accuracy: 0.8698 - val_loss: 2.1633 - val_accuracy: 0.5901\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2622 - accuracy: 0.8732 - val_loss: 2.1635 - val_accuracy: 0.5919\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2516 - accuracy: 0.8737 - val_loss: 2.1283 - val_accuracy: 0.5930\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2635 - accuracy: 0.8716 - val_loss: 2.1872 - val_accuracy: 0.5807\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2620 - accuracy: 0.8723 - val_loss: 2.1357 - val_accuracy: 0.5811\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2547 - accuracy: 0.8724 - val_loss: 2.1901 - val_accuracy: 0.5894\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2593 - accuracy: 0.8710 - val_loss: 2.1914 - val_accuracy: 0.5854\n",
            "History for model 13: <keras.src.callbacks.History object at 0x7e83f3cf64d0>\n",
            "Trial 14: Number of layers = 4, Number of neurons per layer = 30\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 4s 7ms/step - loss: 0.8388 - accuracy: 0.5966 - val_loss: 0.7923 - val_accuracy: 0.6228\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7765 - accuracy: 0.6241 - val_loss: 0.7910 - val_accuracy: 0.6131\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.7634 - accuracy: 0.6324 - val_loss: 0.7886 - val_accuracy: 0.6253\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7518 - accuracy: 0.6409 - val_loss: 0.7798 - val_accuracy: 0.6332\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7418 - accuracy: 0.6468 - val_loss: 0.7823 - val_accuracy: 0.6318\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7355 - accuracy: 0.6539 - val_loss: 0.7774 - val_accuracy: 0.6296\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.7275 - accuracy: 0.6543 - val_loss: 0.7883 - val_accuracy: 0.6257\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7218 - accuracy: 0.6597 - val_loss: 0.7906 - val_accuracy: 0.6329\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7109 - accuracy: 0.6661 - val_loss: 0.7924 - val_accuracy: 0.6268\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7090 - accuracy: 0.6695 - val_loss: 0.7913 - val_accuracy: 0.6300\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7002 - accuracy: 0.6716 - val_loss: 0.7902 - val_accuracy: 0.6300\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6937 - accuracy: 0.6730 - val_loss: 0.7882 - val_accuracy: 0.6311\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6872 - accuracy: 0.6778 - val_loss: 0.7940 - val_accuracy: 0.6199\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.6805 - accuracy: 0.6840 - val_loss: 0.7880 - val_accuracy: 0.6257\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6757 - accuracy: 0.6873 - val_loss: 0.7931 - val_accuracy: 0.6336\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6723 - accuracy: 0.6864 - val_loss: 0.8162 - val_accuracy: 0.6260\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6665 - accuracy: 0.6920 - val_loss: 0.8058 - val_accuracy: 0.6232\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6658 - accuracy: 0.6908 - val_loss: 0.8013 - val_accuracy: 0.6257\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6594 - accuracy: 0.6989 - val_loss: 0.8206 - val_accuracy: 0.6188\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6536 - accuracy: 0.7007 - val_loss: 0.8112 - val_accuracy: 0.6167\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6503 - accuracy: 0.7018 - val_loss: 0.8216 - val_accuracy: 0.6138\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6481 - accuracy: 0.7055 - val_loss: 0.8195 - val_accuracy: 0.6196\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6409 - accuracy: 0.7032 - val_loss: 0.8222 - val_accuracy: 0.6192\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6370 - accuracy: 0.7087 - val_loss: 0.8248 - val_accuracy: 0.6282\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6348 - accuracy: 0.7062 - val_loss: 0.8358 - val_accuracy: 0.6199\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6313 - accuracy: 0.7100 - val_loss: 0.8515 - val_accuracy: 0.6282\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6298 - accuracy: 0.7095 - val_loss: 0.8332 - val_accuracy: 0.6221\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6232 - accuracy: 0.7124 - val_loss: 0.8371 - val_accuracy: 0.6199\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6217 - accuracy: 0.7123 - val_loss: 0.8430 - val_accuracy: 0.6278\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6194 - accuracy: 0.7160 - val_loss: 0.8380 - val_accuracy: 0.6221\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6158 - accuracy: 0.7169 - val_loss: 0.8513 - val_accuracy: 0.6239\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6135 - accuracy: 0.7149 - val_loss: 0.8622 - val_accuracy: 0.6239\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6112 - accuracy: 0.7234 - val_loss: 0.8438 - val_accuracy: 0.6214\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6058 - accuracy: 0.7192 - val_loss: 0.8697 - val_accuracy: 0.6206\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6044 - accuracy: 0.7188 - val_loss: 0.8724 - val_accuracy: 0.6196\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5975 - accuracy: 0.7285 - val_loss: 0.8758 - val_accuracy: 0.6289\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5956 - accuracy: 0.7251 - val_loss: 0.8683 - val_accuracy: 0.6102\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5947 - accuracy: 0.7266 - val_loss: 0.8758 - val_accuracy: 0.6235\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5936 - accuracy: 0.7236 - val_loss: 0.8991 - val_accuracy: 0.6109\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5868 - accuracy: 0.7318 - val_loss: 0.8897 - val_accuracy: 0.6160\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5877 - accuracy: 0.7271 - val_loss: 0.9057 - val_accuracy: 0.6106\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5864 - accuracy: 0.7263 - val_loss: 0.9028 - val_accuracy: 0.6170\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5811 - accuracy: 0.7356 - val_loss: 0.8950 - val_accuracy: 0.6250\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5808 - accuracy: 0.7309 - val_loss: 0.8945 - val_accuracy: 0.6228\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5793 - accuracy: 0.7309 - val_loss: 0.9211 - val_accuracy: 0.6117\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5780 - accuracy: 0.7340 - val_loss: 0.9088 - val_accuracy: 0.6228\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5706 - accuracy: 0.7378 - val_loss: 0.9533 - val_accuracy: 0.6102\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5690 - accuracy: 0.7426 - val_loss: 0.9570 - val_accuracy: 0.6099\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5701 - accuracy: 0.7389 - val_loss: 0.9391 - val_accuracy: 0.6152\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5645 - accuracy: 0.7391 - val_loss: 0.9490 - val_accuracy: 0.6134\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5659 - accuracy: 0.7403 - val_loss: 0.9493 - val_accuracy: 0.6246\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5602 - accuracy: 0.7423 - val_loss: 0.9785 - val_accuracy: 0.6113\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5611 - accuracy: 0.7352 - val_loss: 0.9588 - val_accuracy: 0.6224\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5547 - accuracy: 0.7446 - val_loss: 0.9831 - val_accuracy: 0.6142\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5576 - accuracy: 0.7427 - val_loss: 0.9681 - val_accuracy: 0.6091\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5570 - accuracy: 0.7387 - val_loss: 0.9495 - val_accuracy: 0.6127\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5565 - accuracy: 0.7448 - val_loss: 0.9799 - val_accuracy: 0.6142\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5511 - accuracy: 0.7493 - val_loss: 0.9795 - val_accuracy: 0.6163\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5486 - accuracy: 0.7472 - val_loss: 0.9913 - val_accuracy: 0.6048\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5468 - accuracy: 0.7454 - val_loss: 0.9818 - val_accuracy: 0.6181\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5490 - accuracy: 0.7458 - val_loss: 1.0238 - val_accuracy: 0.6027\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5403 - accuracy: 0.7482 - val_loss: 0.9963 - val_accuracy: 0.6124\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5414 - accuracy: 0.7492 - val_loss: 0.9962 - val_accuracy: 0.6088\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5445 - accuracy: 0.7519 - val_loss: 1.0070 - val_accuracy: 0.6073\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5435 - accuracy: 0.7490 - val_loss: 0.9994 - val_accuracy: 0.6102\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5362 - accuracy: 0.7546 - val_loss: 1.0077 - val_accuracy: 0.6145\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5332 - accuracy: 0.7545 - val_loss: 1.0295 - val_accuracy: 0.6012\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5369 - accuracy: 0.7555 - val_loss: 1.0242 - val_accuracy: 0.6052\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5321 - accuracy: 0.7505 - val_loss: 1.0206 - val_accuracy: 0.6178\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5306 - accuracy: 0.7532 - val_loss: 1.0176 - val_accuracy: 0.6073\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5288 - accuracy: 0.7528 - val_loss: 1.0676 - val_accuracy: 0.6099\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5285 - accuracy: 0.7523 - val_loss: 1.0476 - val_accuracy: 0.6063\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5273 - accuracy: 0.7557 - val_loss: 1.0422 - val_accuracy: 0.6160\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5238 - accuracy: 0.7569 - val_loss: 1.0396 - val_accuracy: 0.6131\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5205 - accuracy: 0.7604 - val_loss: 1.0711 - val_accuracy: 0.6063\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5223 - accuracy: 0.7591 - val_loss: 1.1114 - val_accuracy: 0.5965\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5227 - accuracy: 0.7583 - val_loss: 1.0712 - val_accuracy: 0.6016\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5143 - accuracy: 0.7620 - val_loss: 1.0933 - val_accuracy: 0.6163\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5144 - accuracy: 0.7608 - val_loss: 1.0727 - val_accuracy: 0.6142\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5186 - accuracy: 0.7625 - val_loss: 1.0850 - val_accuracy: 0.6106\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5135 - accuracy: 0.7632 - val_loss: 1.0584 - val_accuracy: 0.6167\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5153 - accuracy: 0.7623 - val_loss: 1.1039 - val_accuracy: 0.6131\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5093 - accuracy: 0.7644 - val_loss: 1.1191 - val_accuracy: 0.6134\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5122 - accuracy: 0.7613 - val_loss: 1.1101 - val_accuracy: 0.5994\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5085 - accuracy: 0.7648 - val_loss: 1.1199 - val_accuracy: 0.6160\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5089 - accuracy: 0.7663 - val_loss: 1.1012 - val_accuracy: 0.6095\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5038 - accuracy: 0.7663 - val_loss: 1.1164 - val_accuracy: 0.6066\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5057 - accuracy: 0.7678 - val_loss: 1.1283 - val_accuracy: 0.6091\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5055 - accuracy: 0.7662 - val_loss: 1.1422 - val_accuracy: 0.6174\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 2s 4ms/step - loss: 0.5059 - accuracy: 0.7673 - val_loss: 1.1474 - val_accuracy: 0.6084\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5025 - accuracy: 0.7700 - val_loss: 1.1570 - val_accuracy: 0.6102\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4991 - accuracy: 0.7691 - val_loss: 1.1500 - val_accuracy: 0.6102\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5025 - accuracy: 0.7676 - val_loss: 1.1504 - val_accuracy: 0.6023\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4940 - accuracy: 0.7701 - val_loss: 1.1662 - val_accuracy: 0.6174\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5006 - accuracy: 0.7709 - val_loss: 1.1882 - val_accuracy: 0.5994\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4933 - accuracy: 0.7748 - val_loss: 1.1705 - val_accuracy: 0.6077\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4966 - accuracy: 0.7700 - val_loss: 1.1449 - val_accuracy: 0.6048\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4970 - accuracy: 0.7720 - val_loss: 1.1606 - val_accuracy: 0.6023\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4914 - accuracy: 0.7755 - val_loss: 1.1806 - val_accuracy: 0.6070\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4885 - accuracy: 0.7755 - val_loss: 1.1878 - val_accuracy: 0.6138\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4933 - accuracy: 0.7762 - val_loss: 1.1692 - val_accuracy: 0.6117\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4896 - accuracy: 0.7736 - val_loss: 1.2661 - val_accuracy: 0.6005\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4886 - accuracy: 0.7774 - val_loss: 1.1989 - val_accuracy: 0.6167\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4844 - accuracy: 0.7780 - val_loss: 1.2216 - val_accuracy: 0.6081\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4812 - accuracy: 0.7800 - val_loss: 1.1866 - val_accuracy: 0.6117\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4868 - accuracy: 0.7751 - val_loss: 1.2091 - val_accuracy: 0.5987\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4874 - accuracy: 0.7788 - val_loss: 1.2022 - val_accuracy: 0.6070\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4880 - accuracy: 0.7776 - val_loss: 1.2158 - val_accuracy: 0.6066\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4885 - accuracy: 0.7719 - val_loss: 1.1993 - val_accuracy: 0.6095\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4865 - accuracy: 0.7777 - val_loss: 1.1860 - val_accuracy: 0.6106\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4766 - accuracy: 0.7794 - val_loss: 1.2195 - val_accuracy: 0.6034\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4797 - accuracy: 0.7764 - val_loss: 1.2145 - val_accuracy: 0.6019\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4722 - accuracy: 0.7850 - val_loss: 1.2330 - val_accuracy: 0.5955\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4749 - accuracy: 0.7816 - val_loss: 1.2681 - val_accuracy: 0.6095\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4734 - accuracy: 0.7824 - val_loss: 1.2602 - val_accuracy: 0.6001\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4733 - accuracy: 0.7818 - val_loss: 1.2421 - val_accuracy: 0.6081\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4752 - accuracy: 0.7805 - val_loss: 1.2767 - val_accuracy: 0.6041\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4705 - accuracy: 0.7849 - val_loss: 1.2736 - val_accuracy: 0.6070\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4724 - accuracy: 0.7807 - val_loss: 1.3044 - val_accuracy: 0.5994\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4680 - accuracy: 0.7846 - val_loss: 1.2591 - val_accuracy: 0.6066\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4725 - accuracy: 0.7803 - val_loss: 1.2596 - val_accuracy: 0.6041\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4693 - accuracy: 0.7874 - val_loss: 1.2728 - val_accuracy: 0.6019\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4695 - accuracy: 0.7856 - val_loss: 1.2648 - val_accuracy: 0.6030\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4684 - accuracy: 0.7834 - val_loss: 1.2854 - val_accuracy: 0.6019\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4660 - accuracy: 0.7852 - val_loss: 1.2846 - val_accuracy: 0.6009\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4653 - accuracy: 0.7834 - val_loss: 1.3163 - val_accuracy: 0.5983\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4657 - accuracy: 0.7851 - val_loss: 1.3327 - val_accuracy: 0.6081\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4642 - accuracy: 0.7859 - val_loss: 1.2594 - val_accuracy: 0.5976\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4723 - accuracy: 0.7816 - val_loss: 1.3127 - val_accuracy: 0.6048\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4616 - accuracy: 0.7863 - val_loss: 1.2998 - val_accuracy: 0.6019\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4632 - accuracy: 0.7861 - val_loss: 1.3178 - val_accuracy: 0.6091\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4688 - accuracy: 0.7852 - val_loss: 1.3154 - val_accuracy: 0.6023\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4627 - accuracy: 0.7896 - val_loss: 1.3208 - val_accuracy: 0.6070\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4567 - accuracy: 0.7885 - val_loss: 1.3155 - val_accuracy: 0.6016\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4547 - accuracy: 0.7893 - val_loss: 1.3447 - val_accuracy: 0.6048\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4568 - accuracy: 0.7884 - val_loss: 1.3192 - val_accuracy: 0.6070\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4592 - accuracy: 0.7907 - val_loss: 1.3340 - val_accuracy: 0.6055\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4607 - accuracy: 0.7900 - val_loss: 1.2954 - val_accuracy: 0.5994\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4628 - accuracy: 0.7896 - val_loss: 1.3902 - val_accuracy: 0.6113\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4552 - accuracy: 0.7910 - val_loss: 1.3418 - val_accuracy: 0.6041\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4543 - accuracy: 0.7898 - val_loss: 1.3530 - val_accuracy: 0.6009\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4515 - accuracy: 0.7895 - val_loss: 1.3112 - val_accuracy: 0.5994\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4498 - accuracy: 0.7942 - val_loss: 1.3540 - val_accuracy: 0.6012\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4459 - accuracy: 0.7955 - val_loss: 1.3899 - val_accuracy: 0.6030\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4490 - accuracy: 0.7931 - val_loss: 1.3799 - val_accuracy: 0.5861\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4555 - accuracy: 0.7905 - val_loss: 1.3579 - val_accuracy: 0.5983\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4505 - accuracy: 0.7914 - val_loss: 1.3893 - val_accuracy: 0.5894\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4542 - accuracy: 0.7920 - val_loss: 1.4246 - val_accuracy: 0.6037\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4426 - accuracy: 0.7954 - val_loss: 1.3920 - val_accuracy: 0.5948\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4428 - accuracy: 0.7923 - val_loss: 1.3751 - val_accuracy: 0.5983\n",
            "History for model 14: <keras.src.callbacks.History object at 0x7e83f3b9a9b0>\n",
            "Trial 15: Number of layers = 4, Number of neurons per layer = 60\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 4s 8ms/step - loss: 0.8291 - accuracy: 0.6028 - val_loss: 0.8032 - val_accuracy: 0.6192\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7760 - accuracy: 0.6259 - val_loss: 0.7899 - val_accuracy: 0.6307\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7584 - accuracy: 0.6350 - val_loss: 0.7843 - val_accuracy: 0.6314\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7452 - accuracy: 0.6417 - val_loss: 0.7841 - val_accuracy: 0.6314\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7366 - accuracy: 0.6507 - val_loss: 0.7920 - val_accuracy: 0.6196\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7269 - accuracy: 0.6559 - val_loss: 0.7933 - val_accuracy: 0.6271\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7194 - accuracy: 0.6619 - val_loss: 0.7777 - val_accuracy: 0.6365\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7103 - accuracy: 0.6694 - val_loss: 0.7789 - val_accuracy: 0.6253\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7031 - accuracy: 0.6650 - val_loss: 0.7952 - val_accuracy: 0.6307\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6965 - accuracy: 0.6714 - val_loss: 0.7957 - val_accuracy: 0.6354\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6888 - accuracy: 0.6784 - val_loss: 0.7882 - val_accuracy: 0.6282\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6815 - accuracy: 0.6804 - val_loss: 0.7806 - val_accuracy: 0.6372\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6757 - accuracy: 0.6800 - val_loss: 0.7989 - val_accuracy: 0.6235\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6695 - accuracy: 0.6868 - val_loss: 0.8018 - val_accuracy: 0.6282\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6628 - accuracy: 0.6874 - val_loss: 0.8145 - val_accuracy: 0.6242\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6563 - accuracy: 0.6927 - val_loss: 0.7932 - val_accuracy: 0.6307\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6507 - accuracy: 0.6968 - val_loss: 0.8109 - val_accuracy: 0.6268\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6449 - accuracy: 0.6919 - val_loss: 0.8017 - val_accuracy: 0.6401\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6368 - accuracy: 0.7020 - val_loss: 0.8427 - val_accuracy: 0.6289\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6322 - accuracy: 0.7099 - val_loss: 0.8276 - val_accuracy: 0.6368\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6252 - accuracy: 0.7117 - val_loss: 0.8397 - val_accuracy: 0.6329\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6216 - accuracy: 0.7129 - val_loss: 0.8351 - val_accuracy: 0.6253\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6155 - accuracy: 0.7122 - val_loss: 0.8523 - val_accuracy: 0.6325\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6108 - accuracy: 0.7159 - val_loss: 0.8675 - val_accuracy: 0.6214\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6072 - accuracy: 0.7200 - val_loss: 0.8556 - val_accuracy: 0.6260\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6025 - accuracy: 0.7206 - val_loss: 0.8570 - val_accuracy: 0.6224\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5948 - accuracy: 0.7230 - val_loss: 0.9003 - val_accuracy: 0.6228\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5874 - accuracy: 0.7268 - val_loss: 0.8720 - val_accuracy: 0.6210\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5859 - accuracy: 0.7282 - val_loss: 0.8874 - val_accuracy: 0.6253\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5816 - accuracy: 0.7309 - val_loss: 0.9121 - val_accuracy: 0.6142\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5747 - accuracy: 0.7335 - val_loss: 0.9196 - val_accuracy: 0.6271\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5705 - accuracy: 0.7337 - val_loss: 0.9231 - val_accuracy: 0.6232\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5704 - accuracy: 0.7337 - val_loss: 0.9562 - val_accuracy: 0.6117\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5648 - accuracy: 0.7384 - val_loss: 0.9408 - val_accuracy: 0.6268\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5643 - accuracy: 0.7375 - val_loss: 0.9543 - val_accuracy: 0.6156\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5538 - accuracy: 0.7432 - val_loss: 0.9653 - val_accuracy: 0.6214\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5511 - accuracy: 0.7413 - val_loss: 0.9572 - val_accuracy: 0.6138\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5531 - accuracy: 0.7426 - val_loss: 0.9568 - val_accuracy: 0.6185\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5442 - accuracy: 0.7441 - val_loss: 0.9940 - val_accuracy: 0.6073\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5442 - accuracy: 0.7438 - val_loss: 0.9861 - val_accuracy: 0.6170\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5412 - accuracy: 0.7467 - val_loss: 1.0075 - val_accuracy: 0.6178\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5354 - accuracy: 0.7501 - val_loss: 1.0269 - val_accuracy: 0.6210\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5324 - accuracy: 0.7558 - val_loss: 0.9678 - val_accuracy: 0.6127\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5314 - accuracy: 0.7545 - val_loss: 1.0404 - val_accuracy: 0.6170\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5233 - accuracy: 0.7576 - val_loss: 1.0583 - val_accuracy: 0.6138\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5250 - accuracy: 0.7582 - val_loss: 1.0379 - val_accuracy: 0.6091\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5175 - accuracy: 0.7576 - val_loss: 1.0452 - val_accuracy: 0.6174\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5119 - accuracy: 0.7627 - val_loss: 1.0498 - val_accuracy: 0.6030\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5094 - accuracy: 0.7635 - val_loss: 1.0696 - val_accuracy: 0.6181\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5116 - accuracy: 0.7596 - val_loss: 1.0590 - val_accuracy: 0.6160\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5105 - accuracy: 0.7585 - val_loss: 1.0803 - val_accuracy: 0.6149\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5006 - accuracy: 0.7662 - val_loss: 1.1226 - val_accuracy: 0.6178\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4987 - accuracy: 0.7698 - val_loss: 1.1313 - val_accuracy: 0.5987\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4987 - accuracy: 0.7645 - val_loss: 1.1079 - val_accuracy: 0.6073\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5054 - accuracy: 0.7627 - val_loss: 1.1119 - val_accuracy: 0.6163\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4916 - accuracy: 0.7715 - val_loss: 1.1304 - val_accuracy: 0.6052\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4885 - accuracy: 0.7713 - val_loss: 1.1629 - val_accuracy: 0.6113\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4793 - accuracy: 0.7761 - val_loss: 1.1835 - val_accuracy: 0.6109\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4910 - accuracy: 0.7724 - val_loss: 1.1514 - val_accuracy: 0.6077\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4798 - accuracy: 0.7802 - val_loss: 1.1708 - val_accuracy: 0.6081\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4824 - accuracy: 0.7751 - val_loss: 1.1708 - val_accuracy: 0.6088\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4789 - accuracy: 0.7777 - val_loss: 1.1384 - val_accuracy: 0.6117\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4751 - accuracy: 0.7806 - val_loss: 1.1836 - val_accuracy: 0.6088\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4776 - accuracy: 0.7816 - val_loss: 1.1568 - val_accuracy: 0.6134\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4679 - accuracy: 0.7831 - val_loss: 1.1888 - val_accuracy: 0.6001\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4671 - accuracy: 0.7812 - val_loss: 1.2087 - val_accuracy: 0.6138\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4638 - accuracy: 0.7861 - val_loss: 1.2555 - val_accuracy: 0.6196\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4611 - accuracy: 0.7868 - val_loss: 1.2293 - val_accuracy: 0.6059\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4569 - accuracy: 0.7845 - val_loss: 1.2337 - val_accuracy: 0.6019\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4590 - accuracy: 0.7836 - val_loss: 1.2075 - val_accuracy: 0.5998\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4553 - accuracy: 0.7893 - val_loss: 1.3040 - val_accuracy: 0.6081\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4527 - accuracy: 0.7866 - val_loss: 1.2759 - val_accuracy: 0.6109\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4521 - accuracy: 0.7863 - val_loss: 1.2886 - val_accuracy: 0.6001\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4501 - accuracy: 0.7916 - val_loss: 1.2874 - val_accuracy: 0.6138\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4476 - accuracy: 0.7901 - val_loss: 1.2558 - val_accuracy: 0.6016\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4485 - accuracy: 0.7949 - val_loss: 1.3313 - val_accuracy: 0.6102\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4468 - accuracy: 0.7939 - val_loss: 1.3148 - val_accuracy: 0.6045\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4414 - accuracy: 0.7967 - val_loss: 1.2933 - val_accuracy: 0.6163\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4412 - accuracy: 0.7937 - val_loss: 1.3494 - val_accuracy: 0.5991\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4359 - accuracy: 0.7948 - val_loss: 1.3259 - val_accuracy: 0.6081\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4341 - accuracy: 0.7972 - val_loss: 1.3546 - val_accuracy: 0.5980\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4315 - accuracy: 0.8011 - val_loss: 1.3584 - val_accuracy: 0.6091\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4281 - accuracy: 0.7976 - val_loss: 1.3691 - val_accuracy: 0.6095\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4280 - accuracy: 0.7976 - val_loss: 1.3727 - val_accuracy: 0.6023\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4318 - accuracy: 0.7961 - val_loss: 1.4009 - val_accuracy: 0.6012\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4263 - accuracy: 0.7988 - val_loss: 1.3715 - val_accuracy: 0.6037\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4233 - accuracy: 0.8020 - val_loss: 1.3748 - val_accuracy: 0.6019\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4168 - accuracy: 0.8082 - val_loss: 1.3936 - val_accuracy: 0.6106\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4206 - accuracy: 0.8053 - val_loss: 1.4049 - val_accuracy: 0.6009\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4170 - accuracy: 0.8053 - val_loss: 1.3852 - val_accuracy: 0.6005\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4163 - accuracy: 0.8012 - val_loss: 1.4071 - val_accuracy: 0.6016\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4224 - accuracy: 0.8040 - val_loss: 1.3907 - val_accuracy: 0.6048\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4165 - accuracy: 0.8038 - val_loss: 1.3855 - val_accuracy: 0.6055\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4086 - accuracy: 0.8074 - val_loss: 1.4353 - val_accuracy: 0.6009\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4040 - accuracy: 0.8103 - val_loss: 1.4221 - val_accuracy: 0.6174\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4105 - accuracy: 0.8080 - val_loss: 1.4955 - val_accuracy: 0.6059\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4018 - accuracy: 0.8104 - val_loss: 1.5133 - val_accuracy: 0.6009\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4107 - accuracy: 0.8076 - val_loss: 1.4525 - val_accuracy: 0.6070\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4121 - accuracy: 0.8082 - val_loss: 1.4773 - val_accuracy: 0.5987\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4055 - accuracy: 0.8092 - val_loss: 1.4660 - val_accuracy: 0.6081\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4030 - accuracy: 0.8100 - val_loss: 1.4792 - val_accuracy: 0.6052\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3950 - accuracy: 0.8121 - val_loss: 1.4933 - val_accuracy: 0.6030\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3938 - accuracy: 0.8136 - val_loss: 1.5402 - val_accuracy: 0.6016\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3992 - accuracy: 0.8107 - val_loss: 1.5209 - val_accuracy: 0.6045\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3982 - accuracy: 0.8133 - val_loss: 1.5083 - val_accuracy: 0.5983\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3979 - accuracy: 0.8136 - val_loss: 1.4678 - val_accuracy: 0.5930\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3990 - accuracy: 0.8143 - val_loss: 1.5751 - val_accuracy: 0.6048\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3905 - accuracy: 0.8161 - val_loss: 1.4881 - val_accuracy: 0.5922\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3922 - accuracy: 0.8129 - val_loss: 1.4975 - val_accuracy: 0.5980\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3823 - accuracy: 0.8181 - val_loss: 1.5749 - val_accuracy: 0.5948\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3929 - accuracy: 0.8150 - val_loss: 1.5578 - val_accuracy: 0.6041\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3914 - accuracy: 0.8141 - val_loss: 1.5501 - val_accuracy: 0.6012\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3880 - accuracy: 0.8204 - val_loss: 1.5450 - val_accuracy: 0.5965\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3846 - accuracy: 0.8189 - val_loss: 1.6121 - val_accuracy: 0.5976\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3859 - accuracy: 0.8179 - val_loss: 1.5699 - val_accuracy: 0.6012\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3779 - accuracy: 0.8227 - val_loss: 1.5522 - val_accuracy: 0.5998\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3776 - accuracy: 0.8244 - val_loss: 1.5653 - val_accuracy: 0.5872\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3806 - accuracy: 0.8223 - val_loss: 1.5809 - val_accuracy: 0.5955\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3781 - accuracy: 0.8220 - val_loss: 1.5818 - val_accuracy: 0.5933\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3813 - accuracy: 0.8239 - val_loss: 1.6581 - val_accuracy: 0.5926\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3773 - accuracy: 0.8233 - val_loss: 1.6365 - val_accuracy: 0.5980\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3812 - accuracy: 0.8188 - val_loss: 1.6191 - val_accuracy: 0.5980\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3685 - accuracy: 0.8208 - val_loss: 1.5891 - val_accuracy: 0.6019\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3692 - accuracy: 0.8245 - val_loss: 1.5920 - val_accuracy: 0.5983\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3680 - accuracy: 0.8253 - val_loss: 1.6709 - val_accuracy: 0.5987\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3719 - accuracy: 0.8221 - val_loss: 1.6610 - val_accuracy: 0.6001\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3671 - accuracy: 0.8282 - val_loss: 1.6342 - val_accuracy: 0.6023\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3719 - accuracy: 0.8228 - val_loss: 1.6657 - val_accuracy: 0.6016\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3675 - accuracy: 0.8240 - val_loss: 1.7407 - val_accuracy: 0.5948\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3601 - accuracy: 0.8271 - val_loss: 1.7173 - val_accuracy: 0.5980\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3581 - accuracy: 0.8276 - val_loss: 1.7136 - val_accuracy: 0.5904\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3538 - accuracy: 0.8304 - val_loss: 1.6897 - val_accuracy: 0.5926\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3682 - accuracy: 0.8252 - val_loss: 1.7388 - val_accuracy: 0.6016\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3667 - accuracy: 0.8272 - val_loss: 1.6928 - val_accuracy: 0.5994\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3700 - accuracy: 0.8246 - val_loss: 1.6882 - val_accuracy: 0.5930\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3619 - accuracy: 0.8258 - val_loss: 1.7069 - val_accuracy: 0.5980\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3653 - accuracy: 0.8296 - val_loss: 1.6695 - val_accuracy: 0.5987\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3509 - accuracy: 0.8330 - val_loss: 1.7246 - val_accuracy: 0.6055\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3563 - accuracy: 0.8285 - val_loss: 1.8060 - val_accuracy: 0.5980\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3531 - accuracy: 0.8332 - val_loss: 1.7605 - val_accuracy: 0.6041\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3514 - accuracy: 0.8313 - val_loss: 1.7549 - val_accuracy: 0.5976\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3616 - accuracy: 0.8271 - val_loss: 1.8197 - val_accuracy: 0.5983\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3530 - accuracy: 0.8339 - val_loss: 1.8306 - val_accuracy: 0.5973\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3477 - accuracy: 0.8318 - val_loss: 1.8052 - val_accuracy: 0.6023\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3504 - accuracy: 0.8352 - val_loss: 1.7457 - val_accuracy: 0.5969\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3588 - accuracy: 0.8333 - val_loss: 1.7873 - val_accuracy: 0.6001\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3630 - accuracy: 0.8332 - val_loss: 1.8256 - val_accuracy: 0.6016\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3517 - accuracy: 0.8331 - val_loss: 1.7904 - val_accuracy: 0.5980\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3442 - accuracy: 0.8313 - val_loss: 1.7800 - val_accuracy: 0.5897\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3375 - accuracy: 0.8373 - val_loss: 1.8011 - val_accuracy: 0.5948\n",
            "History for model 15: <keras.src.callbacks.History object at 0x7e83eb0f7b80>\n",
            "Trial 16: Number of layers = 4, Number of neurons per layer = 90\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 4s 6ms/step - loss: 0.8221 - accuracy: 0.6042 - val_loss: 0.7976 - val_accuracy: 0.6167\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7761 - accuracy: 0.6341 - val_loss: 0.7863 - val_accuracy: 0.6242\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7602 - accuracy: 0.6374 - val_loss: 0.7933 - val_accuracy: 0.6368\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7474 - accuracy: 0.6510 - val_loss: 0.7792 - val_accuracy: 0.6401\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7351 - accuracy: 0.6505 - val_loss: 0.7712 - val_accuracy: 0.6455\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7260 - accuracy: 0.6631 - val_loss: 0.7766 - val_accuracy: 0.6300\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7156 - accuracy: 0.6663 - val_loss: 0.7858 - val_accuracy: 0.6365\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7070 - accuracy: 0.6630 - val_loss: 0.7801 - val_accuracy: 0.6318\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6996 - accuracy: 0.6739 - val_loss: 0.7840 - val_accuracy: 0.6339\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6907 - accuracy: 0.6766 - val_loss: 0.7865 - val_accuracy: 0.6303\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6824 - accuracy: 0.6794 - val_loss: 0.8055 - val_accuracy: 0.6278\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6714 - accuracy: 0.6885 - val_loss: 0.8135 - val_accuracy: 0.6300\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6639 - accuracy: 0.6882 - val_loss: 0.8043 - val_accuracy: 0.6268\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6548 - accuracy: 0.6964 - val_loss: 0.8005 - val_accuracy: 0.6275\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6483 - accuracy: 0.6964 - val_loss: 0.8226 - val_accuracy: 0.6350\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6384 - accuracy: 0.7040 - val_loss: 0.8156 - val_accuracy: 0.6311\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6344 - accuracy: 0.7060 - val_loss: 0.8326 - val_accuracy: 0.6206\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6257 - accuracy: 0.7083 - val_loss: 0.8217 - val_accuracy: 0.6199\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6169 - accuracy: 0.7147 - val_loss: 0.8319 - val_accuracy: 0.6368\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6117 - accuracy: 0.7171 - val_loss: 0.8552 - val_accuracy: 0.6275\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6047 - accuracy: 0.7174 - val_loss: 0.8549 - val_accuracy: 0.6196\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5949 - accuracy: 0.7231 - val_loss: 0.8580 - val_accuracy: 0.6365\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5916 - accuracy: 0.7281 - val_loss: 0.8556 - val_accuracy: 0.6275\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5835 - accuracy: 0.7269 - val_loss: 0.8748 - val_accuracy: 0.6214\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5786 - accuracy: 0.7269 - val_loss: 0.9183 - val_accuracy: 0.6214\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.5719 - accuracy: 0.7368 - val_loss: 0.8981 - val_accuracy: 0.6206\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5666 - accuracy: 0.7346 - val_loss: 0.9146 - val_accuracy: 0.6228\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5623 - accuracy: 0.7389 - val_loss: 0.9221 - val_accuracy: 0.6142\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5595 - accuracy: 0.7380 - val_loss: 0.9436 - val_accuracy: 0.6228\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5497 - accuracy: 0.7451 - val_loss: 0.9335 - val_accuracy: 0.6300\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5472 - accuracy: 0.7452 - val_loss: 0.9555 - val_accuracy: 0.6196\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5400 - accuracy: 0.7478 - val_loss: 0.9489 - val_accuracy: 0.6235\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5320 - accuracy: 0.7503 - val_loss: 0.9601 - val_accuracy: 0.6138\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5312 - accuracy: 0.7553 - val_loss: 0.9866 - val_accuracy: 0.6185\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5284 - accuracy: 0.7567 - val_loss: 0.9987 - val_accuracy: 0.6019\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5241 - accuracy: 0.7578 - val_loss: 1.0135 - val_accuracy: 0.6185\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5168 - accuracy: 0.7597 - val_loss: 0.9931 - val_accuracy: 0.6134\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5101 - accuracy: 0.7582 - val_loss: 1.0388 - val_accuracy: 0.6152\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5100 - accuracy: 0.7640 - val_loss: 1.0543 - val_accuracy: 0.6073\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5019 - accuracy: 0.7596 - val_loss: 1.0495 - val_accuracy: 0.6113\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4976 - accuracy: 0.7660 - val_loss: 1.0574 - val_accuracy: 0.6206\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4932 - accuracy: 0.7710 - val_loss: 1.0981 - val_accuracy: 0.6163\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4933 - accuracy: 0.7733 - val_loss: 1.0994 - val_accuracy: 0.6152\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4867 - accuracy: 0.7707 - val_loss: 1.0887 - val_accuracy: 0.6048\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4849 - accuracy: 0.7747 - val_loss: 1.0955 - val_accuracy: 0.6099\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4841 - accuracy: 0.7741 - val_loss: 1.1258 - val_accuracy: 0.6084\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4743 - accuracy: 0.7757 - val_loss: 1.1678 - val_accuracy: 0.6099\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4743 - accuracy: 0.7758 - val_loss: 1.1366 - val_accuracy: 0.6188\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4698 - accuracy: 0.7775 - val_loss: 1.1928 - val_accuracy: 0.6095\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4651 - accuracy: 0.7806 - val_loss: 1.1638 - val_accuracy: 0.5994\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4563 - accuracy: 0.7868 - val_loss: 1.2579 - val_accuracy: 0.5976\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4591 - accuracy: 0.7829 - val_loss: 1.2502 - val_accuracy: 0.6106\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4557 - accuracy: 0.7866 - val_loss: 1.2769 - val_accuracy: 0.6027\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4535 - accuracy: 0.7861 - val_loss: 1.2392 - val_accuracy: 0.6124\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4502 - accuracy: 0.7914 - val_loss: 1.2499 - val_accuracy: 0.6066\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4432 - accuracy: 0.7887 - val_loss: 1.2639 - val_accuracy: 0.6134\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4359 - accuracy: 0.7958 - val_loss: 1.3004 - val_accuracy: 0.6084\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4364 - accuracy: 0.7915 - val_loss: 1.3509 - val_accuracy: 0.6113\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4505 - accuracy: 0.7928 - val_loss: 1.2907 - val_accuracy: 0.5980\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4341 - accuracy: 0.7986 - val_loss: 1.3033 - val_accuracy: 0.6048\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4221 - accuracy: 0.7960 - val_loss: 1.3905 - val_accuracy: 0.6016\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4333 - accuracy: 0.7926 - val_loss: 1.3720 - val_accuracy: 0.6030\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4349 - accuracy: 0.7957 - val_loss: 1.3028 - val_accuracy: 0.5976\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4190 - accuracy: 0.8024 - val_loss: 1.3542 - val_accuracy: 0.5962\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4151 - accuracy: 0.8034 - val_loss: 1.3689 - val_accuracy: 0.6052\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4236 - accuracy: 0.8004 - val_loss: 1.3916 - val_accuracy: 0.5965\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4171 - accuracy: 0.7992 - val_loss: 1.4533 - val_accuracy: 0.6001\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4131 - accuracy: 0.8012 - val_loss: 1.4221 - val_accuracy: 0.6023\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4028 - accuracy: 0.8077 - val_loss: 1.3607 - val_accuracy: 0.6045\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3969 - accuracy: 0.8121 - val_loss: 1.4351 - val_accuracy: 0.6005\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3964 - accuracy: 0.8144 - val_loss: 1.5315 - val_accuracy: 0.5843\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3949 - accuracy: 0.8149 - val_loss: 1.5712 - val_accuracy: 0.6037\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3992 - accuracy: 0.8082 - val_loss: 1.4965 - val_accuracy: 0.5912\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4015 - accuracy: 0.8073 - val_loss: 1.6100 - val_accuracy: 0.6045\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4005 - accuracy: 0.8089 - val_loss: 1.5560 - val_accuracy: 0.5948\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3959 - accuracy: 0.8106 - val_loss: 1.5948 - val_accuracy: 0.5991\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3977 - accuracy: 0.8128 - val_loss: 1.5194 - val_accuracy: 0.5991\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3834 - accuracy: 0.8161 - val_loss: 1.5409 - val_accuracy: 0.5994\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3843 - accuracy: 0.8193 - val_loss: 1.5866 - val_accuracy: 0.5955\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3824 - accuracy: 0.8191 - val_loss: 1.5705 - val_accuracy: 0.5998\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3820 - accuracy: 0.8154 - val_loss: 1.6106 - val_accuracy: 0.6052\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3765 - accuracy: 0.8188 - val_loss: 1.6226 - val_accuracy: 0.6084\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3826 - accuracy: 0.8201 - val_loss: 1.6881 - val_accuracy: 0.5937\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3709 - accuracy: 0.8240 - val_loss: 1.6373 - val_accuracy: 0.5983\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3705 - accuracy: 0.8198 - val_loss: 1.6299 - val_accuracy: 0.5937\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3716 - accuracy: 0.8219 - val_loss: 1.6649 - val_accuracy: 0.5958\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3818 - accuracy: 0.8152 - val_loss: 1.7026 - val_accuracy: 0.5994\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3662 - accuracy: 0.8233 - val_loss: 1.7077 - val_accuracy: 0.5919\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3648 - accuracy: 0.8261 - val_loss: 1.7237 - val_accuracy: 0.5962\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3566 - accuracy: 0.8298 - val_loss: 1.7168 - val_accuracy: 0.5868\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3515 - accuracy: 0.8297 - val_loss: 1.7083 - val_accuracy: 0.5912\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3552 - accuracy: 0.8246 - val_loss: 1.7563 - val_accuracy: 0.5930\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3533 - accuracy: 0.8292 - val_loss: 1.7052 - val_accuracy: 0.5955\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3633 - accuracy: 0.8243 - val_loss: 1.7562 - val_accuracy: 0.5926\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3579 - accuracy: 0.8288 - val_loss: 1.7600 - val_accuracy: 0.5958\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3490 - accuracy: 0.8293 - val_loss: 1.8240 - val_accuracy: 0.5912\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3580 - accuracy: 0.8282 - val_loss: 1.7750 - val_accuracy: 0.5980\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3484 - accuracy: 0.8313 - val_loss: 1.9166 - val_accuracy: 0.5850\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3513 - accuracy: 0.8353 - val_loss: 1.8595 - val_accuracy: 0.5814\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3505 - accuracy: 0.8339 - val_loss: 1.7702 - val_accuracy: 0.5940\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3405 - accuracy: 0.8378 - val_loss: 1.7735 - val_accuracy: 0.6012\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3416 - accuracy: 0.8336 - val_loss: 1.8754 - val_accuracy: 0.5901\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3328 - accuracy: 0.8362 - val_loss: 1.7818 - val_accuracy: 0.5890\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3401 - accuracy: 0.8349 - val_loss: 1.8891 - val_accuracy: 0.5847\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3319 - accuracy: 0.8391 - val_loss: 1.9145 - val_accuracy: 0.5922\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3320 - accuracy: 0.8374 - val_loss: 2.0113 - val_accuracy: 0.5789\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3236 - accuracy: 0.8436 - val_loss: 2.0223 - val_accuracy: 0.5894\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3475 - accuracy: 0.8313 - val_loss: 1.9282 - val_accuracy: 0.5962\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3462 - accuracy: 0.8353 - val_loss: 1.8962 - val_accuracy: 0.5987\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3358 - accuracy: 0.8379 - val_loss: 1.9175 - val_accuracy: 0.5868\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3215 - accuracy: 0.8435 - val_loss: 1.9094 - val_accuracy: 0.5886\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3296 - accuracy: 0.8442 - val_loss: 1.9726 - val_accuracy: 0.5822\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3272 - accuracy: 0.8408 - val_loss: 2.0077 - val_accuracy: 0.5897\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3332 - accuracy: 0.8393 - val_loss: 1.9095 - val_accuracy: 0.5778\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3164 - accuracy: 0.8479 - val_loss: 1.9719 - val_accuracy: 0.5868\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3141 - accuracy: 0.8482 - val_loss: 1.9597 - val_accuracy: 0.5832\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3204 - accuracy: 0.8439 - val_loss: 2.0233 - val_accuracy: 0.5915\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3244 - accuracy: 0.8446 - val_loss: 1.9839 - val_accuracy: 0.5858\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3311 - accuracy: 0.8390 - val_loss: 2.0711 - val_accuracy: 0.5919\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3316 - accuracy: 0.8429 - val_loss: 2.0177 - val_accuracy: 0.5840\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3126 - accuracy: 0.8467 - val_loss: 2.0425 - val_accuracy: 0.5840\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3104 - accuracy: 0.8475 - val_loss: 2.0875 - val_accuracy: 0.5879\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3061 - accuracy: 0.8498 - val_loss: 2.1414 - val_accuracy: 0.5854\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3064 - accuracy: 0.8477 - val_loss: 2.1276 - val_accuracy: 0.5832\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3056 - accuracy: 0.8504 - val_loss: 2.0164 - val_accuracy: 0.5858\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3106 - accuracy: 0.8506 - val_loss: 2.0923 - val_accuracy: 0.5915\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3078 - accuracy: 0.8480 - val_loss: 2.1381 - val_accuracy: 0.5840\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3038 - accuracy: 0.8495 - val_loss: 2.1493 - val_accuracy: 0.5897\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3133 - accuracy: 0.8491 - val_loss: 2.0830 - val_accuracy: 0.5858\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3169 - accuracy: 0.8479 - val_loss: 2.0884 - val_accuracy: 0.5930\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3095 - accuracy: 0.8507 - val_loss: 2.2560 - val_accuracy: 0.5843\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3075 - accuracy: 0.8525 - val_loss: 2.1954 - val_accuracy: 0.5836\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3048 - accuracy: 0.8536 - val_loss: 2.1685 - val_accuracy: 0.5868\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2941 - accuracy: 0.8545 - val_loss: 2.2273 - val_accuracy: 0.5858\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2928 - accuracy: 0.8566 - val_loss: 2.1720 - val_accuracy: 0.5948\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2981 - accuracy: 0.8545 - val_loss: 2.2187 - val_accuracy: 0.5858\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2970 - accuracy: 0.8542 - val_loss: 2.1669 - val_accuracy: 0.5858\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2915 - accuracy: 0.8550 - val_loss: 2.2048 - val_accuracy: 0.5908\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3015 - accuracy: 0.8554 - val_loss: 2.1432 - val_accuracy: 0.5937\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2964 - accuracy: 0.8536 - val_loss: 2.2698 - val_accuracy: 0.5825\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3020 - accuracy: 0.8525 - val_loss: 2.2226 - val_accuracy: 0.5786\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3085 - accuracy: 0.8545 - val_loss: 2.2479 - val_accuracy: 0.5753\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2984 - accuracy: 0.8535 - val_loss: 2.2953 - val_accuracy: 0.5847\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2902 - accuracy: 0.8578 - val_loss: 2.3364 - val_accuracy: 0.5843\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2976 - accuracy: 0.8546 - val_loss: 2.2910 - val_accuracy: 0.5850\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.2889 - accuracy: 0.8594 - val_loss: 2.3959 - val_accuracy: 0.5922\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2793 - accuracy: 0.8596 - val_loss: 2.3537 - val_accuracy: 0.5879\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2753 - accuracy: 0.8634 - val_loss: 2.4204 - val_accuracy: 0.5804\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2789 - accuracy: 0.8590 - val_loss: 2.3223 - val_accuracy: 0.5782\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2993 - accuracy: 0.8538 - val_loss: 2.1861 - val_accuracy: 0.5872\n",
            "History for model 16: <keras.src.callbacks.History object at 0x7e83eaee77c0>\n",
            "Trial 17: Number of layers = 4, Number of neurons per layer = 120\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 5s 9ms/step - loss: 0.8214 - accuracy: 0.6073 - val_loss: 0.8066 - val_accuracy: 0.6156\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7740 - accuracy: 0.6272 - val_loss: 0.7867 - val_accuracy: 0.6271\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7565 - accuracy: 0.6355 - val_loss: 0.7700 - val_accuracy: 0.6390\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7422 - accuracy: 0.6455 - val_loss: 0.7809 - val_accuracy: 0.6383\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7318 - accuracy: 0.6539 - val_loss: 0.7901 - val_accuracy: 0.6289\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7227 - accuracy: 0.6586 - val_loss: 0.7842 - val_accuracy: 0.6271\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7116 - accuracy: 0.6658 - val_loss: 0.7683 - val_accuracy: 0.6383\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7012 - accuracy: 0.6693 - val_loss: 0.7864 - val_accuracy: 0.6286\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6939 - accuracy: 0.6743 - val_loss: 0.7764 - val_accuracy: 0.6365\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6811 - accuracy: 0.6805 - val_loss: 0.7728 - val_accuracy: 0.6455\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6732 - accuracy: 0.6871 - val_loss: 0.7932 - val_accuracy: 0.6282\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6631 - accuracy: 0.6907 - val_loss: 0.7876 - val_accuracy: 0.6347\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6535 - accuracy: 0.6964 - val_loss: 0.8025 - val_accuracy: 0.6350\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.6464 - accuracy: 0.6974 - val_loss: 0.7901 - val_accuracy: 0.6314\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6396 - accuracy: 0.7021 - val_loss: 0.8078 - val_accuracy: 0.6253\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6263 - accuracy: 0.7077 - val_loss: 0.8261 - val_accuracy: 0.6332\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6179 - accuracy: 0.7131 - val_loss: 0.8307 - val_accuracy: 0.6321\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6103 - accuracy: 0.7164 - val_loss: 0.8272 - val_accuracy: 0.6307\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6027 - accuracy: 0.7192 - val_loss: 0.8182 - val_accuracy: 0.6188\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5945 - accuracy: 0.7256 - val_loss: 0.8540 - val_accuracy: 0.6300\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5890 - accuracy: 0.7289 - val_loss: 0.8356 - val_accuracy: 0.6271\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5787 - accuracy: 0.7259 - val_loss: 0.8800 - val_accuracy: 0.6214\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5699 - accuracy: 0.7360 - val_loss: 0.8845 - val_accuracy: 0.6257\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5626 - accuracy: 0.7380 - val_loss: 0.9067 - val_accuracy: 0.6278\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5526 - accuracy: 0.7427 - val_loss: 0.9125 - val_accuracy: 0.6286\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5548 - accuracy: 0.7410 - val_loss: 0.9359 - val_accuracy: 0.6242\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5439 - accuracy: 0.7515 - val_loss: 0.9610 - val_accuracy: 0.6286\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5381 - accuracy: 0.7475 - val_loss: 0.9503 - val_accuracy: 0.6242\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5277 - accuracy: 0.7534 - val_loss: 0.9508 - val_accuracy: 0.6260\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5237 - accuracy: 0.7562 - val_loss: 0.9640 - val_accuracy: 0.6268\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5202 - accuracy: 0.7547 - val_loss: 1.0092 - val_accuracy: 0.6278\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5126 - accuracy: 0.7596 - val_loss: 1.0311 - val_accuracy: 0.6214\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5084 - accuracy: 0.7605 - val_loss: 1.0022 - val_accuracy: 0.6271\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4972 - accuracy: 0.7662 - val_loss: 1.0835 - val_accuracy: 0.6160\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4954 - accuracy: 0.7718 - val_loss: 1.1065 - val_accuracy: 0.6163\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4910 - accuracy: 0.7700 - val_loss: 1.0679 - val_accuracy: 0.6174\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4829 - accuracy: 0.7725 - val_loss: 1.0617 - val_accuracy: 0.6167\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4807 - accuracy: 0.7729 - val_loss: 1.1152 - val_accuracy: 0.6196\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4822 - accuracy: 0.7732 - val_loss: 1.1209 - val_accuracy: 0.6224\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4679 - accuracy: 0.7827 - val_loss: 1.1424 - val_accuracy: 0.6192\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4630 - accuracy: 0.7843 - val_loss: 1.1455 - val_accuracy: 0.6221\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4577 - accuracy: 0.7836 - val_loss: 1.2138 - val_accuracy: 0.5980\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4515 - accuracy: 0.7879 - val_loss: 1.1930 - val_accuracy: 0.6235\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4471 - accuracy: 0.7886 - val_loss: 1.1871 - val_accuracy: 0.6170\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4472 - accuracy: 0.7899 - val_loss: 1.2457 - val_accuracy: 0.6142\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4465 - accuracy: 0.7908 - val_loss: 1.1939 - val_accuracy: 0.6070\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4350 - accuracy: 0.7951 - val_loss: 1.2876 - val_accuracy: 0.6102\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4334 - accuracy: 0.7968 - val_loss: 1.2155 - val_accuracy: 0.6185\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4255 - accuracy: 0.7999 - val_loss: 1.2557 - val_accuracy: 0.6070\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4328 - accuracy: 0.7970 - val_loss: 1.2287 - val_accuracy: 0.6145\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4212 - accuracy: 0.7976 - val_loss: 1.2742 - val_accuracy: 0.6185\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4186 - accuracy: 0.7999 - val_loss: 1.2816 - val_accuracy: 0.6134\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4213 - accuracy: 0.7985 - val_loss: 1.2877 - val_accuracy: 0.6203\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4125 - accuracy: 0.8034 - val_loss: 1.3307 - val_accuracy: 0.6120\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4045 - accuracy: 0.8063 - val_loss: 1.3412 - val_accuracy: 0.6088\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3967 - accuracy: 0.8101 - val_loss: 1.3973 - val_accuracy: 0.6102\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4025 - accuracy: 0.8066 - val_loss: 1.3494 - val_accuracy: 0.6163\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3978 - accuracy: 0.8098 - val_loss: 1.3096 - val_accuracy: 0.6142\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3987 - accuracy: 0.8133 - val_loss: 1.4309 - val_accuracy: 0.6081\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3902 - accuracy: 0.8089 - val_loss: 1.4080 - val_accuracy: 0.6099\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3941 - accuracy: 0.8127 - val_loss: 1.5052 - val_accuracy: 0.6077\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3844 - accuracy: 0.8165 - val_loss: 1.5580 - val_accuracy: 0.6156\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3753 - accuracy: 0.8162 - val_loss: 1.4846 - val_accuracy: 0.6117\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3735 - accuracy: 0.8222 - val_loss: 1.4689 - val_accuracy: 0.6037\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3728 - accuracy: 0.8211 - val_loss: 1.5350 - val_accuracy: 0.6081\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3687 - accuracy: 0.8184 - val_loss: 1.5742 - val_accuracy: 0.6063\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3700 - accuracy: 0.8216 - val_loss: 1.5877 - val_accuracy: 0.6081\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3701 - accuracy: 0.8206 - val_loss: 1.5454 - val_accuracy: 0.6045\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3682 - accuracy: 0.8254 - val_loss: 1.6946 - val_accuracy: 0.6149\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3834 - accuracy: 0.8164 - val_loss: 1.5171 - val_accuracy: 0.6052\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3648 - accuracy: 0.8240 - val_loss: 1.6091 - val_accuracy: 0.6012\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3506 - accuracy: 0.8296 - val_loss: 1.6298 - val_accuracy: 0.6052\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3531 - accuracy: 0.8341 - val_loss: 1.6971 - val_accuracy: 0.6009\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3454 - accuracy: 0.8300 - val_loss: 1.6425 - val_accuracy: 0.5969\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3430 - accuracy: 0.8347 - val_loss: 1.7752 - val_accuracy: 0.6027\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3526 - accuracy: 0.8282 - val_loss: 1.7097 - val_accuracy: 0.6063\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3538 - accuracy: 0.8305 - val_loss: 1.6794 - val_accuracy: 0.6106\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3476 - accuracy: 0.8295 - val_loss: 1.7068 - val_accuracy: 0.6063\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3390 - accuracy: 0.8330 - val_loss: 1.7767 - val_accuracy: 0.6001\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3345 - accuracy: 0.8352 - val_loss: 1.8166 - val_accuracy: 0.6145\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3417 - accuracy: 0.8323 - val_loss: 1.7927 - val_accuracy: 0.5994\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3319 - accuracy: 0.8385 - val_loss: 1.6713 - val_accuracy: 0.5933\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3346 - accuracy: 0.8365 - val_loss: 1.7793 - val_accuracy: 0.5973\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3389 - accuracy: 0.8384 - val_loss: 1.7769 - val_accuracy: 0.6023\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3304 - accuracy: 0.8386 - val_loss: 1.7441 - val_accuracy: 0.6052\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3241 - accuracy: 0.8398 - val_loss: 1.8835 - val_accuracy: 0.6041\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3241 - accuracy: 0.8428 - val_loss: 1.8822 - val_accuracy: 0.6037\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3351 - accuracy: 0.8367 - val_loss: 1.7441 - val_accuracy: 0.5883\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3309 - accuracy: 0.8375 - val_loss: 1.9523 - val_accuracy: 0.6091\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3211 - accuracy: 0.8408 - val_loss: 1.8820 - val_accuracy: 0.5976\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3101 - accuracy: 0.8446 - val_loss: 1.9487 - val_accuracy: 0.5937\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3156 - accuracy: 0.8420 - val_loss: 1.9829 - val_accuracy: 0.6059\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3217 - accuracy: 0.8435 - val_loss: 1.8288 - val_accuracy: 0.6009\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3057 - accuracy: 0.8488 - val_loss: 1.8822 - val_accuracy: 0.5958\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3033 - accuracy: 0.8537 - val_loss: 2.0860 - val_accuracy: 0.6055\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3005 - accuracy: 0.8493 - val_loss: 1.9974 - val_accuracy: 0.5955\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3126 - accuracy: 0.8463 - val_loss: 2.0041 - val_accuracy: 0.5868\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3219 - accuracy: 0.8467 - val_loss: 2.0806 - val_accuracy: 0.6030\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3159 - accuracy: 0.8439 - val_loss: 2.0662 - val_accuracy: 0.5958\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3161 - accuracy: 0.8448 - val_loss: 2.0177 - val_accuracy: 0.5886\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2915 - accuracy: 0.8552 - val_loss: 1.9997 - val_accuracy: 0.6012\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2924 - accuracy: 0.8575 - val_loss: 2.0886 - val_accuracy: 0.6052\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2923 - accuracy: 0.8567 - val_loss: 2.0976 - val_accuracy: 0.5948\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2928 - accuracy: 0.8535 - val_loss: 2.1069 - val_accuracy: 0.5868\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2998 - accuracy: 0.8505 - val_loss: 2.1565 - val_accuracy: 0.5983\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3048 - accuracy: 0.8479 - val_loss: 2.0238 - val_accuracy: 0.5973\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2914 - accuracy: 0.8588 - val_loss: 2.2911 - val_accuracy: 0.5962\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2891 - accuracy: 0.8566 - val_loss: 2.2755 - val_accuracy: 0.5901\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2852 - accuracy: 0.8535 - val_loss: 2.3104 - val_accuracy: 0.5980\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2876 - accuracy: 0.8570 - val_loss: 2.2369 - val_accuracy: 0.6023\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2908 - accuracy: 0.8563 - val_loss: 2.2173 - val_accuracy: 0.5976\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2907 - accuracy: 0.8566 - val_loss: 2.1333 - val_accuracy: 0.5940\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2944 - accuracy: 0.8525 - val_loss: 2.1422 - val_accuracy: 0.5955\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2955 - accuracy: 0.8565 - val_loss: 2.0783 - val_accuracy: 0.5894\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2810 - accuracy: 0.8599 - val_loss: 2.1944 - val_accuracy: 0.5901\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2791 - accuracy: 0.8602 - val_loss: 2.2092 - val_accuracy: 0.5955\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2743 - accuracy: 0.8613 - val_loss: 2.3663 - val_accuracy: 0.5930\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2780 - accuracy: 0.8633 - val_loss: 2.3458 - val_accuracy: 0.5847\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2934 - accuracy: 0.8576 - val_loss: 2.1561 - val_accuracy: 0.5901\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2876 - accuracy: 0.8573 - val_loss: 2.1584 - val_accuracy: 0.5965\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2827 - accuracy: 0.8603 - val_loss: 2.2289 - val_accuracy: 0.5969\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2759 - accuracy: 0.8634 - val_loss: 2.2854 - val_accuracy: 0.5973\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2636 - accuracy: 0.8686 - val_loss: 2.2763 - val_accuracy: 0.5933\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2745 - accuracy: 0.8642 - val_loss: 2.3244 - val_accuracy: 0.5937\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2790 - accuracy: 0.8608 - val_loss: 2.4027 - val_accuracy: 0.5912\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2789 - accuracy: 0.8617 - val_loss: 2.3629 - val_accuracy: 0.5901\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2596 - accuracy: 0.8669 - val_loss: 2.3751 - val_accuracy: 0.5897\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2694 - accuracy: 0.8630 - val_loss: 2.3453 - val_accuracy: 0.5930\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2721 - accuracy: 0.8643 - val_loss: 2.2817 - val_accuracy: 0.5858\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2660 - accuracy: 0.8655 - val_loss: 2.3901 - val_accuracy: 0.5840\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2874 - accuracy: 0.8588 - val_loss: 2.2257 - val_accuracy: 0.5843\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2697 - accuracy: 0.8624 - val_loss: 2.3192 - val_accuracy: 0.5825\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2738 - accuracy: 0.8666 - val_loss: 2.2287 - val_accuracy: 0.5915\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2554 - accuracy: 0.8705 - val_loss: 2.3784 - val_accuracy: 0.5868\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2711 - accuracy: 0.8647 - val_loss: 2.1907 - val_accuracy: 0.5818\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2716 - accuracy: 0.8613 - val_loss: 2.3163 - val_accuracy: 0.5973\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2558 - accuracy: 0.8695 - val_loss: 2.5459 - val_accuracy: 0.5818\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2635 - accuracy: 0.8672 - val_loss: 2.3086 - val_accuracy: 0.5944\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2649 - accuracy: 0.8698 - val_loss: 2.3428 - val_accuracy: 0.5865\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2671 - accuracy: 0.8710 - val_loss: 2.3730 - val_accuracy: 0.5883\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2558 - accuracy: 0.8686 - val_loss: 2.3771 - val_accuracy: 0.5919\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2609 - accuracy: 0.8722 - val_loss: 2.3301 - val_accuracy: 0.5983\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2665 - accuracy: 0.8703 - val_loss: 2.4364 - val_accuracy: 0.5872\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2534 - accuracy: 0.8700 - val_loss: 2.3946 - val_accuracy: 0.5955\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2452 - accuracy: 0.8739 - val_loss: 2.4778 - val_accuracy: 0.5912\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2554 - accuracy: 0.8669 - val_loss: 2.4976 - val_accuracy: 0.5951\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2476 - accuracy: 0.8709 - val_loss: 2.6064 - val_accuracy: 0.5962\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2619 - accuracy: 0.8699 - val_loss: 2.5278 - val_accuracy: 0.5948\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2704 - accuracy: 0.8656 - val_loss: 2.4982 - val_accuracy: 0.5965\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2523 - accuracy: 0.8714 - val_loss: 2.4044 - val_accuracy: 0.5987\n",
            "History for model 17: <keras.src.callbacks.History object at 0x7e83eaccf370>\n",
            "Trial 18: Number of layers = 4, Number of neurons per layer = 150\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 4s 7ms/step - loss: 0.8230 - accuracy: 0.6052 - val_loss: 0.8049 - val_accuracy: 0.6246\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7742 - accuracy: 0.6306 - val_loss: 0.7943 - val_accuracy: 0.6282\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7595 - accuracy: 0.6402 - val_loss: 0.7756 - val_accuracy: 0.6361\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7429 - accuracy: 0.6486 - val_loss: 0.7759 - val_accuracy: 0.6375\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.7315 - accuracy: 0.6545 - val_loss: 0.7966 - val_accuracy: 0.6214\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7208 - accuracy: 0.6591 - val_loss: 0.7858 - val_accuracy: 0.6365\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7076 - accuracy: 0.6680 - val_loss: 0.7740 - val_accuracy: 0.6401\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6973 - accuracy: 0.6750 - val_loss: 0.7964 - val_accuracy: 0.6278\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6887 - accuracy: 0.6757 - val_loss: 0.7913 - val_accuracy: 0.6300\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6763 - accuracy: 0.6818 - val_loss: 0.7876 - val_accuracy: 0.6199\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6666 - accuracy: 0.6876 - val_loss: 0.7910 - val_accuracy: 0.6278\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6555 - accuracy: 0.6929 - val_loss: 0.8181 - val_accuracy: 0.6278\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6452 - accuracy: 0.6971 - val_loss: 0.8009 - val_accuracy: 0.6336\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6370 - accuracy: 0.6995 - val_loss: 0.8220 - val_accuracy: 0.6329\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6256 - accuracy: 0.7075 - val_loss: 0.8229 - val_accuracy: 0.6253\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6186 - accuracy: 0.7140 - val_loss: 0.8608 - val_accuracy: 0.6156\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6057 - accuracy: 0.7204 - val_loss: 0.8516 - val_accuracy: 0.6300\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5984 - accuracy: 0.7216 - val_loss: 0.8533 - val_accuracy: 0.6232\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5908 - accuracy: 0.7229 - val_loss: 0.8681 - val_accuracy: 0.6260\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5795 - accuracy: 0.7271 - val_loss: 0.8855 - val_accuracy: 0.6221\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5697 - accuracy: 0.7333 - val_loss: 0.9116 - val_accuracy: 0.6170\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5618 - accuracy: 0.7376 - val_loss: 0.9042 - val_accuracy: 0.6210\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5605 - accuracy: 0.7388 - val_loss: 0.9152 - val_accuracy: 0.6224\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5470 - accuracy: 0.7443 - val_loss: 0.8964 - val_accuracy: 0.6167\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5367 - accuracy: 0.7460 - val_loss: 0.9601 - val_accuracy: 0.6221\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5252 - accuracy: 0.7521 - val_loss: 1.0144 - val_accuracy: 0.6268\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.5293 - accuracy: 0.7500 - val_loss: 0.9986 - val_accuracy: 0.6228\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5224 - accuracy: 0.7540 - val_loss: 0.9840 - val_accuracy: 0.6232\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5149 - accuracy: 0.7533 - val_loss: 1.0593 - val_accuracy: 0.6307\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5021 - accuracy: 0.7620 - val_loss: 1.0811 - val_accuracy: 0.6264\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4942 - accuracy: 0.7644 - val_loss: 1.1007 - val_accuracy: 0.6124\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4942 - accuracy: 0.7638 - val_loss: 1.0933 - val_accuracy: 0.6185\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4860 - accuracy: 0.7703 - val_loss: 1.1075 - val_accuracy: 0.6048\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4888 - accuracy: 0.7654 - val_loss: 1.1916 - val_accuracy: 0.6066\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4799 - accuracy: 0.7716 - val_loss: 1.1695 - val_accuracy: 0.6185\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4639 - accuracy: 0.7798 - val_loss: 1.1638 - val_accuracy: 0.6048\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4687 - accuracy: 0.7757 - val_loss: 1.2639 - val_accuracy: 0.6045\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4629 - accuracy: 0.7815 - val_loss: 1.2386 - val_accuracy: 0.6041\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4483 - accuracy: 0.7881 - val_loss: 1.2326 - val_accuracy: 0.6091\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4494 - accuracy: 0.7878 - val_loss: 1.2365 - val_accuracy: 0.6127\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4457 - accuracy: 0.7851 - val_loss: 1.2421 - val_accuracy: 0.6099\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4417 - accuracy: 0.7885 - val_loss: 1.2950 - val_accuracy: 0.5969\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4306 - accuracy: 0.7939 - val_loss: 1.2993 - val_accuracy: 0.6102\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4287 - accuracy: 0.7900 - val_loss: 1.3008 - val_accuracy: 0.6066\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4212 - accuracy: 0.7973 - val_loss: 1.3884 - val_accuracy: 0.6045\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4258 - accuracy: 0.7973 - val_loss: 1.2907 - val_accuracy: 0.6099\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4172 - accuracy: 0.7966 - val_loss: 1.3533 - val_accuracy: 0.6045\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4094 - accuracy: 0.7998 - val_loss: 1.4068 - val_accuracy: 0.5973\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4162 - accuracy: 0.7966 - val_loss: 1.3929 - val_accuracy: 0.6034\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4049 - accuracy: 0.8032 - val_loss: 1.5843 - val_accuracy: 0.5994\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4029 - accuracy: 0.8065 - val_loss: 1.5007 - val_accuracy: 0.5937\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3956 - accuracy: 0.8041 - val_loss: 1.4471 - val_accuracy: 0.6041\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3832 - accuracy: 0.8133 - val_loss: 1.5380 - val_accuracy: 0.5998\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3858 - accuracy: 0.8109 - val_loss: 1.6410 - val_accuracy: 0.5958\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3843 - accuracy: 0.8148 - val_loss: 1.5181 - val_accuracy: 0.6081\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3911 - accuracy: 0.8153 - val_loss: 1.5089 - val_accuracy: 0.5998\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3904 - accuracy: 0.8118 - val_loss: 1.5901 - val_accuracy: 0.5951\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3785 - accuracy: 0.8165 - val_loss: 1.5906 - val_accuracy: 0.5904\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3637 - accuracy: 0.8223 - val_loss: 1.6041 - val_accuracy: 0.5969\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3650 - accuracy: 0.8211 - val_loss: 1.6624 - val_accuracy: 0.5933\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3687 - accuracy: 0.8199 - val_loss: 1.5674 - val_accuracy: 0.6019\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3631 - accuracy: 0.8208 - val_loss: 1.6795 - val_accuracy: 0.5944\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3521 - accuracy: 0.8254 - val_loss: 1.7902 - val_accuracy: 0.5940\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3620 - accuracy: 0.8216 - val_loss: 1.8266 - val_accuracy: 0.5915\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3545 - accuracy: 0.8289 - val_loss: 1.8061 - val_accuracy: 0.5836\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3583 - accuracy: 0.8236 - val_loss: 1.7752 - val_accuracy: 0.5908\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3461 - accuracy: 0.8261 - val_loss: 1.8489 - val_accuracy: 0.5840\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3496 - accuracy: 0.8327 - val_loss: 1.8472 - val_accuracy: 0.5940\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3499 - accuracy: 0.8271 - val_loss: 1.8378 - val_accuracy: 0.6059\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3412 - accuracy: 0.8344 - val_loss: 1.7898 - val_accuracy: 0.5933\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3324 - accuracy: 0.8352 - val_loss: 1.8901 - val_accuracy: 0.5912\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3379 - accuracy: 0.8301 - val_loss: 1.8814 - val_accuracy: 0.5958\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3443 - accuracy: 0.8305 - val_loss: 1.8347 - val_accuracy: 0.5901\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3290 - accuracy: 0.8390 - val_loss: 1.9472 - val_accuracy: 0.5829\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3301 - accuracy: 0.8389 - val_loss: 1.9888 - val_accuracy: 0.5872\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3265 - accuracy: 0.8369 - val_loss: 1.8800 - val_accuracy: 0.5804\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3264 - accuracy: 0.8389 - val_loss: 1.9217 - val_accuracy: 0.5886\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3304 - accuracy: 0.8358 - val_loss: 2.0039 - val_accuracy: 0.5822\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3181 - accuracy: 0.8395 - val_loss: 2.1009 - val_accuracy: 0.5868\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3213 - accuracy: 0.8398 - val_loss: 2.0606 - val_accuracy: 0.5912\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3216 - accuracy: 0.8437 - val_loss: 2.0836 - val_accuracy: 0.5793\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3235 - accuracy: 0.8409 - val_loss: 2.0255 - val_accuracy: 0.5861\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3150 - accuracy: 0.8453 - val_loss: 1.9716 - val_accuracy: 0.5868\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3060 - accuracy: 0.8453 - val_loss: 2.2526 - val_accuracy: 0.5901\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3077 - accuracy: 0.8452 - val_loss: 2.0527 - val_accuracy: 0.5840\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3076 - accuracy: 0.8515 - val_loss: 2.0632 - val_accuracy: 0.5800\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3127 - accuracy: 0.8449 - val_loss: 2.0091 - val_accuracy: 0.5858\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3047 - accuracy: 0.8501 - val_loss: 2.1491 - val_accuracy: 0.5908\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2969 - accuracy: 0.8524 - val_loss: 2.1366 - val_accuracy: 0.5980\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2882 - accuracy: 0.8531 - val_loss: 2.1767 - val_accuracy: 0.5800\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2961 - accuracy: 0.8502 - val_loss: 2.2271 - val_accuracy: 0.5685\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2966 - accuracy: 0.8492 - val_loss: 2.1111 - val_accuracy: 0.5897\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3031 - accuracy: 0.8486 - val_loss: 2.2485 - val_accuracy: 0.5804\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3031 - accuracy: 0.8520 - val_loss: 2.1363 - val_accuracy: 0.5732\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2849 - accuracy: 0.8561 - val_loss: 2.2288 - val_accuracy: 0.5775\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2887 - accuracy: 0.8501 - val_loss: 2.2501 - val_accuracy: 0.5850\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2842 - accuracy: 0.8534 - val_loss: 2.2610 - val_accuracy: 0.5836\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2973 - accuracy: 0.8545 - val_loss: 2.2865 - val_accuracy: 0.5807\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2899 - accuracy: 0.8571 - val_loss: 2.4290 - val_accuracy: 0.5786\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2803 - accuracy: 0.8580 - val_loss: 2.3023 - val_accuracy: 0.5761\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2928 - accuracy: 0.8561 - val_loss: 2.3634 - val_accuracy: 0.5868\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2838 - accuracy: 0.8593 - val_loss: 2.3234 - val_accuracy: 0.5793\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2802 - accuracy: 0.8636 - val_loss: 2.3956 - val_accuracy: 0.5775\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2755 - accuracy: 0.8593 - val_loss: 2.4457 - val_accuracy: 0.5793\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3008 - accuracy: 0.8502 - val_loss: 2.2842 - val_accuracy: 0.5743\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2942 - accuracy: 0.8558 - val_loss: 2.1946 - val_accuracy: 0.5818\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2733 - accuracy: 0.8629 - val_loss: 2.3851 - val_accuracy: 0.5778\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2654 - accuracy: 0.8629 - val_loss: 2.4675 - val_accuracy: 0.5786\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2551 - accuracy: 0.8669 - val_loss: 2.4260 - val_accuracy: 0.5721\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2640 - accuracy: 0.8648 - val_loss: 2.5121 - val_accuracy: 0.5786\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2661 - accuracy: 0.8642 - val_loss: 2.4793 - val_accuracy: 0.5778\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2767 - accuracy: 0.8605 - val_loss: 2.4953 - val_accuracy: 0.5725\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2777 - accuracy: 0.8611 - val_loss: 2.5058 - val_accuracy: 0.5811\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2925 - accuracy: 0.8549 - val_loss: 2.4949 - val_accuracy: 0.5890\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2726 - accuracy: 0.8635 - val_loss: 2.4873 - val_accuracy: 0.5843\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2660 - accuracy: 0.8657 - val_loss: 2.3896 - val_accuracy: 0.5807\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2559 - accuracy: 0.8684 - val_loss: 2.4266 - val_accuracy: 0.5793\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2654 - accuracy: 0.8607 - val_loss: 2.5209 - val_accuracy: 0.5836\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2501 - accuracy: 0.8703 - val_loss: 2.4850 - val_accuracy: 0.5843\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2643 - accuracy: 0.8631 - val_loss: 2.4280 - val_accuracy: 0.5778\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2763 - accuracy: 0.8605 - val_loss: 2.5823 - val_accuracy: 0.5883\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2661 - accuracy: 0.8656 - val_loss: 2.5029 - val_accuracy: 0.5764\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2763 - accuracy: 0.8663 - val_loss: 2.3924 - val_accuracy: 0.5868\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2693 - accuracy: 0.8687 - val_loss: 2.3948 - val_accuracy: 0.5868\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2529 - accuracy: 0.8739 - val_loss: 2.6010 - val_accuracy: 0.5890\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2419 - accuracy: 0.8762 - val_loss: 2.7615 - val_accuracy: 0.5872\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2442 - accuracy: 0.8724 - val_loss: 2.6647 - val_accuracy: 0.5789\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2554 - accuracy: 0.8681 - val_loss: 2.5914 - val_accuracy: 0.5850\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2645 - accuracy: 0.8652 - val_loss: 2.5653 - val_accuracy: 0.5811\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2668 - accuracy: 0.8676 - val_loss: 2.4880 - val_accuracy: 0.5771\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2547 - accuracy: 0.8687 - val_loss: 2.7365 - val_accuracy: 0.5796\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2436 - accuracy: 0.8747 - val_loss: 2.7861 - val_accuracy: 0.5829\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2474 - accuracy: 0.8703 - val_loss: 2.6442 - val_accuracy: 0.5721\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2382 - accuracy: 0.8741 - val_loss: 2.8013 - val_accuracy: 0.5825\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2535 - accuracy: 0.8718 - val_loss: 2.8091 - val_accuracy: 0.5865\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2581 - accuracy: 0.8712 - val_loss: 2.6487 - val_accuracy: 0.5876\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2396 - accuracy: 0.8766 - val_loss: 2.9104 - val_accuracy: 0.5681\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2503 - accuracy: 0.8725 - val_loss: 2.8168 - val_accuracy: 0.5775\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2626 - accuracy: 0.8675 - val_loss: 2.7316 - val_accuracy: 0.5886\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2396 - accuracy: 0.8756 - val_loss: 2.8909 - val_accuracy: 0.5789\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2390 - accuracy: 0.8778 - val_loss: 2.8806 - val_accuracy: 0.5825\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2384 - accuracy: 0.8742 - val_loss: 2.8091 - val_accuracy: 0.5854\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2400 - accuracy: 0.8765 - val_loss: 2.8411 - val_accuracy: 0.5836\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2410 - accuracy: 0.8752 - val_loss: 3.0759 - val_accuracy: 0.5807\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2514 - accuracy: 0.8739 - val_loss: 2.6693 - val_accuracy: 0.5850\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2456 - accuracy: 0.8751 - val_loss: 2.8340 - val_accuracy: 0.5782\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2504 - accuracy: 0.8718 - val_loss: 2.8218 - val_accuracy: 0.5832\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2551 - accuracy: 0.8741 - val_loss: 2.6766 - val_accuracy: 0.5782\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2435 - accuracy: 0.8772 - val_loss: 2.8514 - val_accuracy: 0.5840\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2277 - accuracy: 0.8806 - val_loss: 2.8520 - val_accuracy: 0.5768\n",
            "History for model 18: <keras.src.callbacks.History object at 0x7e83e7f9c1c0>\n",
            "Trial 19: Number of layers = 4, Number of neurons per layer = 180\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 7s 9ms/step - loss: 0.8187 - accuracy: 0.6070 - val_loss: 0.7863 - val_accuracy: 0.6300\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.7726 - accuracy: 0.6294 - val_loss: 0.7762 - val_accuracy: 0.6289\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.7582 - accuracy: 0.6418 - val_loss: 0.7826 - val_accuracy: 0.6422\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.7417 - accuracy: 0.6478 - val_loss: 0.8018 - val_accuracy: 0.6210\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.7308 - accuracy: 0.6517 - val_loss: 0.7688 - val_accuracy: 0.6350\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.7181 - accuracy: 0.6593 - val_loss: 0.7785 - val_accuracy: 0.6321\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.7083 - accuracy: 0.6649 - val_loss: 0.7808 - val_accuracy: 0.6318\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6963 - accuracy: 0.6722 - val_loss: 0.7917 - val_accuracy: 0.6329\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6854 - accuracy: 0.6790 - val_loss: 0.8024 - val_accuracy: 0.6375\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6772 - accuracy: 0.6808 - val_loss: 0.7906 - val_accuracy: 0.6347\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6648 - accuracy: 0.6894 - val_loss: 0.8111 - val_accuracy: 0.6235\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6560 - accuracy: 0.6908 - val_loss: 0.7810 - val_accuracy: 0.6383\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.6485 - accuracy: 0.6952 - val_loss: 0.7956 - val_accuracy: 0.6433\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.6328 - accuracy: 0.7027 - val_loss: 0.8089 - val_accuracy: 0.6390\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.6276 - accuracy: 0.7084 - val_loss: 0.8297 - val_accuracy: 0.6268\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6192 - accuracy: 0.7105 - val_loss: 0.8483 - val_accuracy: 0.6300\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6069 - accuracy: 0.7159 - val_loss: 0.8384 - val_accuracy: 0.6361\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.5982 - accuracy: 0.7220 - val_loss: 0.8454 - val_accuracy: 0.6286\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.5875 - accuracy: 0.7228 - val_loss: 0.8946 - val_accuracy: 0.6268\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5756 - accuracy: 0.7256 - val_loss: 0.9057 - val_accuracy: 0.6350\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5692 - accuracy: 0.7365 - val_loss: 0.9176 - val_accuracy: 0.6293\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.5646 - accuracy: 0.7337 - val_loss: 0.9419 - val_accuracy: 0.6246\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5527 - accuracy: 0.7396 - val_loss: 0.9906 - val_accuracy: 0.6311\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5460 - accuracy: 0.7391 - val_loss: 0.9978 - val_accuracy: 0.6206\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.5369 - accuracy: 0.7485 - val_loss: 1.0436 - val_accuracy: 0.6286\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5272 - accuracy: 0.7502 - val_loss: 1.0248 - val_accuracy: 0.6314\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.5263 - accuracy: 0.7550 - val_loss: 1.0167 - val_accuracy: 0.6260\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.5135 - accuracy: 0.7571 - val_loss: 1.0423 - val_accuracy: 0.6174\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.5091 - accuracy: 0.7617 - val_loss: 1.0226 - val_accuracy: 0.6102\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5058 - accuracy: 0.7646 - val_loss: 1.0225 - val_accuracy: 0.6199\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4952 - accuracy: 0.7680 - val_loss: 1.1111 - val_accuracy: 0.6156\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.4901 - accuracy: 0.7677 - val_loss: 1.0972 - val_accuracy: 0.6228\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4834 - accuracy: 0.7727 - val_loss: 1.1366 - val_accuracy: 0.6253\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4771 - accuracy: 0.7726 - val_loss: 1.1404 - val_accuracy: 0.6170\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4734 - accuracy: 0.7745 - val_loss: 1.1385 - val_accuracy: 0.6063\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4713 - accuracy: 0.7763 - val_loss: 1.1308 - val_accuracy: 0.6131\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4607 - accuracy: 0.7797 - val_loss: 1.1787 - val_accuracy: 0.6084\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4504 - accuracy: 0.7891 - val_loss: 1.2190 - val_accuracy: 0.6077\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4412 - accuracy: 0.7902 - val_loss: 1.2223 - val_accuracy: 0.6138\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4375 - accuracy: 0.7920 - val_loss: 1.2682 - val_accuracy: 0.6156\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4356 - accuracy: 0.7940 - val_loss: 1.3280 - val_accuracy: 0.6160\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4485 - accuracy: 0.7906 - val_loss: 1.3025 - val_accuracy: 0.6131\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4260 - accuracy: 0.7958 - val_loss: 1.3074 - val_accuracy: 0.5969\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4187 - accuracy: 0.8030 - val_loss: 1.3459 - val_accuracy: 0.6005\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4201 - accuracy: 0.8003 - val_loss: 1.2926 - val_accuracy: 0.6048\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4141 - accuracy: 0.7990 - val_loss: 1.4715 - val_accuracy: 0.6131\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4123 - accuracy: 0.8034 - val_loss: 1.3893 - val_accuracy: 0.6142\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4112 - accuracy: 0.8051 - val_loss: 1.4072 - val_accuracy: 0.6160\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4011 - accuracy: 0.8088 - val_loss: 1.3931 - val_accuracy: 0.6077\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.3990 - accuracy: 0.8080 - val_loss: 1.4445 - val_accuracy: 0.5940\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3908 - accuracy: 0.8088 - val_loss: 1.5086 - val_accuracy: 0.6099\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3896 - accuracy: 0.8130 - val_loss: 1.5271 - val_accuracy: 0.6124\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3837 - accuracy: 0.8147 - val_loss: 1.5963 - val_accuracy: 0.6117\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3788 - accuracy: 0.8181 - val_loss: 1.6248 - val_accuracy: 0.6077\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3745 - accuracy: 0.8175 - val_loss: 1.6574 - val_accuracy: 0.6134\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3784 - accuracy: 0.8172 - val_loss: 1.6308 - val_accuracy: 0.6048\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3763 - accuracy: 0.8192 - val_loss: 1.5727 - val_accuracy: 0.5991\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3652 - accuracy: 0.8216 - val_loss: 1.7141 - val_accuracy: 0.6001\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.3664 - accuracy: 0.8230 - val_loss: 1.7670 - val_accuracy: 0.5958\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3708 - accuracy: 0.8207 - val_loss: 1.6893 - val_accuracy: 0.6045\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3605 - accuracy: 0.8287 - val_loss: 1.7680 - val_accuracy: 0.6037\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3554 - accuracy: 0.8261 - val_loss: 1.7010 - val_accuracy: 0.6034\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3432 - accuracy: 0.8318 - val_loss: 1.7492 - val_accuracy: 0.6070\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3441 - accuracy: 0.8298 - val_loss: 1.8114 - val_accuracy: 0.5962\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3452 - accuracy: 0.8293 - val_loss: 1.7859 - val_accuracy: 0.5865\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3379 - accuracy: 0.8352 - val_loss: 1.8500 - val_accuracy: 0.5976\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3443 - accuracy: 0.8323 - val_loss: 1.7411 - val_accuracy: 0.6019\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3322 - accuracy: 0.8358 - val_loss: 1.9886 - val_accuracy: 0.5991\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3291 - accuracy: 0.8362 - val_loss: 1.8425 - val_accuracy: 0.5937\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3399 - accuracy: 0.8354 - val_loss: 1.8811 - val_accuracy: 0.6052\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3271 - accuracy: 0.8400 - val_loss: 1.8615 - val_accuracy: 0.5915\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3246 - accuracy: 0.8395 - val_loss: 1.9090 - val_accuracy: 0.5858\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3215 - accuracy: 0.8413 - val_loss: 1.9362 - val_accuracy: 0.5940\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3301 - accuracy: 0.8409 - val_loss: 1.9080 - val_accuracy: 0.5976\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3188 - accuracy: 0.8445 - val_loss: 1.9809 - val_accuracy: 0.5926\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3246 - accuracy: 0.8394 - val_loss: 1.9537 - val_accuracy: 0.5872\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3111 - accuracy: 0.8444 - val_loss: 1.9754 - val_accuracy: 0.5894\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3079 - accuracy: 0.8436 - val_loss: 2.1575 - val_accuracy: 0.5958\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3299 - accuracy: 0.8427 - val_loss: 2.0082 - val_accuracy: 0.5865\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.3000 - accuracy: 0.8483 - val_loss: 2.0178 - val_accuracy: 0.5976\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2996 - accuracy: 0.8480 - val_loss: 2.0625 - val_accuracy: 0.5865\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3081 - accuracy: 0.8465 - val_loss: 2.0733 - val_accuracy: 0.5948\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3138 - accuracy: 0.8462 - val_loss: 2.0928 - val_accuracy: 0.5937\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2968 - accuracy: 0.8509 - val_loss: 2.0504 - val_accuracy: 0.5829\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2893 - accuracy: 0.8526 - val_loss: 2.1541 - val_accuracy: 0.5804\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 3s 10ms/step - loss: 0.3016 - accuracy: 0.8478 - val_loss: 2.1316 - val_accuracy: 0.5933\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3075 - accuracy: 0.8492 - val_loss: 2.0408 - val_accuracy: 0.5930\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2955 - accuracy: 0.8535 - val_loss: 2.2236 - val_accuracy: 0.5951\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2988 - accuracy: 0.8496 - val_loss: 2.2534 - val_accuracy: 0.5836\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2981 - accuracy: 0.8525 - val_loss: 2.2992 - val_accuracy: 0.5836\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2809 - accuracy: 0.8562 - val_loss: 2.3343 - val_accuracy: 0.5969\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2724 - accuracy: 0.8614 - val_loss: 2.2789 - val_accuracy: 0.5854\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2907 - accuracy: 0.8498 - val_loss: 2.3550 - val_accuracy: 0.5973\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2812 - accuracy: 0.8567 - val_loss: 2.4191 - val_accuracy: 0.5926\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2804 - accuracy: 0.8564 - val_loss: 2.3567 - val_accuracy: 0.5876\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2889 - accuracy: 0.8551 - val_loss: 2.3344 - val_accuracy: 0.5948\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2962 - accuracy: 0.8515 - val_loss: 2.3102 - val_accuracy: 0.5883\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2891 - accuracy: 0.8552 - val_loss: 2.2270 - val_accuracy: 0.5804\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2603 - accuracy: 0.8625 - val_loss: 2.3611 - val_accuracy: 0.5872\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2636 - accuracy: 0.8647 - val_loss: 2.4648 - val_accuracy: 0.5879\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2919 - accuracy: 0.8536 - val_loss: 2.2784 - val_accuracy: 0.5980\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2840 - accuracy: 0.8603 - val_loss: 2.5177 - val_accuracy: 0.5908\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2851 - accuracy: 0.8579 - val_loss: 2.5379 - val_accuracy: 0.6059\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2714 - accuracy: 0.8624 - val_loss: 2.3954 - val_accuracy: 0.5969\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2577 - accuracy: 0.8691 - val_loss: 2.4090 - val_accuracy: 0.5897\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2612 - accuracy: 0.8647 - val_loss: 2.5694 - val_accuracy: 0.5912\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2699 - accuracy: 0.8630 - val_loss: 2.4755 - val_accuracy: 0.6030\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2600 - accuracy: 0.8700 - val_loss: 2.5780 - val_accuracy: 0.5955\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2594 - accuracy: 0.8678 - val_loss: 2.4471 - val_accuracy: 0.5901\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2550 - accuracy: 0.8670 - val_loss: 2.5714 - val_accuracy: 0.5868\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2619 - accuracy: 0.8669 - val_loss: 2.5338 - val_accuracy: 0.5926\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2737 - accuracy: 0.8630 - val_loss: 2.5407 - val_accuracy: 0.5825\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2734 - accuracy: 0.8641 - val_loss: 2.3258 - val_accuracy: 0.5868\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2604 - accuracy: 0.8687 - val_loss: 2.3433 - val_accuracy: 0.5843\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2451 - accuracy: 0.8716 - val_loss: 2.5489 - val_accuracy: 0.5825\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2471 - accuracy: 0.8711 - val_loss: 2.5624 - val_accuracy: 0.5883\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2537 - accuracy: 0.8711 - val_loss: 2.4720 - val_accuracy: 0.5930\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2557 - accuracy: 0.8679 - val_loss: 2.4726 - val_accuracy: 0.5908\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2471 - accuracy: 0.8730 - val_loss: 2.5266 - val_accuracy: 0.5829\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2524 - accuracy: 0.8709 - val_loss: 2.5792 - val_accuracy: 0.5861\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2518 - accuracy: 0.8704 - val_loss: 2.6134 - val_accuracy: 0.5948\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2625 - accuracy: 0.8675 - val_loss: 2.4476 - val_accuracy: 0.5858\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2589 - accuracy: 0.8704 - val_loss: 2.6130 - val_accuracy: 0.5850\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2397 - accuracy: 0.8759 - val_loss: 2.6462 - val_accuracy: 0.5886\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2430 - accuracy: 0.8697 - val_loss: 2.7841 - val_accuracy: 0.5782\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2411 - accuracy: 0.8741 - val_loss: 2.7360 - val_accuracy: 0.5829\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2485 - accuracy: 0.8707 - val_loss: 2.7044 - val_accuracy: 0.5807\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2569 - accuracy: 0.8704 - val_loss: 2.5639 - val_accuracy: 0.5965\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2520 - accuracy: 0.8729 - val_loss: 2.7495 - val_accuracy: 0.5951\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2351 - accuracy: 0.8759 - val_loss: 2.8448 - val_accuracy: 0.5930\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2357 - accuracy: 0.8730 - val_loss: 2.7559 - val_accuracy: 0.5930\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2411 - accuracy: 0.8733 - val_loss: 2.6529 - val_accuracy: 0.5897\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2578 - accuracy: 0.8736 - val_loss: 2.7732 - val_accuracy: 0.5904\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2308 - accuracy: 0.8795 - val_loss: 2.9404 - val_accuracy: 0.5937\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2274 - accuracy: 0.8826 - val_loss: 2.8584 - val_accuracy: 0.5886\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2329 - accuracy: 0.8786 - val_loss: 2.7371 - val_accuracy: 0.5922\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2332 - accuracy: 0.8767 - val_loss: 2.7301 - val_accuracy: 0.5901\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2362 - accuracy: 0.8770 - val_loss: 2.9942 - val_accuracy: 0.5926\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2335 - accuracy: 0.8780 - val_loss: 2.9942 - val_accuracy: 0.5850\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2374 - accuracy: 0.8761 - val_loss: 2.7908 - val_accuracy: 0.5818\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2298 - accuracy: 0.8809 - val_loss: 2.8502 - val_accuracy: 0.5764\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2429 - accuracy: 0.8783 - val_loss: 2.6709 - val_accuracy: 0.5804\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2432 - accuracy: 0.8782 - val_loss: 2.8210 - val_accuracy: 0.5948\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2391 - accuracy: 0.8761 - val_loss: 2.7324 - val_accuracy: 0.5940\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2307 - accuracy: 0.8803 - val_loss: 2.7858 - val_accuracy: 0.5904\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2155 - accuracy: 0.8856 - val_loss: 2.9290 - val_accuracy: 0.5796\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2207 - accuracy: 0.8844 - val_loss: 3.0652 - val_accuracy: 0.5962\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2369 - accuracy: 0.8805 - val_loss: 2.8856 - val_accuracy: 0.5976\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2378 - accuracy: 0.8785 - val_loss: 2.8345 - val_accuracy: 0.5829\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2437 - accuracy: 0.8750 - val_loss: 2.8136 - val_accuracy: 0.5958\n",
            "History for model 19: <keras.src.callbacks.History object at 0x7e83e7d941c0>\n",
            "Trial 20: Number of layers = 5, Number of neurons per layer = 30\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 5s 8ms/step - loss: 0.8394 - accuracy: 0.5915 - val_loss: 0.7947 - val_accuracy: 0.6199\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7763 - accuracy: 0.6268 - val_loss: 0.7977 - val_accuracy: 0.6224\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7630 - accuracy: 0.6392 - val_loss: 0.7890 - val_accuracy: 0.6250\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7523 - accuracy: 0.6437 - val_loss: 0.7872 - val_accuracy: 0.6242\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7417 - accuracy: 0.6508 - val_loss: 0.7740 - val_accuracy: 0.6350\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7320 - accuracy: 0.6524 - val_loss: 0.7865 - val_accuracy: 0.6311\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7221 - accuracy: 0.6582 - val_loss: 0.7836 - val_accuracy: 0.6289\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7167 - accuracy: 0.6630 - val_loss: 0.7738 - val_accuracy: 0.6411\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.7109 - accuracy: 0.6655 - val_loss: 0.7787 - val_accuracy: 0.6386\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7032 - accuracy: 0.6722 - val_loss: 0.7776 - val_accuracy: 0.6419\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6973 - accuracy: 0.6766 - val_loss: 0.7837 - val_accuracy: 0.6437\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6896 - accuracy: 0.6762 - val_loss: 0.7899 - val_accuracy: 0.6242\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6841 - accuracy: 0.6796 - val_loss: 0.7867 - val_accuracy: 0.6383\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6783 - accuracy: 0.6814 - val_loss: 0.7774 - val_accuracy: 0.6321\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6717 - accuracy: 0.6893 - val_loss: 0.8029 - val_accuracy: 0.6325\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6681 - accuracy: 0.6893 - val_loss: 0.8020 - val_accuracy: 0.6296\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6636 - accuracy: 0.6908 - val_loss: 0.8114 - val_accuracy: 0.6325\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.6564 - accuracy: 0.6935 - val_loss: 0.8373 - val_accuracy: 0.6311\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6541 - accuracy: 0.6972 - val_loss: 0.7997 - val_accuracy: 0.6303\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6483 - accuracy: 0.7006 - val_loss: 0.8089 - val_accuracy: 0.6286\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6416 - accuracy: 0.7038 - val_loss: 0.8224 - val_accuracy: 0.6228\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6390 - accuracy: 0.7000 - val_loss: 0.8276 - val_accuracy: 0.6210\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6350 - accuracy: 0.7043 - val_loss: 0.8121 - val_accuracy: 0.6206\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6318 - accuracy: 0.7068 - val_loss: 0.8423 - val_accuracy: 0.6170\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6268 - accuracy: 0.7095 - val_loss: 0.8248 - val_accuracy: 0.6203\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6217 - accuracy: 0.7138 - val_loss: 0.8575 - val_accuracy: 0.6185\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6191 - accuracy: 0.7140 - val_loss: 0.8461 - val_accuracy: 0.6210\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.6181 - accuracy: 0.7100 - val_loss: 0.8523 - val_accuracy: 0.6239\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6121 - accuracy: 0.7176 - val_loss: 0.8723 - val_accuracy: 0.6138\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6107 - accuracy: 0.7172 - val_loss: 0.8773 - val_accuracy: 0.6224\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6068 - accuracy: 0.7174 - val_loss: 0.8473 - val_accuracy: 0.6282\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6038 - accuracy: 0.7190 - val_loss: 0.8688 - val_accuracy: 0.6196\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5998 - accuracy: 0.7238 - val_loss: 0.8973 - val_accuracy: 0.6214\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6009 - accuracy: 0.7236 - val_loss: 0.8800 - val_accuracy: 0.6196\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5937 - accuracy: 0.7213 - val_loss: 0.8960 - val_accuracy: 0.6271\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5976 - accuracy: 0.7225 - val_loss: 0.8933 - val_accuracy: 0.6311\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5912 - accuracy: 0.7232 - val_loss: 0.8852 - val_accuracy: 0.6170\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5856 - accuracy: 0.7293 - val_loss: 0.8994 - val_accuracy: 0.6206\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5835 - accuracy: 0.7343 - val_loss: 0.9098 - val_accuracy: 0.6120\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5834 - accuracy: 0.7312 - val_loss: 0.9058 - val_accuracy: 0.6185\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5788 - accuracy: 0.7330 - val_loss: 0.9123 - val_accuracy: 0.6296\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5785 - accuracy: 0.7300 - val_loss: 0.9243 - val_accuracy: 0.6199\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5757 - accuracy: 0.7283 - val_loss: 0.9501 - val_accuracy: 0.6199\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5727 - accuracy: 0.7347 - val_loss: 0.9648 - val_accuracy: 0.6091\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5708 - accuracy: 0.7356 - val_loss: 0.9658 - val_accuracy: 0.6242\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5676 - accuracy: 0.7372 - val_loss: 0.9334 - val_accuracy: 0.6264\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5619 - accuracy: 0.7396 - val_loss: 0.9895 - val_accuracy: 0.6149\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5674 - accuracy: 0.7372 - val_loss: 0.9665 - val_accuracy: 0.6185\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5604 - accuracy: 0.7385 - val_loss: 0.9912 - val_accuracy: 0.6239\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5634 - accuracy: 0.7357 - val_loss: 0.9864 - val_accuracy: 0.6152\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5546 - accuracy: 0.7410 - val_loss: 0.9771 - val_accuracy: 0.6142\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5580 - accuracy: 0.7427 - val_loss: 0.9791 - val_accuracy: 0.6246\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5534 - accuracy: 0.7392 - val_loss: 0.9827 - val_accuracy: 0.6127\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5510 - accuracy: 0.7452 - val_loss: 1.0032 - val_accuracy: 0.6174\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5480 - accuracy: 0.7435 - val_loss: 1.0152 - val_accuracy: 0.6134\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5452 - accuracy: 0.7423 - val_loss: 1.0132 - val_accuracy: 0.6127\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5441 - accuracy: 0.7460 - val_loss: 1.0355 - val_accuracy: 0.6174\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5430 - accuracy: 0.7474 - val_loss: 1.0471 - val_accuracy: 0.6228\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5437 - accuracy: 0.7441 - val_loss: 1.0394 - val_accuracy: 0.6106\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5460 - accuracy: 0.7478 - val_loss: 0.9791 - val_accuracy: 0.6134\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5416 - accuracy: 0.7488 - val_loss: 1.0431 - val_accuracy: 0.6127\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5343 - accuracy: 0.7519 - val_loss: 1.0302 - val_accuracy: 0.6257\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5392 - accuracy: 0.7518 - val_loss: 0.9964 - val_accuracy: 0.6145\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5341 - accuracy: 0.7529 - val_loss: 1.0423 - val_accuracy: 0.6142\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5259 - accuracy: 0.7559 - val_loss: 1.0551 - val_accuracy: 0.6088\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5259 - accuracy: 0.7527 - val_loss: 1.0219 - val_accuracy: 0.6167\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5248 - accuracy: 0.7602 - val_loss: 1.0438 - val_accuracy: 0.6127\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5238 - accuracy: 0.7566 - val_loss: 1.0615 - val_accuracy: 0.6217\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5231 - accuracy: 0.7560 - val_loss: 1.1346 - val_accuracy: 0.6217\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5247 - accuracy: 0.7546 - val_loss: 1.0835 - val_accuracy: 0.6120\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5202 - accuracy: 0.7604 - val_loss: 1.0832 - val_accuracy: 0.6160\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5156 - accuracy: 0.7586 - val_loss: 1.0889 - val_accuracy: 0.6192\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5133 - accuracy: 0.7570 - val_loss: 1.0759 - val_accuracy: 0.6099\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5153 - accuracy: 0.7644 - val_loss: 1.0901 - val_accuracy: 0.6188\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5155 - accuracy: 0.7625 - val_loss: 1.0943 - val_accuracy: 0.6131\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5111 - accuracy: 0.7635 - val_loss: 1.1292 - val_accuracy: 0.6199\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5093 - accuracy: 0.7645 - val_loss: 1.1545 - val_accuracy: 0.6081\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5089 - accuracy: 0.7614 - val_loss: 1.1755 - val_accuracy: 0.6117\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5009 - accuracy: 0.7647 - val_loss: 1.1203 - val_accuracy: 0.6034\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5126 - accuracy: 0.7623 - val_loss: 1.1594 - val_accuracy: 0.6070\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5032 - accuracy: 0.7683 - val_loss: 1.1473 - val_accuracy: 0.6099\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4990 - accuracy: 0.7664 - val_loss: 1.2207 - val_accuracy: 0.6066\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4971 - accuracy: 0.7694 - val_loss: 1.1740 - val_accuracy: 0.6066\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5009 - accuracy: 0.7630 - val_loss: 1.1911 - val_accuracy: 0.6081\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4953 - accuracy: 0.7673 - val_loss: 1.1816 - val_accuracy: 0.6099\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4940 - accuracy: 0.7691 - val_loss: 1.1915 - val_accuracy: 0.6023\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4966 - accuracy: 0.7699 - val_loss: 1.1908 - val_accuracy: 0.6152\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4893 - accuracy: 0.7717 - val_loss: 1.2415 - val_accuracy: 0.6134\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4898 - accuracy: 0.7701 - val_loss: 1.2351 - val_accuracy: 0.6145\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4876 - accuracy: 0.7724 - val_loss: 1.2290 - val_accuracy: 0.6063\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4863 - accuracy: 0.7718 - val_loss: 1.2551 - val_accuracy: 0.6217\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4879 - accuracy: 0.7716 - val_loss: 1.2968 - val_accuracy: 0.6063\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4867 - accuracy: 0.7762 - val_loss: 1.2819 - val_accuracy: 0.5948\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4934 - accuracy: 0.7712 - val_loss: 1.2407 - val_accuracy: 0.6185\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4857 - accuracy: 0.7743 - val_loss: 1.2543 - val_accuracy: 0.6088\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4854 - accuracy: 0.7700 - val_loss: 1.3267 - val_accuracy: 0.6142\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4855 - accuracy: 0.7775 - val_loss: 1.1790 - val_accuracy: 0.6063\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4831 - accuracy: 0.7737 - val_loss: 1.2509 - val_accuracy: 0.6192\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4838 - accuracy: 0.7753 - val_loss: 1.2832 - val_accuracy: 0.6099\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4778 - accuracy: 0.7768 - val_loss: 1.2771 - val_accuracy: 0.6117\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4798 - accuracy: 0.7771 - val_loss: 1.2347 - val_accuracy: 0.6091\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4725 - accuracy: 0.7843 - val_loss: 1.2943 - val_accuracy: 0.6120\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4690 - accuracy: 0.7807 - val_loss: 1.3264 - val_accuracy: 0.6095\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4743 - accuracy: 0.7748 - val_loss: 1.3325 - val_accuracy: 0.6113\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4715 - accuracy: 0.7819 - val_loss: 1.2986 - val_accuracy: 0.6081\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4769 - accuracy: 0.7798 - val_loss: 1.2884 - val_accuracy: 0.6034\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4665 - accuracy: 0.7783 - val_loss: 1.3429 - val_accuracy: 0.6102\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4611 - accuracy: 0.7843 - val_loss: 1.3357 - val_accuracy: 0.6045\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4597 - accuracy: 0.7834 - val_loss: 1.3635 - val_accuracy: 0.5991\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4702 - accuracy: 0.7823 - val_loss: 1.3721 - val_accuracy: 0.5998\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4698 - accuracy: 0.7807 - val_loss: 1.3109 - val_accuracy: 0.6203\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4682 - accuracy: 0.7866 - val_loss: 1.3536 - val_accuracy: 0.6027\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4665 - accuracy: 0.7817 - val_loss: 1.4551 - val_accuracy: 0.5940\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4679 - accuracy: 0.7803 - val_loss: 1.3874 - val_accuracy: 0.6034\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4666 - accuracy: 0.7810 - val_loss: 1.3510 - val_accuracy: 0.6030\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4548 - accuracy: 0.7843 - val_loss: 1.3930 - val_accuracy: 0.6084\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4530 - accuracy: 0.7873 - val_loss: 1.3829 - val_accuracy: 0.6045\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4630 - accuracy: 0.7809 - val_loss: 1.3322 - val_accuracy: 0.6120\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4541 - accuracy: 0.7902 - val_loss: 1.3959 - val_accuracy: 0.6059\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4585 - accuracy: 0.7834 - val_loss: 1.2976 - val_accuracy: 0.6073\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4628 - accuracy: 0.7801 - val_loss: 1.4290 - val_accuracy: 0.5983\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4554 - accuracy: 0.7865 - val_loss: 1.3915 - val_accuracy: 0.6059\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4506 - accuracy: 0.7906 - val_loss: 1.4380 - val_accuracy: 0.5944\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4508 - accuracy: 0.7865 - val_loss: 1.4343 - val_accuracy: 0.5987\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4585 - accuracy: 0.7849 - val_loss: 1.4136 - val_accuracy: 0.6109\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4561 - accuracy: 0.7864 - val_loss: 1.3622 - val_accuracy: 0.6099\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4486 - accuracy: 0.7898 - val_loss: 1.3954 - val_accuracy: 0.6091\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4541 - accuracy: 0.7878 - val_loss: 1.3841 - val_accuracy: 0.6088\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4512 - accuracy: 0.7917 - val_loss: 1.4282 - val_accuracy: 0.6127\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4457 - accuracy: 0.7918 - val_loss: 1.4318 - val_accuracy: 0.5983\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4518 - accuracy: 0.7874 - val_loss: 1.4787 - val_accuracy: 0.6034\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4507 - accuracy: 0.7917 - val_loss: 1.4422 - val_accuracy: 0.6048\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4482 - accuracy: 0.7906 - val_loss: 1.4262 - val_accuracy: 0.6066\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4450 - accuracy: 0.7940 - val_loss: 1.4680 - val_accuracy: 0.6073\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4377 - accuracy: 0.7946 - val_loss: 1.5113 - val_accuracy: 0.6081\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4422 - accuracy: 0.7896 - val_loss: 1.4610 - val_accuracy: 0.6030\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4409 - accuracy: 0.7940 - val_loss: 1.4990 - val_accuracy: 0.5994\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4384 - accuracy: 0.7941 - val_loss: 1.5057 - val_accuracy: 0.6066\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4350 - accuracy: 0.7945 - val_loss: 1.4844 - val_accuracy: 0.6059\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4519 - accuracy: 0.7907 - val_loss: 1.4787 - val_accuracy: 0.6001\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4477 - accuracy: 0.7912 - val_loss: 1.4933 - val_accuracy: 0.6034\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4359 - accuracy: 0.7958 - val_loss: 1.5069 - val_accuracy: 0.6113\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4373 - accuracy: 0.7944 - val_loss: 1.5202 - val_accuracy: 0.5998\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4396 - accuracy: 0.7961 - val_loss: 1.6314 - val_accuracy: 0.6084\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4320 - accuracy: 0.7965 - val_loss: 1.5504 - val_accuracy: 0.6005\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4328 - accuracy: 0.7973 - val_loss: 1.5349 - val_accuracy: 0.6070\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4274 - accuracy: 0.7994 - val_loss: 1.4754 - val_accuracy: 0.5958\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4329 - accuracy: 0.7977 - val_loss: 1.6094 - val_accuracy: 0.5987\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4357 - accuracy: 0.7986 - val_loss: 1.6175 - val_accuracy: 0.6037\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4294 - accuracy: 0.7977 - val_loss: 1.6188 - val_accuracy: 0.6016\n",
            "History for model 20: <keras.src.callbacks.History object at 0x7e83e7f9cc70>\n",
            "Trial 21: Number of layers = 5, Number of neurons per layer = 60\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 4s 6ms/step - loss: 0.8296 - accuracy: 0.5953 - val_loss: 0.8034 - val_accuracy: 0.6052\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7752 - accuracy: 0.6277 - val_loss: 0.7897 - val_accuracy: 0.6160\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7601 - accuracy: 0.6373 - val_loss: 0.7948 - val_accuracy: 0.6170\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7470 - accuracy: 0.6438 - val_loss: 0.7781 - val_accuracy: 0.6235\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.7391 - accuracy: 0.6489 - val_loss: 0.7795 - val_accuracy: 0.6311\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7285 - accuracy: 0.6523 - val_loss: 0.7726 - val_accuracy: 0.6343\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7177 - accuracy: 0.6632 - val_loss: 0.7750 - val_accuracy: 0.6386\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7099 - accuracy: 0.6644 - val_loss: 0.7753 - val_accuracy: 0.6347\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7026 - accuracy: 0.6669 - val_loss: 0.7806 - val_accuracy: 0.6293\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6927 - accuracy: 0.6733 - val_loss: 0.7793 - val_accuracy: 0.6397\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6867 - accuracy: 0.6816 - val_loss: 0.7871 - val_accuracy: 0.6361\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6772 - accuracy: 0.6796 - val_loss: 0.7749 - val_accuracy: 0.6375\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6699 - accuracy: 0.6876 - val_loss: 0.7976 - val_accuracy: 0.6221\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6621 - accuracy: 0.6868 - val_loss: 0.7960 - val_accuracy: 0.6339\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.6571 - accuracy: 0.6951 - val_loss: 0.8106 - val_accuracy: 0.6185\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6559 - accuracy: 0.6944 - val_loss: 0.8000 - val_accuracy: 0.6375\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6415 - accuracy: 0.6989 - val_loss: 0.8176 - val_accuracy: 0.6303\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6382 - accuracy: 0.7031 - val_loss: 0.8095 - val_accuracy: 0.6339\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6324 - accuracy: 0.7079 - val_loss: 0.8102 - val_accuracy: 0.6228\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6299 - accuracy: 0.7092 - val_loss: 0.8155 - val_accuracy: 0.6311\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6191 - accuracy: 0.7129 - val_loss: 0.8320 - val_accuracy: 0.6397\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6168 - accuracy: 0.7120 - val_loss: 0.8420 - val_accuracy: 0.6293\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6099 - accuracy: 0.7129 - val_loss: 0.8386 - val_accuracy: 0.6361\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6071 - accuracy: 0.7219 - val_loss: 0.8716 - val_accuracy: 0.6246\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6016 - accuracy: 0.7212 - val_loss: 0.8513 - val_accuracy: 0.6347\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5960 - accuracy: 0.7242 - val_loss: 0.8418 - val_accuracy: 0.6268\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5898 - accuracy: 0.7260 - val_loss: 0.8642 - val_accuracy: 0.6296\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.5831 - accuracy: 0.7229 - val_loss: 0.8649 - val_accuracy: 0.6311\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5799 - accuracy: 0.7303 - val_loss: 0.8599 - val_accuracy: 0.6357\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5770 - accuracy: 0.7320 - val_loss: 0.8723 - val_accuracy: 0.6293\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5711 - accuracy: 0.7331 - val_loss: 0.8871 - val_accuracy: 0.6260\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5617 - accuracy: 0.7385 - val_loss: 0.9039 - val_accuracy: 0.6293\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5571 - accuracy: 0.7444 - val_loss: 0.9191 - val_accuracy: 0.6300\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5575 - accuracy: 0.7401 - val_loss: 0.9105 - val_accuracy: 0.6214\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5492 - accuracy: 0.7437 - val_loss: 0.9048 - val_accuracy: 0.6379\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5488 - accuracy: 0.7459 - val_loss: 0.9287 - val_accuracy: 0.6307\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5390 - accuracy: 0.7489 - val_loss: 0.9432 - val_accuracy: 0.6282\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5365 - accuracy: 0.7501 - val_loss: 0.9727 - val_accuracy: 0.6329\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5411 - accuracy: 0.7486 - val_loss: 0.9396 - val_accuracy: 0.6314\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5310 - accuracy: 0.7549 - val_loss: 0.9143 - val_accuracy: 0.6321\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5253 - accuracy: 0.7560 - val_loss: 0.9627 - val_accuracy: 0.6235\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5239 - accuracy: 0.7570 - val_loss: 0.9722 - val_accuracy: 0.6332\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5220 - accuracy: 0.7552 - val_loss: 0.9677 - val_accuracy: 0.6206\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5167 - accuracy: 0.7630 - val_loss: 1.0035 - val_accuracy: 0.6210\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5081 - accuracy: 0.7655 - val_loss: 1.0268 - val_accuracy: 0.6275\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5148 - accuracy: 0.7567 - val_loss: 1.0046 - val_accuracy: 0.6289\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.5078 - accuracy: 0.7656 - val_loss: 0.9985 - val_accuracy: 0.6181\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4972 - accuracy: 0.7677 - val_loss: 1.0394 - val_accuracy: 0.6174\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5017 - accuracy: 0.7632 - val_loss: 1.0362 - val_accuracy: 0.6170\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4999 - accuracy: 0.7687 - val_loss: 1.0652 - val_accuracy: 0.6232\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4944 - accuracy: 0.7738 - val_loss: 1.0314 - val_accuracy: 0.6214\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4971 - accuracy: 0.7689 - val_loss: 1.0242 - val_accuracy: 0.6196\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4881 - accuracy: 0.7726 - val_loss: 1.0449 - val_accuracy: 0.6167\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4810 - accuracy: 0.7790 - val_loss: 1.0954 - val_accuracy: 0.6206\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4824 - accuracy: 0.7754 - val_loss: 1.0813 - val_accuracy: 0.6149\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4758 - accuracy: 0.7800 - val_loss: 1.1150 - val_accuracy: 0.6142\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4725 - accuracy: 0.7823 - val_loss: 1.1593 - val_accuracy: 0.6196\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4777 - accuracy: 0.7763 - val_loss: 1.1408 - val_accuracy: 0.6142\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4748 - accuracy: 0.7815 - val_loss: 1.1268 - val_accuracy: 0.6088\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4662 - accuracy: 0.7830 - val_loss: 1.0985 - val_accuracy: 0.6268\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4618 - accuracy: 0.7870 - val_loss: 1.1351 - val_accuracy: 0.6145\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4674 - accuracy: 0.7851 - val_loss: 1.1051 - val_accuracy: 0.6152\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4570 - accuracy: 0.7844 - val_loss: 1.1952 - val_accuracy: 0.6206\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4557 - accuracy: 0.7892 - val_loss: 1.1424 - val_accuracy: 0.6196\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4588 - accuracy: 0.7859 - val_loss: 1.1455 - val_accuracy: 0.6077\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4521 - accuracy: 0.7888 - val_loss: 1.2246 - val_accuracy: 0.6178\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4471 - accuracy: 0.7914 - val_loss: 1.2100 - val_accuracy: 0.6181\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4482 - accuracy: 0.7897 - val_loss: 1.1804 - val_accuracy: 0.6170\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4394 - accuracy: 0.7969 - val_loss: 1.1977 - val_accuracy: 0.6081\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4457 - accuracy: 0.7948 - val_loss: 1.2391 - val_accuracy: 0.6109\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4346 - accuracy: 0.7976 - val_loss: 1.2913 - val_accuracy: 0.6077\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4351 - accuracy: 0.7919 - val_loss: 1.2658 - val_accuracy: 0.6134\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4354 - accuracy: 0.7992 - val_loss: 1.2661 - val_accuracy: 0.6120\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4313 - accuracy: 0.7976 - val_loss: 1.2649 - val_accuracy: 0.6174\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4319 - accuracy: 0.7993 - val_loss: 1.2521 - val_accuracy: 0.6113\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4284 - accuracy: 0.7971 - val_loss: 1.3043 - val_accuracy: 0.6188\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.4317 - accuracy: 0.7985 - val_loss: 1.2721 - val_accuracy: 0.6131\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4312 - accuracy: 0.7971 - val_loss: 1.2676 - val_accuracy: 0.6142\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4173 - accuracy: 0.8020 - val_loss: 1.4183 - val_accuracy: 0.6055\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4189 - accuracy: 0.8022 - val_loss: 1.3177 - val_accuracy: 0.6163\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4171 - accuracy: 0.8056 - val_loss: 1.3745 - val_accuracy: 0.6160\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4223 - accuracy: 0.8010 - val_loss: 1.3640 - val_accuracy: 0.6268\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4132 - accuracy: 0.8066 - val_loss: 1.3859 - val_accuracy: 0.6109\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4091 - accuracy: 0.8068 - val_loss: 1.3863 - val_accuracy: 0.6084\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4096 - accuracy: 0.8062 - val_loss: 1.3413 - val_accuracy: 0.6059\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4087 - accuracy: 0.8054 - val_loss: 1.3121 - val_accuracy: 0.6084\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4072 - accuracy: 0.8110 - val_loss: 1.3880 - val_accuracy: 0.6001\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4055 - accuracy: 0.8095 - val_loss: 1.4284 - val_accuracy: 0.6113\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4095 - accuracy: 0.8064 - val_loss: 1.4693 - val_accuracy: 0.6063\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4019 - accuracy: 0.8087 - val_loss: 1.4330 - val_accuracy: 0.6027\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3934 - accuracy: 0.8074 - val_loss: 1.5159 - val_accuracy: 0.6023\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3983 - accuracy: 0.8125 - val_loss: 1.4586 - val_accuracy: 0.6016\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3989 - accuracy: 0.8093 - val_loss: 1.4774 - val_accuracy: 0.6120\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4040 - accuracy: 0.8130 - val_loss: 1.4148 - val_accuracy: 0.6156\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3945 - accuracy: 0.8142 - val_loss: 1.5579 - val_accuracy: 0.6030\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4082 - accuracy: 0.8148 - val_loss: 1.5183 - val_accuracy: 0.6109\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3896 - accuracy: 0.8174 - val_loss: 1.4759 - val_accuracy: 0.5980\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3867 - accuracy: 0.8138 - val_loss: 1.6187 - val_accuracy: 0.6016\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3887 - accuracy: 0.8171 - val_loss: 1.5530 - val_accuracy: 0.6124\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3829 - accuracy: 0.8175 - val_loss: 1.5267 - val_accuracy: 0.6055\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3801 - accuracy: 0.8149 - val_loss: 1.6157 - val_accuracy: 0.6059\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3849 - accuracy: 0.8190 - val_loss: 1.5191 - val_accuracy: 0.6156\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3797 - accuracy: 0.8198 - val_loss: 1.6180 - val_accuracy: 0.6030\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3766 - accuracy: 0.8194 - val_loss: 1.6051 - val_accuracy: 0.6027\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3718 - accuracy: 0.8194 - val_loss: 1.5797 - val_accuracy: 0.5994\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3785 - accuracy: 0.8192 - val_loss: 1.5993 - val_accuracy: 0.6088\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3690 - accuracy: 0.8244 - val_loss: 1.5810 - val_accuracy: 0.6127\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3693 - accuracy: 0.8253 - val_loss: 1.6581 - val_accuracy: 0.5991\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3747 - accuracy: 0.8227 - val_loss: 1.6258 - val_accuracy: 0.6109\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3811 - accuracy: 0.8203 - val_loss: 1.6735 - val_accuracy: 0.5948\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3786 - accuracy: 0.8230 - val_loss: 1.7266 - val_accuracy: 0.6095\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3672 - accuracy: 0.8252 - val_loss: 1.6308 - val_accuracy: 0.5976\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3607 - accuracy: 0.8249 - val_loss: 1.6736 - val_accuracy: 0.6023\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3610 - accuracy: 0.8278 - val_loss: 1.6197 - val_accuracy: 0.6052\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3635 - accuracy: 0.8256 - val_loss: 1.7279 - val_accuracy: 0.5983\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3620 - accuracy: 0.8252 - val_loss: 1.7443 - val_accuracy: 0.5933\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3612 - accuracy: 0.8258 - val_loss: 1.7103 - val_accuracy: 0.5976\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3609 - accuracy: 0.8312 - val_loss: 1.7010 - val_accuracy: 0.6091\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3557 - accuracy: 0.8294 - val_loss: 1.7596 - val_accuracy: 0.6081\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3606 - accuracy: 0.8276 - val_loss: 1.7462 - val_accuracy: 0.5998\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3652 - accuracy: 0.8238 - val_loss: 1.7704 - val_accuracy: 0.6196\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3495 - accuracy: 0.8284 - val_loss: 1.9152 - val_accuracy: 0.6030\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3561 - accuracy: 0.8299 - val_loss: 1.8357 - val_accuracy: 0.5973\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3486 - accuracy: 0.8318 - val_loss: 1.8077 - val_accuracy: 0.6019\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3588 - accuracy: 0.8278 - val_loss: 1.7387 - val_accuracy: 0.5940\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3607 - accuracy: 0.8271 - val_loss: 1.7106 - val_accuracy: 0.5915\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3581 - accuracy: 0.8298 - val_loss: 1.6929 - val_accuracy: 0.6023\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3486 - accuracy: 0.8331 - val_loss: 1.9168 - val_accuracy: 0.6001\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3377 - accuracy: 0.8401 - val_loss: 1.8122 - val_accuracy: 0.5919\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3454 - accuracy: 0.8328 - val_loss: 1.9272 - val_accuracy: 0.6009\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3438 - accuracy: 0.8362 - val_loss: 2.0107 - val_accuracy: 0.6005\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3615 - accuracy: 0.8283 - val_loss: 1.8588 - val_accuracy: 0.5951\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3462 - accuracy: 0.8360 - val_loss: 1.8608 - val_accuracy: 0.5973\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3391 - accuracy: 0.8356 - val_loss: 1.9275 - val_accuracy: 0.6030\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3403 - accuracy: 0.8373 - val_loss: 1.8284 - val_accuracy: 0.6066\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3517 - accuracy: 0.8313 - val_loss: 1.8074 - val_accuracy: 0.6066\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3412 - accuracy: 0.8368 - val_loss: 1.8258 - val_accuracy: 0.5951\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3405 - accuracy: 0.8335 - val_loss: 1.8259 - val_accuracy: 0.5926\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3513 - accuracy: 0.8295 - val_loss: 1.9074 - val_accuracy: 0.5944\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3320 - accuracy: 0.8378 - val_loss: 1.8868 - val_accuracy: 0.5940\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3410 - accuracy: 0.8346 - val_loss: 1.8525 - val_accuracy: 0.6005\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3350 - accuracy: 0.8398 - val_loss: 1.9397 - val_accuracy: 0.5969\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 2s 5ms/step - loss: 0.3306 - accuracy: 0.8410 - val_loss: 1.9050 - val_accuracy: 0.5940\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3333 - accuracy: 0.8372 - val_loss: 1.9361 - val_accuracy: 0.6066\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3344 - accuracy: 0.8360 - val_loss: 1.9336 - val_accuracy: 0.5890\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3436 - accuracy: 0.8384 - val_loss: 1.9476 - val_accuracy: 0.5944\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3272 - accuracy: 0.8381 - val_loss: 1.9941 - val_accuracy: 0.6001\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3268 - accuracy: 0.8405 - val_loss: 1.9689 - val_accuracy: 0.5955\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3330 - accuracy: 0.8391 - val_loss: 2.0015 - val_accuracy: 0.5976\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3374 - accuracy: 0.8377 - val_loss: 1.9147 - val_accuracy: 0.5937\n",
            "History for model 21: <keras.src.callbacks.History object at 0x7e83fa551ea0>\n",
            "Trial 22: Number of layers = 5, Number of neurons per layer = 90\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 4s 8ms/step - loss: 0.8236 - accuracy: 0.6036 - val_loss: 0.7927 - val_accuracy: 0.6235\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7741 - accuracy: 0.6301 - val_loss: 0.7966 - val_accuracy: 0.6192\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7615 - accuracy: 0.6369 - val_loss: 0.7803 - val_accuracy: 0.6257\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7469 - accuracy: 0.6446 - val_loss: 0.7746 - val_accuracy: 0.6357\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7360 - accuracy: 0.6530 - val_loss: 0.7792 - val_accuracy: 0.6339\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7284 - accuracy: 0.6558 - val_loss: 0.7868 - val_accuracy: 0.6224\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7168 - accuracy: 0.6615 - val_loss: 0.7736 - val_accuracy: 0.6343\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7038 - accuracy: 0.6688 - val_loss: 0.7898 - val_accuracy: 0.6361\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6977 - accuracy: 0.6744 - val_loss: 0.7813 - val_accuracy: 0.6311\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6855 - accuracy: 0.6775 - val_loss: 0.7737 - val_accuracy: 0.6368\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6784 - accuracy: 0.6792 - val_loss: 0.7831 - val_accuracy: 0.6314\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6664 - accuracy: 0.6866 - val_loss: 0.7949 - val_accuracy: 0.6296\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6593 - accuracy: 0.6922 - val_loss: 0.7959 - val_accuracy: 0.6336\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6516 - accuracy: 0.6979 - val_loss: 0.7940 - val_accuracy: 0.6437\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6428 - accuracy: 0.6984 - val_loss: 0.8019 - val_accuracy: 0.6339\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6363 - accuracy: 0.7010 - val_loss: 0.8004 - val_accuracy: 0.6408\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6294 - accuracy: 0.7044 - val_loss: 0.8116 - val_accuracy: 0.6379\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6199 - accuracy: 0.7112 - val_loss: 0.8155 - val_accuracy: 0.6365\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.6136 - accuracy: 0.7147 - val_loss: 0.8427 - val_accuracy: 0.6311\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.6070 - accuracy: 0.7148 - val_loss: 0.8362 - val_accuracy: 0.6214\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5998 - accuracy: 0.7182 - val_loss: 0.8379 - val_accuracy: 0.6332\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5923 - accuracy: 0.7212 - val_loss: 0.8771 - val_accuracy: 0.6221\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5841 - accuracy: 0.7271 - val_loss: 0.9080 - val_accuracy: 0.6365\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5827 - accuracy: 0.7295 - val_loss: 0.8780 - val_accuracy: 0.6332\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5745 - accuracy: 0.7300 - val_loss: 0.8789 - val_accuracy: 0.6350\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5688 - accuracy: 0.7345 - val_loss: 0.9376 - val_accuracy: 0.6289\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5605 - accuracy: 0.7366 - val_loss: 0.9621 - val_accuracy: 0.6217\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5541 - accuracy: 0.7405 - val_loss: 0.9802 - val_accuracy: 0.6264\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5485 - accuracy: 0.7437 - val_loss: 0.9401 - val_accuracy: 0.6311\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5474 - accuracy: 0.7446 - val_loss: 0.9570 - val_accuracy: 0.6250\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5398 - accuracy: 0.7465 - val_loss: 1.0012 - val_accuracy: 0.6318\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5333 - accuracy: 0.7491 - val_loss: 1.0014 - val_accuracy: 0.6203\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5280 - accuracy: 0.7560 - val_loss: 1.0379 - val_accuracy: 0.6185\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.5219 - accuracy: 0.7555 - val_loss: 0.9956 - val_accuracy: 0.6156\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5209 - accuracy: 0.7585 - val_loss: 1.0677 - val_accuracy: 0.6174\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.5105 - accuracy: 0.7574 - val_loss: 1.0973 - val_accuracy: 0.6232\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5087 - accuracy: 0.7602 - val_loss: 1.0230 - val_accuracy: 0.6163\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5060 - accuracy: 0.7627 - val_loss: 1.0719 - val_accuracy: 0.6246\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.5090 - accuracy: 0.7590 - val_loss: 1.1232 - val_accuracy: 0.6113\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4983 - accuracy: 0.7698 - val_loss: 1.1191 - val_accuracy: 0.6149\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4882 - accuracy: 0.7667 - val_loss: 1.1943 - val_accuracy: 0.6102\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4869 - accuracy: 0.7725 - val_loss: 1.1364 - val_accuracy: 0.6178\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4823 - accuracy: 0.7717 - val_loss: 1.1189 - val_accuracy: 0.6145\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4812 - accuracy: 0.7729 - val_loss: 1.1067 - val_accuracy: 0.6117\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4785 - accuracy: 0.7762 - val_loss: 1.2457 - val_accuracy: 0.6113\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4626 - accuracy: 0.7789 - val_loss: 1.2452 - val_accuracy: 0.6206\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4731 - accuracy: 0.7782 - val_loss: 1.2477 - val_accuracy: 0.6199\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4592 - accuracy: 0.7840 - val_loss: 1.2889 - val_accuracy: 0.6127\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4626 - accuracy: 0.7818 - val_loss: 1.2457 - val_accuracy: 0.6081\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4508 - accuracy: 0.7843 - val_loss: 1.3600 - val_accuracy: 0.6106\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4513 - accuracy: 0.7860 - val_loss: 1.3019 - val_accuracy: 0.6066\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4445 - accuracy: 0.7886 - val_loss: 1.3581 - val_accuracy: 0.6138\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4403 - accuracy: 0.7861 - val_loss: 1.3682 - val_accuracy: 0.5969\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4426 - accuracy: 0.7885 - val_loss: 1.4206 - val_accuracy: 0.5998\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4358 - accuracy: 0.7899 - val_loss: 1.4794 - val_accuracy: 0.6088\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4404 - accuracy: 0.7924 - val_loss: 1.3652 - val_accuracy: 0.6120\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4351 - accuracy: 0.7918 - val_loss: 1.3830 - val_accuracy: 0.6106\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4361 - accuracy: 0.7934 - val_loss: 1.4340 - val_accuracy: 0.6084\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4150 - accuracy: 0.7979 - val_loss: 1.4222 - val_accuracy: 0.6152\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4195 - accuracy: 0.7983 - val_loss: 1.4213 - val_accuracy: 0.6070\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4213 - accuracy: 0.7967 - val_loss: 1.5059 - val_accuracy: 0.6073\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4166 - accuracy: 0.8011 - val_loss: 1.4738 - val_accuracy: 0.6124\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4106 - accuracy: 0.8039 - val_loss: 1.3936 - val_accuracy: 0.6041\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4028 - accuracy: 0.8080 - val_loss: 1.5507 - val_accuracy: 0.6055\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4096 - accuracy: 0.8031 - val_loss: 1.5105 - val_accuracy: 0.6009\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4033 - accuracy: 0.8039 - val_loss: 1.6971 - val_accuracy: 0.5969\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3960 - accuracy: 0.8061 - val_loss: 1.6415 - val_accuracy: 0.6052\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3969 - accuracy: 0.8125 - val_loss: 1.5892 - val_accuracy: 0.6088\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4019 - accuracy: 0.8052 - val_loss: 1.5777 - val_accuracy: 0.6045\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3942 - accuracy: 0.8116 - val_loss: 1.6593 - val_accuracy: 0.6034\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3960 - accuracy: 0.8106 - val_loss: 1.5830 - val_accuracy: 0.6131\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3826 - accuracy: 0.8113 - val_loss: 1.7265 - val_accuracy: 0.6063\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3994 - accuracy: 0.8094 - val_loss: 1.6685 - val_accuracy: 0.6045\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3836 - accuracy: 0.8136 - val_loss: 1.7447 - val_accuracy: 0.6023\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3808 - accuracy: 0.8132 - val_loss: 1.6193 - val_accuracy: 0.5948\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3754 - accuracy: 0.8207 - val_loss: 1.7314 - val_accuracy: 0.5991\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3858 - accuracy: 0.8131 - val_loss: 1.7090 - val_accuracy: 0.5829\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3761 - accuracy: 0.8154 - val_loss: 1.8638 - val_accuracy: 0.6009\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3723 - accuracy: 0.8195 - val_loss: 1.7669 - val_accuracy: 0.5933\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3808 - accuracy: 0.8176 - val_loss: 1.7536 - val_accuracy: 0.6088\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3784 - accuracy: 0.8189 - val_loss: 1.6120 - val_accuracy: 0.5980\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3655 - accuracy: 0.8225 - val_loss: 1.8994 - val_accuracy: 0.6037\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3645 - accuracy: 0.8269 - val_loss: 1.8747 - val_accuracy: 0.5987\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3647 - accuracy: 0.8223 - val_loss: 1.8777 - val_accuracy: 0.6034\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3665 - accuracy: 0.8224 - val_loss: 1.7854 - val_accuracy: 0.5976\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3483 - accuracy: 0.8308 - val_loss: 1.8002 - val_accuracy: 0.6012\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3548 - accuracy: 0.8286 - val_loss: 1.9458 - val_accuracy: 0.5933\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3720 - accuracy: 0.8205 - val_loss: 1.8291 - val_accuracy: 0.6019\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3639 - accuracy: 0.8209 - val_loss: 1.9233 - val_accuracy: 0.6099\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3462 - accuracy: 0.8296 - val_loss: 1.9406 - val_accuracy: 0.6012\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3485 - accuracy: 0.8288 - val_loss: 2.0046 - val_accuracy: 0.5912\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3463 - accuracy: 0.8294 - val_loss: 2.0158 - val_accuracy: 0.6027\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3523 - accuracy: 0.8250 - val_loss: 1.8927 - val_accuracy: 0.6045\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3372 - accuracy: 0.8328 - val_loss: 1.9715 - val_accuracy: 0.5965\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3397 - accuracy: 0.8330 - val_loss: 2.1155 - val_accuracy: 0.5901\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3528 - accuracy: 0.8289 - val_loss: 2.0036 - val_accuracy: 0.6045\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3435 - accuracy: 0.8328 - val_loss: 1.9263 - val_accuracy: 0.5994\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3203 - accuracy: 0.8389 - val_loss: 2.1577 - val_accuracy: 0.5969\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3380 - accuracy: 0.8361 - val_loss: 2.1220 - val_accuracy: 0.5958\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3337 - accuracy: 0.8374 - val_loss: 2.1159 - val_accuracy: 0.5965\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3405 - accuracy: 0.8332 - val_loss: 2.1204 - val_accuracy: 0.5951\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3332 - accuracy: 0.8332 - val_loss: 2.2108 - val_accuracy: 0.5912\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3377 - accuracy: 0.8364 - val_loss: 2.0192 - val_accuracy: 0.5983\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3243 - accuracy: 0.8378 - val_loss: 2.2367 - val_accuracy: 0.6030\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3204 - accuracy: 0.8403 - val_loss: 2.3303 - val_accuracy: 0.5915\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3323 - accuracy: 0.8372 - val_loss: 2.1378 - val_accuracy: 0.5976\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3191 - accuracy: 0.8401 - val_loss: 2.1674 - val_accuracy: 0.5940\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3355 - accuracy: 0.8393 - val_loss: 2.0787 - val_accuracy: 0.5937\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3297 - accuracy: 0.8404 - val_loss: 2.1070 - val_accuracy: 0.5955\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3276 - accuracy: 0.8389 - val_loss: 2.2876 - val_accuracy: 0.5930\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3188 - accuracy: 0.8436 - val_loss: 2.3982 - val_accuracy: 0.5930\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3250 - accuracy: 0.8385 - val_loss: 2.1420 - val_accuracy: 0.5965\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3146 - accuracy: 0.8422 - val_loss: 2.3304 - val_accuracy: 0.5969\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3139 - accuracy: 0.8421 - val_loss: 2.4231 - val_accuracy: 0.5930\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3234 - accuracy: 0.8421 - val_loss: 2.2306 - val_accuracy: 0.5886\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3100 - accuracy: 0.8465 - val_loss: 2.3118 - val_accuracy: 0.5793\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3026 - accuracy: 0.8468 - val_loss: 2.3086 - val_accuracy: 0.5832\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3081 - accuracy: 0.8457 - val_loss: 2.4710 - val_accuracy: 0.5858\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3066 - accuracy: 0.8476 - val_loss: 2.4996 - val_accuracy: 0.5980\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3121 - accuracy: 0.8446 - val_loss: 2.3322 - val_accuracy: 0.5962\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3048 - accuracy: 0.8518 - val_loss: 2.4397 - val_accuracy: 0.5919\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3101 - accuracy: 0.8482 - val_loss: 2.2529 - val_accuracy: 0.5904\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3227 - accuracy: 0.8468 - val_loss: 2.1710 - val_accuracy: 0.5890\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3090 - accuracy: 0.8458 - val_loss: 2.3211 - val_accuracy: 0.5955\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2884 - accuracy: 0.8538 - val_loss: 2.5063 - val_accuracy: 0.5847\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3003 - accuracy: 0.8492 - val_loss: 2.3205 - val_accuracy: 0.5778\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2978 - accuracy: 0.8531 - val_loss: 2.5025 - val_accuracy: 0.5930\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2863 - accuracy: 0.8552 - val_loss: 2.5797 - val_accuracy: 0.5980\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2980 - accuracy: 0.8543 - val_loss: 2.3596 - val_accuracy: 0.5965\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3130 - accuracy: 0.8470 - val_loss: 2.3899 - val_accuracy: 0.5897\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2969 - accuracy: 0.8526 - val_loss: 2.4112 - val_accuracy: 0.5976\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2962 - accuracy: 0.8519 - val_loss: 2.4953 - val_accuracy: 0.5894\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2872 - accuracy: 0.8608 - val_loss: 2.4231 - val_accuracy: 0.5850\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3027 - accuracy: 0.8507 - val_loss: 2.5976 - val_accuracy: 0.5930\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2911 - accuracy: 0.8536 - val_loss: 2.4605 - val_accuracy: 0.5811\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2949 - accuracy: 0.8546 - val_loss: 2.4082 - val_accuracy: 0.5944\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3003 - accuracy: 0.8473 - val_loss: 2.5140 - val_accuracy: 0.5876\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2862 - accuracy: 0.8572 - val_loss: 2.4969 - val_accuracy: 0.5872\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2874 - accuracy: 0.8561 - val_loss: 2.4587 - val_accuracy: 0.5768\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2816 - accuracy: 0.8565 - val_loss: 2.6385 - val_accuracy: 0.5868\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2780 - accuracy: 0.8607 - val_loss: 2.4175 - val_accuracy: 0.5865\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2983 - accuracy: 0.8534 - val_loss: 2.3723 - val_accuracy: 0.5883\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2813 - accuracy: 0.8585 - val_loss: 2.5516 - val_accuracy: 0.5854\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2959 - accuracy: 0.8552 - val_loss: 2.5082 - val_accuracy: 0.5915\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2701 - accuracy: 0.8595 - val_loss: 2.4619 - val_accuracy: 0.5843\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2844 - accuracy: 0.8599 - val_loss: 2.4903 - val_accuracy: 0.5807\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2740 - accuracy: 0.8630 - val_loss: 2.3550 - val_accuracy: 0.5814\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2760 - accuracy: 0.8608 - val_loss: 2.5224 - val_accuracy: 0.5796\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2825 - accuracy: 0.8625 - val_loss: 2.5235 - val_accuracy: 0.5814\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2850 - accuracy: 0.8590 - val_loss: 2.4296 - val_accuracy: 0.5840\n",
            "History for model 22: <keras.src.callbacks.History object at 0x7e83e798c1f0>\n",
            "Trial 23: Number of layers = 5, Number of neurons per layer = 120\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 5s 9ms/step - loss: 0.8224 - accuracy: 0.6047 - val_loss: 0.8106 - val_accuracy: 0.6138\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.7768 - accuracy: 0.6294 - val_loss: 0.7917 - val_accuracy: 0.6242\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.7572 - accuracy: 0.6397 - val_loss: 0.7870 - val_accuracy: 0.6275\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.7472 - accuracy: 0.6446 - val_loss: 0.7858 - val_accuracy: 0.6321\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7355 - accuracy: 0.6531 - val_loss: 0.7775 - val_accuracy: 0.6289\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.7260 - accuracy: 0.6580 - val_loss: 0.7950 - val_accuracy: 0.6386\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.7147 - accuracy: 0.6648 - val_loss: 0.7676 - val_accuracy: 0.6372\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7020 - accuracy: 0.6683 - val_loss: 0.7663 - val_accuracy: 0.6404\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.6948 - accuracy: 0.6746 - val_loss: 0.7941 - val_accuracy: 0.6390\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6858 - accuracy: 0.6800 - val_loss: 0.7720 - val_accuracy: 0.6357\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6722 - accuracy: 0.6901 - val_loss: 0.8081 - val_accuracy: 0.6282\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.6643 - accuracy: 0.6917 - val_loss: 0.8074 - val_accuracy: 0.6383\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6567 - accuracy: 0.6898 - val_loss: 0.8032 - val_accuracy: 0.6372\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6485 - accuracy: 0.6967 - val_loss: 0.8041 - val_accuracy: 0.6329\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.6330 - accuracy: 0.7093 - val_loss: 0.8265 - val_accuracy: 0.6347\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6302 - accuracy: 0.7095 - val_loss: 0.8285 - val_accuracy: 0.6375\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6205 - accuracy: 0.7128 - val_loss: 0.8476 - val_accuracy: 0.6289\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.6120 - accuracy: 0.7149 - val_loss: 0.8370 - val_accuracy: 0.6242\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6032 - accuracy: 0.7175 - val_loss: 0.8703 - val_accuracy: 0.6250\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5963 - accuracy: 0.7239 - val_loss: 0.8807 - val_accuracy: 0.6268\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5921 - accuracy: 0.7224 - val_loss: 0.8866 - val_accuracy: 0.6311\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5757 - accuracy: 0.7345 - val_loss: 0.8999 - val_accuracy: 0.6325\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5728 - accuracy: 0.7335 - val_loss: 0.9098 - val_accuracy: 0.6275\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5645 - accuracy: 0.7378 - val_loss: 0.9162 - val_accuracy: 0.6325\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5580 - accuracy: 0.7389 - val_loss: 0.9744 - val_accuracy: 0.6286\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5526 - accuracy: 0.7413 - val_loss: 0.9431 - val_accuracy: 0.6181\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5421 - accuracy: 0.7432 - val_loss: 0.9093 - val_accuracy: 0.6260\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5356 - accuracy: 0.7493 - val_loss: 1.0162 - val_accuracy: 0.6232\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5268 - accuracy: 0.7525 - val_loss: 1.0214 - val_accuracy: 0.6246\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.5243 - accuracy: 0.7556 - val_loss: 1.0436 - val_accuracy: 0.6199\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5164 - accuracy: 0.7574 - val_loss: 1.0011 - val_accuracy: 0.6296\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5181 - accuracy: 0.7590 - val_loss: 0.9954 - val_accuracy: 0.6217\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5079 - accuracy: 0.7617 - val_loss: 1.0667 - val_accuracy: 0.6210\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5049 - accuracy: 0.7607 - val_loss: 1.0933 - val_accuracy: 0.6217\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4946 - accuracy: 0.7660 - val_loss: 1.1278 - val_accuracy: 0.6228\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4856 - accuracy: 0.7669 - val_loss: 1.1294 - val_accuracy: 0.6203\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4943 - accuracy: 0.7700 - val_loss: 1.1104 - val_accuracy: 0.6167\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4776 - accuracy: 0.7738 - val_loss: 1.1693 - val_accuracy: 0.6217\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4840 - accuracy: 0.7695 - val_loss: 1.1175 - val_accuracy: 0.6091\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4722 - accuracy: 0.7792 - val_loss: 1.1380 - val_accuracy: 0.6149\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4649 - accuracy: 0.7789 - val_loss: 1.2337 - val_accuracy: 0.6221\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4606 - accuracy: 0.7800 - val_loss: 1.2191 - val_accuracy: 0.6268\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4563 - accuracy: 0.7832 - val_loss: 1.2114 - val_accuracy: 0.6235\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4518 - accuracy: 0.7851 - val_loss: 1.3436 - val_accuracy: 0.6178\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4521 - accuracy: 0.7826 - val_loss: 1.3238 - val_accuracy: 0.6041\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4449 - accuracy: 0.7892 - val_loss: 1.3134 - val_accuracy: 0.6206\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4429 - accuracy: 0.7869 - val_loss: 1.2799 - val_accuracy: 0.6217\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.4443 - accuracy: 0.7863 - val_loss: 1.4338 - val_accuracy: 0.6260\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4473 - accuracy: 0.7871 - val_loss: 1.3446 - val_accuracy: 0.6185\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4293 - accuracy: 0.7917 - val_loss: 1.4112 - val_accuracy: 0.6203\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4161 - accuracy: 0.7994 - val_loss: 1.4353 - val_accuracy: 0.6156\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4269 - accuracy: 0.7967 - val_loss: 1.3899 - val_accuracy: 0.6059\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4116 - accuracy: 0.8009 - val_loss: 1.5172 - val_accuracy: 0.6145\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4162 - accuracy: 0.8021 - val_loss: 1.3331 - val_accuracy: 0.6203\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4109 - accuracy: 0.8042 - val_loss: 1.5651 - val_accuracy: 0.6081\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4020 - accuracy: 0.8031 - val_loss: 1.5083 - val_accuracy: 0.6214\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4140 - accuracy: 0.8037 - val_loss: 1.5030 - val_accuracy: 0.6170\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4084 - accuracy: 0.8059 - val_loss: 1.4438 - val_accuracy: 0.6127\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4043 - accuracy: 0.8040 - val_loss: 1.5331 - val_accuracy: 0.6052\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4065 - accuracy: 0.8060 - val_loss: 1.5608 - val_accuracy: 0.6117\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3899 - accuracy: 0.8083 - val_loss: 1.5540 - val_accuracy: 0.6134\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3848 - accuracy: 0.8112 - val_loss: 1.5513 - val_accuracy: 0.6106\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3937 - accuracy: 0.8127 - val_loss: 1.6366 - val_accuracy: 0.6138\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3938 - accuracy: 0.8127 - val_loss: 1.6622 - val_accuracy: 0.6019\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3766 - accuracy: 0.8139 - val_loss: 1.7059 - val_accuracy: 0.6019\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3752 - accuracy: 0.8145 - val_loss: 1.7118 - val_accuracy: 0.6106\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3782 - accuracy: 0.8152 - val_loss: 1.8009 - val_accuracy: 0.6059\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3794 - accuracy: 0.8217 - val_loss: 1.7257 - val_accuracy: 0.6163\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3762 - accuracy: 0.8199 - val_loss: 1.6808 - val_accuracy: 0.6066\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3684 - accuracy: 0.8182 - val_loss: 1.7348 - val_accuracy: 0.6106\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3646 - accuracy: 0.8225 - val_loss: 1.7055 - val_accuracy: 0.6019\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3725 - accuracy: 0.8213 - val_loss: 1.9262 - val_accuracy: 0.5894\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3668 - accuracy: 0.8190 - val_loss: 1.8789 - val_accuracy: 0.6163\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3546 - accuracy: 0.8250 - val_loss: 1.7652 - val_accuracy: 0.6034\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3531 - accuracy: 0.8267 - val_loss: 2.0425 - val_accuracy: 0.6055\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3424 - accuracy: 0.8296 - val_loss: 1.9203 - val_accuracy: 0.6120\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3636 - accuracy: 0.8263 - val_loss: 1.9544 - val_accuracy: 0.5940\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3777 - accuracy: 0.8197 - val_loss: 1.8133 - val_accuracy: 0.6073\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3516 - accuracy: 0.8244 - val_loss: 1.9024 - val_accuracy: 0.6037\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3429 - accuracy: 0.8306 - val_loss: 1.9285 - val_accuracy: 0.5919\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3484 - accuracy: 0.8305 - val_loss: 1.9426 - val_accuracy: 0.6012\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3405 - accuracy: 0.8301 - val_loss: 1.8760 - val_accuracy: 0.5958\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3587 - accuracy: 0.8236 - val_loss: 1.8825 - val_accuracy: 0.5944\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3380 - accuracy: 0.8332 - val_loss: 1.9027 - val_accuracy: 0.5973\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3354 - accuracy: 0.8332 - val_loss: 2.0729 - val_accuracy: 0.5908\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3440 - accuracy: 0.8304 - val_loss: 2.0204 - val_accuracy: 0.6073\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3264 - accuracy: 0.8370 - val_loss: 2.1449 - val_accuracy: 0.6063\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3432 - accuracy: 0.8332 - val_loss: 2.0002 - val_accuracy: 0.5958\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3391 - accuracy: 0.8317 - val_loss: 2.0063 - val_accuracy: 0.6009\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3220 - accuracy: 0.8364 - val_loss: 1.9814 - val_accuracy: 0.5969\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3295 - accuracy: 0.8374 - val_loss: 2.0121 - val_accuracy: 0.5983\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3268 - accuracy: 0.8403 - val_loss: 2.0954 - val_accuracy: 0.5987\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3148 - accuracy: 0.8428 - val_loss: 2.1294 - val_accuracy: 0.5951\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3359 - accuracy: 0.8367 - val_loss: 1.9527 - val_accuracy: 0.6084\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3183 - accuracy: 0.8400 - val_loss: 2.3041 - val_accuracy: 0.6091\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3263 - accuracy: 0.8394 - val_loss: 2.0061 - val_accuracy: 0.6063\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3212 - accuracy: 0.8400 - val_loss: 2.0700 - val_accuracy: 0.6117\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3252 - accuracy: 0.8389 - val_loss: 2.0597 - val_accuracy: 0.5955\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3309 - accuracy: 0.8392 - val_loss: 2.0717 - val_accuracy: 0.6142\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3079 - accuracy: 0.8432 - val_loss: 2.1834 - val_accuracy: 0.6045\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3050 - accuracy: 0.8501 - val_loss: 2.2516 - val_accuracy: 0.5980\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3109 - accuracy: 0.8464 - val_loss: 2.2216 - val_accuracy: 0.5998\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3090 - accuracy: 0.8442 - val_loss: 2.2192 - val_accuracy: 0.5976\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3104 - accuracy: 0.8421 - val_loss: 2.2055 - val_accuracy: 0.6023\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3065 - accuracy: 0.8481 - val_loss: 2.3250 - val_accuracy: 0.6037\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3091 - accuracy: 0.8440 - val_loss: 2.1044 - val_accuracy: 0.6037\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2996 - accuracy: 0.8455 - val_loss: 2.3250 - val_accuracy: 0.6055\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3017 - accuracy: 0.8488 - val_loss: 2.2894 - val_accuracy: 0.6070\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.3100 - accuracy: 0.8437 - val_loss: 2.0810 - val_accuracy: 0.6059\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2998 - accuracy: 0.8483 - val_loss: 2.2819 - val_accuracy: 0.6009\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3062 - accuracy: 0.8502 - val_loss: 2.2520 - val_accuracy: 0.5998\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2873 - accuracy: 0.8537 - val_loss: 2.4460 - val_accuracy: 0.5987\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2929 - accuracy: 0.8528 - val_loss: 2.1807 - val_accuracy: 0.5951\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3141 - accuracy: 0.8494 - val_loss: 2.4184 - val_accuracy: 0.6055\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2927 - accuracy: 0.8536 - val_loss: 2.3198 - val_accuracy: 0.5965\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2785 - accuracy: 0.8593 - val_loss: 2.3529 - val_accuracy: 0.5998\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3004 - accuracy: 0.8526 - val_loss: 2.2245 - val_accuracy: 0.6030\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2891 - accuracy: 0.8510 - val_loss: 2.3477 - val_accuracy: 0.5965\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2844 - accuracy: 0.8534 - val_loss: 2.2303 - val_accuracy: 0.6063\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2827 - accuracy: 0.8545 - val_loss: 2.4089 - val_accuracy: 0.6030\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2867 - accuracy: 0.8549 - val_loss: 2.3774 - val_accuracy: 0.6120\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2900 - accuracy: 0.8562 - val_loss: 2.3265 - val_accuracy: 0.6027\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2808 - accuracy: 0.8554 - val_loss: 2.5291 - val_accuracy: 0.6048\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2802 - accuracy: 0.8581 - val_loss: 2.3520 - val_accuracy: 0.6016\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2888 - accuracy: 0.8562 - val_loss: 2.2673 - val_accuracy: 0.5969\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2789 - accuracy: 0.8559 - val_loss: 2.4427 - val_accuracy: 0.5998\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2717 - accuracy: 0.8583 - val_loss: 2.6146 - val_accuracy: 0.5983\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3018 - accuracy: 0.8533 - val_loss: 2.5145 - val_accuracy: 0.6016\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2805 - accuracy: 0.8611 - val_loss: 2.5059 - val_accuracy: 0.5969\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2745 - accuracy: 0.8635 - val_loss: 2.6003 - val_accuracy: 0.5998\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2657 - accuracy: 0.8608 - val_loss: 2.6258 - val_accuracy: 0.5965\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2711 - accuracy: 0.8598 - val_loss: 2.6702 - val_accuracy: 0.5926\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2763 - accuracy: 0.8580 - val_loss: 2.5396 - val_accuracy: 0.6059\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2717 - accuracy: 0.8624 - val_loss: 2.5916 - val_accuracy: 0.6077\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2830 - accuracy: 0.8617 - val_loss: 2.5611 - val_accuracy: 0.5908\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2728 - accuracy: 0.8634 - val_loss: 2.5223 - val_accuracy: 0.5876\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2669 - accuracy: 0.8643 - val_loss: 2.6246 - val_accuracy: 0.5894\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2612 - accuracy: 0.8626 - val_loss: 2.4767 - val_accuracy: 0.6055\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2674 - accuracy: 0.8623 - val_loss: 2.7446 - val_accuracy: 0.6055\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2633 - accuracy: 0.8644 - val_loss: 2.9048 - val_accuracy: 0.6034\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2716 - accuracy: 0.8602 - val_loss: 2.5623 - val_accuracy: 0.5937\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2871 - accuracy: 0.8584 - val_loss: 2.4844 - val_accuracy: 0.5983\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2679 - accuracy: 0.8622 - val_loss: 2.7313 - val_accuracy: 0.6012\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2496 - accuracy: 0.8695 - val_loss: 2.9061 - val_accuracy: 0.6012\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2630 - accuracy: 0.8668 - val_loss: 2.6929 - val_accuracy: 0.6081\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 6ms/step - loss: 0.2573 - accuracy: 0.8654 - val_loss: 2.7394 - val_accuracy: 0.6019\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2565 - accuracy: 0.8684 - val_loss: 2.8008 - val_accuracy: 0.5940\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2671 - accuracy: 0.8628 - val_loss: 2.5968 - val_accuracy: 0.5919\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2529 - accuracy: 0.8705 - val_loss: 2.8126 - val_accuracy: 0.6009\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2373 - accuracy: 0.8747 - val_loss: 2.9767 - val_accuracy: 0.5890\n",
            "History for model 23: <keras.src.callbacks.History object at 0x7e83de8cab60>\n",
            "Trial 24: Number of layers = 5, Number of neurons per layer = 150\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 5s 8ms/step - loss: 0.8193 - accuracy: 0.6011 - val_loss: 0.7860 - val_accuracy: 0.6264\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.7740 - accuracy: 0.6321 - val_loss: 0.8018 - val_accuracy: 0.6246\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.7610 - accuracy: 0.6390 - val_loss: 0.7823 - val_accuracy: 0.6300\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.7449 - accuracy: 0.6476 - val_loss: 0.7850 - val_accuracy: 0.6239\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.7339 - accuracy: 0.6534 - val_loss: 0.7695 - val_accuracy: 0.6339\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.7211 - accuracy: 0.6582 - val_loss: 0.7956 - val_accuracy: 0.6318\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.7105 - accuracy: 0.6624 - val_loss: 0.8006 - val_accuracy: 0.6260\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.7032 - accuracy: 0.6687 - val_loss: 0.7856 - val_accuracy: 0.6379\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6887 - accuracy: 0.6769 - val_loss: 0.7973 - val_accuracy: 0.6343\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6794 - accuracy: 0.6797 - val_loss: 0.7887 - val_accuracy: 0.6379\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6711 - accuracy: 0.6871 - val_loss: 0.8338 - val_accuracy: 0.6286\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.6624 - accuracy: 0.6878 - val_loss: 0.8027 - val_accuracy: 0.6303\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6517 - accuracy: 0.6962 - val_loss: 0.7908 - val_accuracy: 0.6242\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6423 - accuracy: 0.7025 - val_loss: 0.8424 - val_accuracy: 0.6257\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.6327 - accuracy: 0.7046 - val_loss: 0.8327 - val_accuracy: 0.6429\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.6215 - accuracy: 0.7101 - val_loss: 0.8702 - val_accuracy: 0.6185\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6177 - accuracy: 0.7102 - val_loss: 0.8542 - val_accuracy: 0.6232\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.6046 - accuracy: 0.7176 - val_loss: 0.8951 - val_accuracy: 0.6235\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.5988 - accuracy: 0.7209 - val_loss: 0.8667 - val_accuracy: 0.6300\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.5898 - accuracy: 0.7236 - val_loss: 0.8660 - val_accuracy: 0.6224\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.5789 - accuracy: 0.7277 - val_loss: 0.9475 - val_accuracy: 0.6289\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5719 - accuracy: 0.7339 - val_loss: 0.8954 - val_accuracy: 0.6228\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5678 - accuracy: 0.7370 - val_loss: 0.9067 - val_accuracy: 0.6260\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.5533 - accuracy: 0.7381 - val_loss: 0.9378 - val_accuracy: 0.6188\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.5444 - accuracy: 0.7429 - val_loss: 0.9853 - val_accuracy: 0.6185\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.5381 - accuracy: 0.7502 - val_loss: 1.0463 - val_accuracy: 0.6088\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5319 - accuracy: 0.7502 - val_loss: 1.0132 - val_accuracy: 0.6264\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5187 - accuracy: 0.7556 - val_loss: 1.0981 - val_accuracy: 0.6131\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.5174 - accuracy: 0.7587 - val_loss: 1.0306 - val_accuracy: 0.6185\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5128 - accuracy: 0.7599 - val_loss: 1.0462 - val_accuracy: 0.6232\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5006 - accuracy: 0.7622 - val_loss: 1.1351 - val_accuracy: 0.6124\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4991 - accuracy: 0.7644 - val_loss: 1.1610 - val_accuracy: 0.6217\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4982 - accuracy: 0.7683 - val_loss: 1.1358 - val_accuracy: 0.6131\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4874 - accuracy: 0.7675 - val_loss: 1.1943 - val_accuracy: 0.6138\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4773 - accuracy: 0.7724 - val_loss: 1.2114 - val_accuracy: 0.6138\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4716 - accuracy: 0.7768 - val_loss: 1.2051 - val_accuracy: 0.6185\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.4694 - accuracy: 0.7799 - val_loss: 1.1659 - val_accuracy: 0.6250\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4581 - accuracy: 0.7839 - val_loss: 1.2934 - val_accuracy: 0.6091\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4509 - accuracy: 0.7871 - val_loss: 1.3309 - val_accuracy: 0.6109\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4471 - accuracy: 0.7862 - val_loss: 1.2267 - val_accuracy: 0.6091\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4417 - accuracy: 0.7880 - val_loss: 1.3249 - val_accuracy: 0.6167\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4348 - accuracy: 0.7940 - val_loss: 1.3827 - val_accuracy: 0.6041\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.4353 - accuracy: 0.7929 - val_loss: 1.3697 - val_accuracy: 0.6142\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4280 - accuracy: 0.7960 - val_loss: 1.4286 - val_accuracy: 0.6095\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4201 - accuracy: 0.7968 - val_loss: 1.5003 - val_accuracy: 0.6124\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4306 - accuracy: 0.7943 - val_loss: 1.4452 - val_accuracy: 0.6250\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4164 - accuracy: 0.8030 - val_loss: 1.4912 - val_accuracy: 0.6109\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4163 - accuracy: 0.8012 - val_loss: 1.4717 - val_accuracy: 0.6120\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.4048 - accuracy: 0.8047 - val_loss: 1.4567 - val_accuracy: 0.6131\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4033 - accuracy: 0.8061 - val_loss: 1.6045 - val_accuracy: 0.6167\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4008 - accuracy: 0.8046 - val_loss: 1.5512 - val_accuracy: 0.5858\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4055 - accuracy: 0.8083 - val_loss: 1.5157 - val_accuracy: 0.6070\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.4046 - accuracy: 0.8083 - val_loss: 1.4588 - val_accuracy: 0.6059\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3790 - accuracy: 0.8200 - val_loss: 1.6246 - val_accuracy: 0.5980\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3940 - accuracy: 0.8107 - val_loss: 1.6337 - val_accuracy: 0.6059\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3734 - accuracy: 0.8194 - val_loss: 1.7215 - val_accuracy: 0.5991\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3687 - accuracy: 0.8195 - val_loss: 1.7643 - val_accuracy: 0.6005\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3704 - accuracy: 0.8185 - val_loss: 1.7412 - val_accuracy: 0.5958\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3687 - accuracy: 0.8190 - val_loss: 1.7309 - val_accuracy: 0.5926\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3862 - accuracy: 0.8145 - val_loss: 1.6381 - val_accuracy: 0.6030\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3630 - accuracy: 0.8239 - val_loss: 1.7428 - val_accuracy: 0.5991\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3679 - accuracy: 0.8237 - val_loss: 1.6528 - val_accuracy: 0.5933\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3600 - accuracy: 0.8248 - val_loss: 1.7852 - val_accuracy: 0.5991\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3564 - accuracy: 0.8268 - val_loss: 1.7937 - val_accuracy: 0.5969\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3481 - accuracy: 0.8291 - val_loss: 1.9394 - val_accuracy: 0.6138\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3473 - accuracy: 0.8277 - val_loss: 1.8558 - val_accuracy: 0.5987\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3442 - accuracy: 0.8314 - val_loss: 1.9386 - val_accuracy: 0.5886\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3423 - accuracy: 0.8339 - val_loss: 1.9309 - val_accuracy: 0.6041\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3509 - accuracy: 0.8252 - val_loss: 1.8788 - val_accuracy: 0.6045\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3418 - accuracy: 0.8305 - val_loss: 1.9826 - val_accuracy: 0.6073\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3413 - accuracy: 0.8293 - val_loss: 1.7847 - val_accuracy: 0.5991\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3412 - accuracy: 0.8319 - val_loss: 1.8526 - val_accuracy: 0.5962\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3379 - accuracy: 0.8330 - val_loss: 1.9716 - val_accuracy: 0.5991\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3374 - accuracy: 0.8376 - val_loss: 2.0093 - val_accuracy: 0.6001\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3313 - accuracy: 0.8367 - val_loss: 1.9111 - val_accuracy: 0.5940\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3205 - accuracy: 0.8415 - val_loss: 1.8335 - val_accuracy: 0.5908\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3244 - accuracy: 0.8411 - val_loss: 2.1522 - val_accuracy: 0.5886\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3210 - accuracy: 0.8411 - val_loss: 2.0146 - val_accuracy: 0.5983\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3217 - accuracy: 0.8397 - val_loss: 2.0830 - val_accuracy: 0.6027\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3217 - accuracy: 0.8382 - val_loss: 1.9499 - val_accuracy: 0.6059\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3291 - accuracy: 0.8359 - val_loss: 2.0202 - val_accuracy: 0.6052\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3193 - accuracy: 0.8411 - val_loss: 2.0354 - val_accuracy: 0.5998\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.3003 - accuracy: 0.8496 - val_loss: 2.0627 - val_accuracy: 0.5944\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2971 - accuracy: 0.8480 - val_loss: 2.3420 - val_accuracy: 0.6012\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3050 - accuracy: 0.8470 - val_loss: 2.1443 - val_accuracy: 0.5973\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.3109 - accuracy: 0.8474 - val_loss: 2.2400 - val_accuracy: 0.5901\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3079 - accuracy: 0.8457 - val_loss: 2.1939 - val_accuracy: 0.5897\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2988 - accuracy: 0.8473 - val_loss: 2.2775 - val_accuracy: 0.5955\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3101 - accuracy: 0.8438 - val_loss: 2.0377 - val_accuracy: 0.6063\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.3022 - accuracy: 0.8495 - val_loss: 2.1742 - val_accuracy: 0.5886\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3040 - accuracy: 0.8491 - val_loss: 2.0635 - val_accuracy: 0.5976\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2937 - accuracy: 0.8514 - val_loss: 2.1848 - val_accuracy: 0.5976\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2925 - accuracy: 0.8509 - val_loss: 2.2227 - val_accuracy: 0.5940\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2872 - accuracy: 0.8527 - val_loss: 2.3164 - val_accuracy: 0.5998\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3049 - accuracy: 0.8504 - val_loss: 2.1300 - val_accuracy: 0.5894\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2817 - accuracy: 0.8526 - val_loss: 2.4993 - val_accuracy: 0.5958\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3001 - accuracy: 0.8506 - val_loss: 2.2306 - val_accuracy: 0.5944\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2949 - accuracy: 0.8544 - val_loss: 2.1637 - val_accuracy: 0.5890\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2781 - accuracy: 0.8560 - val_loss: 2.6005 - val_accuracy: 0.5994\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2808 - accuracy: 0.8564 - val_loss: 2.2441 - val_accuracy: 0.5991\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2909 - accuracy: 0.8569 - val_loss: 2.1469 - val_accuracy: 0.5886\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2792 - accuracy: 0.8581 - val_loss: 2.3483 - val_accuracy: 0.6016\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2740 - accuracy: 0.8559 - val_loss: 2.4428 - val_accuracy: 0.5948\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2737 - accuracy: 0.8606 - val_loss: 2.3553 - val_accuracy: 0.5883\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2767 - accuracy: 0.8574 - val_loss: 2.3430 - val_accuracy: 0.5922\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2815 - accuracy: 0.8560 - val_loss: 2.2418 - val_accuracy: 0.6012\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2729 - accuracy: 0.8627 - val_loss: 2.4297 - val_accuracy: 0.5883\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2765 - accuracy: 0.8577 - val_loss: 2.3879 - val_accuracy: 0.5944\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2685 - accuracy: 0.8622 - val_loss: 2.4932 - val_accuracy: 0.5969\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2725 - accuracy: 0.8613 - val_loss: 2.4700 - val_accuracy: 0.5919\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2798 - accuracy: 0.8578 - val_loss: 2.4161 - val_accuracy: 0.5865\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2770 - accuracy: 0.8589 - val_loss: 2.3869 - val_accuracy: 0.5832\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2574 - accuracy: 0.8660 - val_loss: 2.4148 - val_accuracy: 0.5912\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2622 - accuracy: 0.8669 - val_loss: 2.4895 - val_accuracy: 0.5908\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2567 - accuracy: 0.8667 - val_loss: 2.5675 - val_accuracy: 0.5858\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2756 - accuracy: 0.8657 - val_loss: 2.4546 - val_accuracy: 0.5940\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2574 - accuracy: 0.8710 - val_loss: 2.2458 - val_accuracy: 0.5840\n",
            "Epoch 118/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2665 - accuracy: 0.8635 - val_loss: 2.1926 - val_accuracy: 0.5973\n",
            "Epoch 119/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2730 - accuracy: 0.8632 - val_loss: 2.3012 - val_accuracy: 0.5965\n",
            "Epoch 120/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2439 - accuracy: 0.8706 - val_loss: 2.6925 - val_accuracy: 0.5897\n",
            "Epoch 121/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2540 - accuracy: 0.8702 - val_loss: 2.6331 - val_accuracy: 0.5904\n",
            "Epoch 122/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2539 - accuracy: 0.8653 - val_loss: 2.4969 - val_accuracy: 0.5922\n",
            "Epoch 123/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2514 - accuracy: 0.8665 - val_loss: 2.7355 - val_accuracy: 0.5847\n",
            "Epoch 124/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2662 - accuracy: 0.8658 - val_loss: 2.3538 - val_accuracy: 0.5915\n",
            "Epoch 125/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2612 - accuracy: 0.8691 - val_loss: 2.6134 - val_accuracy: 0.5937\n",
            "Epoch 126/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2669 - accuracy: 0.8669 - val_loss: 2.4462 - val_accuracy: 0.5818\n",
            "Epoch 127/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2600 - accuracy: 0.8670 - val_loss: 2.5158 - val_accuracy: 0.5854\n",
            "Epoch 128/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2434 - accuracy: 0.8730 - val_loss: 2.7113 - val_accuracy: 0.5955\n",
            "Epoch 129/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2488 - accuracy: 0.8711 - val_loss: 2.5440 - val_accuracy: 0.5894\n",
            "Epoch 130/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2498 - accuracy: 0.8720 - val_loss: 2.6510 - val_accuracy: 0.5854\n",
            "Epoch 131/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2500 - accuracy: 0.8697 - val_loss: 2.5654 - val_accuracy: 0.5840\n",
            "Epoch 132/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2387 - accuracy: 0.8754 - val_loss: 2.7568 - val_accuracy: 0.5843\n",
            "Epoch 133/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2434 - accuracy: 0.8745 - val_loss: 2.5625 - val_accuracy: 0.5904\n",
            "Epoch 134/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2595 - accuracy: 0.8690 - val_loss: 2.5203 - val_accuracy: 0.5800\n",
            "Epoch 135/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2486 - accuracy: 0.8745 - val_loss: 2.6922 - val_accuracy: 0.5976\n",
            "Epoch 136/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2371 - accuracy: 0.8769 - val_loss: 2.7311 - val_accuracy: 0.5840\n",
            "Epoch 137/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2479 - accuracy: 0.8718 - val_loss: 2.6354 - val_accuracy: 0.5858\n",
            "Epoch 138/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2400 - accuracy: 0.8737 - val_loss: 2.6750 - val_accuracy: 0.5836\n",
            "Epoch 139/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2350 - accuracy: 0.8759 - val_loss: 2.6729 - val_accuracy: 0.5843\n",
            "Epoch 140/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2467 - accuracy: 0.8688 - val_loss: 2.5782 - val_accuracy: 0.5937\n",
            "Epoch 141/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2463 - accuracy: 0.8710 - val_loss: 2.8096 - val_accuracy: 0.5951\n",
            "Epoch 142/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2389 - accuracy: 0.8740 - val_loss: 2.8418 - val_accuracy: 0.5922\n",
            "Epoch 143/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2475 - accuracy: 0.8733 - val_loss: 2.7010 - val_accuracy: 0.5937\n",
            "Epoch 144/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2469 - accuracy: 0.8759 - val_loss: 2.8099 - val_accuracy: 0.5944\n",
            "Epoch 145/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2479 - accuracy: 0.8711 - val_loss: 2.7212 - val_accuracy: 0.5908\n",
            "Epoch 146/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2372 - accuracy: 0.8756 - val_loss: 2.8028 - val_accuracy: 0.5829\n",
            "Epoch 147/150\n",
            "348/348 [==============================] - 2s 7ms/step - loss: 0.2271 - accuracy: 0.8773 - val_loss: 2.7646 - val_accuracy: 0.5807\n",
            "Epoch 148/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2420 - accuracy: 0.8769 - val_loss: 2.7507 - val_accuracy: 0.5840\n",
            "Epoch 149/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2206 - accuracy: 0.8797 - val_loss: 2.9974 - val_accuracy: 0.5876\n",
            "Epoch 150/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2377 - accuracy: 0.8773 - val_loss: 2.5442 - val_accuracy: 0.5789\n",
            "History for model 24: <keras.src.callbacks.History object at 0x7e83de705f90>\n",
            "Trial 25: Number of layers = 5, Number of neurons per layer = 180\n",
            "Epoch 1/150\n",
            "348/348 [==============================] - 6s 10ms/step - loss: 0.8227 - accuracy: 0.5985 - val_loss: 0.8049 - val_accuracy: 0.6282\n",
            "Epoch 2/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.7756 - accuracy: 0.6287 - val_loss: 0.7891 - val_accuracy: 0.6221\n",
            "Epoch 3/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.7588 - accuracy: 0.6380 - val_loss: 0.7829 - val_accuracy: 0.6347\n",
            "Epoch 4/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.7468 - accuracy: 0.6453 - val_loss: 0.8001 - val_accuracy: 0.6253\n",
            "Epoch 5/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.7356 - accuracy: 0.6479 - val_loss: 0.7741 - val_accuracy: 0.6257\n",
            "Epoch 6/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.7276 - accuracy: 0.6536 - val_loss: 0.7902 - val_accuracy: 0.6336\n",
            "Epoch 7/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.7132 - accuracy: 0.6549 - val_loss: 0.7819 - val_accuracy: 0.6404\n",
            "Epoch 8/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.7046 - accuracy: 0.6627 - val_loss: 0.7763 - val_accuracy: 0.6354\n",
            "Epoch 9/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6937 - accuracy: 0.6715 - val_loss: 0.7921 - val_accuracy: 0.6300\n",
            "Epoch 10/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6858 - accuracy: 0.6787 - val_loss: 0.8037 - val_accuracy: 0.6318\n",
            "Epoch 11/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6720 - accuracy: 0.6852 - val_loss: 0.8035 - val_accuracy: 0.6397\n",
            "Epoch 12/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6666 - accuracy: 0.6842 - val_loss: 0.7951 - val_accuracy: 0.6332\n",
            "Epoch 13/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6538 - accuracy: 0.6908 - val_loss: 0.7941 - val_accuracy: 0.6286\n",
            "Epoch 14/150\n",
            "348/348 [==============================] - 3s 10ms/step - loss: 0.6414 - accuracy: 0.6954 - val_loss: 0.8115 - val_accuracy: 0.6286\n",
            "Epoch 15/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6330 - accuracy: 0.7058 - val_loss: 0.8269 - val_accuracy: 0.6293\n",
            "Epoch 16/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6277 - accuracy: 0.7034 - val_loss: 0.8059 - val_accuracy: 0.6257\n",
            "Epoch 17/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6160 - accuracy: 0.7078 - val_loss: 0.8194 - val_accuracy: 0.6372\n",
            "Epoch 18/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.6075 - accuracy: 0.7143 - val_loss: 0.8367 - val_accuracy: 0.6275\n",
            "Epoch 19/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5925 - accuracy: 0.7190 - val_loss: 0.8824 - val_accuracy: 0.6268\n",
            "Epoch 20/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.5866 - accuracy: 0.7243 - val_loss: 0.8601 - val_accuracy: 0.6350\n",
            "Epoch 21/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5760 - accuracy: 0.7327 - val_loss: 0.8925 - val_accuracy: 0.6242\n",
            "Epoch 22/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5667 - accuracy: 0.7304 - val_loss: 0.8864 - val_accuracy: 0.6224\n",
            "Epoch 23/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5620 - accuracy: 0.7365 - val_loss: 0.9497 - val_accuracy: 0.6142\n",
            "Epoch 24/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5546 - accuracy: 0.7407 - val_loss: 0.8997 - val_accuracy: 0.6296\n",
            "Epoch 25/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.5576 - accuracy: 0.7424 - val_loss: 0.9168 - val_accuracy: 0.6264\n",
            "Epoch 26/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.5357 - accuracy: 0.7472 - val_loss: 0.9731 - val_accuracy: 0.6163\n",
            "Epoch 27/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.5313 - accuracy: 0.7549 - val_loss: 0.9961 - val_accuracy: 0.6221\n",
            "Epoch 28/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.5254 - accuracy: 0.7531 - val_loss: 0.9906 - val_accuracy: 0.6239\n",
            "Epoch 29/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.5134 - accuracy: 0.7592 - val_loss: 1.0169 - val_accuracy: 0.6217\n",
            "Epoch 30/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.5022 - accuracy: 0.7644 - val_loss: 1.0778 - val_accuracy: 0.6178\n",
            "Epoch 31/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.5064 - accuracy: 0.7623 - val_loss: 1.1220 - val_accuracy: 0.6232\n",
            "Epoch 32/150\n",
            "348/348 [==============================] - 3s 10ms/step - loss: 0.4903 - accuracy: 0.7718 - val_loss: 1.1421 - val_accuracy: 0.6066\n",
            "Epoch 33/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4821 - accuracy: 0.7697 - val_loss: 1.1659 - val_accuracy: 0.6138\n",
            "Epoch 34/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4873 - accuracy: 0.7715 - val_loss: 1.1176 - val_accuracy: 0.6149\n",
            "Epoch 35/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4745 - accuracy: 0.7777 - val_loss: 1.1198 - val_accuracy: 0.6152\n",
            "Epoch 36/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4708 - accuracy: 0.7765 - val_loss: 1.1863 - val_accuracy: 0.6221\n",
            "Epoch 37/150\n",
            "348/348 [==============================] - 3s 10ms/step - loss: 0.4660 - accuracy: 0.7802 - val_loss: 1.2583 - val_accuracy: 0.6109\n",
            "Epoch 38/150\n",
            "348/348 [==============================] - 3s 10ms/step - loss: 0.4589 - accuracy: 0.7831 - val_loss: 1.2588 - val_accuracy: 0.6127\n",
            "Epoch 39/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4530 - accuracy: 0.7852 - val_loss: 1.2999 - val_accuracy: 0.6081\n",
            "Epoch 40/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4530 - accuracy: 0.7887 - val_loss: 1.2439 - val_accuracy: 0.6170\n",
            "Epoch 41/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4406 - accuracy: 0.7897 - val_loss: 1.2563 - val_accuracy: 0.6099\n",
            "Epoch 42/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.4362 - accuracy: 0.7949 - val_loss: 1.2900 - val_accuracy: 0.6055\n",
            "Epoch 43/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4289 - accuracy: 0.7961 - val_loss: 1.3331 - val_accuracy: 0.6077\n",
            "Epoch 44/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.4250 - accuracy: 0.7999 - val_loss: 1.2979 - val_accuracy: 0.6037\n",
            "Epoch 45/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4191 - accuracy: 0.7992 - val_loss: 1.3148 - val_accuracy: 0.6120\n",
            "Epoch 46/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4210 - accuracy: 0.7977 - val_loss: 1.3744 - val_accuracy: 0.6019\n",
            "Epoch 47/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4107 - accuracy: 0.8026 - val_loss: 1.3742 - val_accuracy: 0.6106\n",
            "Epoch 48/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4131 - accuracy: 0.8025 - val_loss: 1.4293 - val_accuracy: 0.5965\n",
            "Epoch 49/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.4043 - accuracy: 0.8047 - val_loss: 1.4868 - val_accuracy: 0.6109\n",
            "Epoch 50/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.4003 - accuracy: 0.8067 - val_loss: 1.4458 - val_accuracy: 0.6037\n",
            "Epoch 51/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.4071 - accuracy: 0.8062 - val_loss: 1.5203 - val_accuracy: 0.6045\n",
            "Epoch 52/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3963 - accuracy: 0.8078 - val_loss: 1.4047 - val_accuracy: 0.5919\n",
            "Epoch 53/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3920 - accuracy: 0.8105 - val_loss: 1.4103 - val_accuracy: 0.6081\n",
            "Epoch 54/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.3866 - accuracy: 0.8154 - val_loss: 1.5286 - val_accuracy: 0.6023\n",
            "Epoch 55/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3807 - accuracy: 0.8147 - val_loss: 1.6712 - val_accuracy: 0.5987\n",
            "Epoch 56/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.3824 - accuracy: 0.8172 - val_loss: 1.6203 - val_accuracy: 0.6023\n",
            "Epoch 57/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3708 - accuracy: 0.8199 - val_loss: 1.7637 - val_accuracy: 0.6088\n",
            "Epoch 58/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3763 - accuracy: 0.8155 - val_loss: 1.6388 - val_accuracy: 0.6052\n",
            "Epoch 59/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3597 - accuracy: 0.8218 - val_loss: 1.7986 - val_accuracy: 0.5915\n",
            "Epoch 60/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3744 - accuracy: 0.8231 - val_loss: 1.5470 - val_accuracy: 0.5832\n",
            "Epoch 61/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.3581 - accuracy: 0.8285 - val_loss: 1.7519 - val_accuracy: 0.5983\n",
            "Epoch 62/150\n",
            "348/348 [==============================] - 3s 10ms/step - loss: 0.3484 - accuracy: 0.8274 - val_loss: 1.8777 - val_accuracy: 0.5944\n",
            "Epoch 63/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.3492 - accuracy: 0.8293 - val_loss: 1.7252 - val_accuracy: 0.5987\n",
            "Epoch 64/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.3509 - accuracy: 0.8271 - val_loss: 1.7863 - val_accuracy: 0.5980\n",
            "Epoch 65/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3498 - accuracy: 0.8304 - val_loss: 1.7097 - val_accuracy: 0.5973\n",
            "Epoch 66/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.3630 - accuracy: 0.8245 - val_loss: 1.6561 - val_accuracy: 0.5937\n",
            "Epoch 67/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.3560 - accuracy: 0.8275 - val_loss: 1.7277 - val_accuracy: 0.5955\n",
            "Epoch 68/150\n",
            "348/348 [==============================] - 3s 10ms/step - loss: 0.3325 - accuracy: 0.8366 - val_loss: 1.9486 - val_accuracy: 0.5948\n",
            "Epoch 69/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3321 - accuracy: 0.8334 - val_loss: 1.9810 - val_accuracy: 0.5944\n",
            "Epoch 70/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3284 - accuracy: 0.8342 - val_loss: 1.9016 - val_accuracy: 0.5937\n",
            "Epoch 71/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3432 - accuracy: 0.8354 - val_loss: 1.7901 - val_accuracy: 0.5948\n",
            "Epoch 72/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3425 - accuracy: 0.8335 - val_loss: 1.8444 - val_accuracy: 0.5944\n",
            "Epoch 73/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3099 - accuracy: 0.8456 - val_loss: 2.0589 - val_accuracy: 0.6005\n",
            "Epoch 74/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3169 - accuracy: 0.8421 - val_loss: 1.9782 - val_accuracy: 0.5883\n",
            "Epoch 75/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.3159 - accuracy: 0.8404 - val_loss: 2.0908 - val_accuracy: 0.5933\n",
            "Epoch 76/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3549 - accuracy: 0.8314 - val_loss: 1.8611 - val_accuracy: 0.5998\n",
            "Epoch 77/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.3202 - accuracy: 0.8395 - val_loss: 1.7483 - val_accuracy: 0.5807\n",
            "Epoch 78/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3056 - accuracy: 0.8425 - val_loss: 2.0692 - val_accuracy: 0.5854\n",
            "Epoch 79/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2951 - accuracy: 0.8474 - val_loss: 2.1225 - val_accuracy: 0.5926\n",
            "Epoch 80/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3214 - accuracy: 0.8403 - val_loss: 1.9663 - val_accuracy: 0.5901\n",
            "Epoch 81/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.3083 - accuracy: 0.8446 - val_loss: 2.0552 - val_accuracy: 0.5890\n",
            "Epoch 82/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3049 - accuracy: 0.8442 - val_loss: 2.1599 - val_accuracy: 0.5951\n",
            "Epoch 83/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3031 - accuracy: 0.8471 - val_loss: 2.2092 - val_accuracy: 0.5976\n",
            "Epoch 84/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3066 - accuracy: 0.8481 - val_loss: 2.1013 - val_accuracy: 0.5962\n",
            "Epoch 85/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.3094 - accuracy: 0.8469 - val_loss: 2.0013 - val_accuracy: 0.5944\n",
            "Epoch 86/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.3041 - accuracy: 0.8507 - val_loss: 2.0926 - val_accuracy: 0.5861\n",
            "Epoch 87/150\n",
            "348/348 [==============================] - 4s 10ms/step - loss: 0.2910 - accuracy: 0.8563 - val_loss: 2.0847 - val_accuracy: 0.5922\n",
            "Epoch 88/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2873 - accuracy: 0.8544 - val_loss: 2.0435 - val_accuracy: 0.5883\n",
            "Epoch 89/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2912 - accuracy: 0.8523 - val_loss: 2.1031 - val_accuracy: 0.5850\n",
            "Epoch 90/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2920 - accuracy: 0.8538 - val_loss: 2.0715 - val_accuracy: 0.5926\n",
            "Epoch 91/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2937 - accuracy: 0.8494 - val_loss: 2.2681 - val_accuracy: 0.5901\n",
            "Epoch 92/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2895 - accuracy: 0.8557 - val_loss: 2.2211 - val_accuracy: 0.5948\n",
            "Epoch 93/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2863 - accuracy: 0.8551 - val_loss: 2.2836 - val_accuracy: 0.5897\n",
            "Epoch 94/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2804 - accuracy: 0.8590 - val_loss: 2.2979 - val_accuracy: 0.5796\n",
            "Epoch 95/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2680 - accuracy: 0.8610 - val_loss: 2.3587 - val_accuracy: 0.5832\n",
            "Epoch 96/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2771 - accuracy: 0.8599 - val_loss: 2.3170 - val_accuracy: 0.5940\n",
            "Epoch 97/150\n",
            "348/348 [==============================] - 3s 10ms/step - loss: 0.2873 - accuracy: 0.8557 - val_loss: 2.2518 - val_accuracy: 0.5901\n",
            "Epoch 98/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2838 - accuracy: 0.8528 - val_loss: 2.0732 - val_accuracy: 0.5854\n",
            "Epoch 99/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2868 - accuracy: 0.8565 - val_loss: 2.1126 - val_accuracy: 0.5937\n",
            "Epoch 100/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2920 - accuracy: 0.8548 - val_loss: 2.1979 - val_accuracy: 0.5962\n",
            "Epoch 101/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2805 - accuracy: 0.8581 - val_loss: 2.2991 - val_accuracy: 0.5965\n",
            "Epoch 102/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2589 - accuracy: 0.8642 - val_loss: 2.3367 - val_accuracy: 0.6001\n",
            "Epoch 103/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2562 - accuracy: 0.8644 - val_loss: 2.5702 - val_accuracy: 0.5919\n",
            "Epoch 104/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2757 - accuracy: 0.8592 - val_loss: 2.3464 - val_accuracy: 0.5876\n",
            "Epoch 105/150\n",
            "348/348 [==============================] - 3s 10ms/step - loss: 0.2763 - accuracy: 0.8605 - val_loss: 2.3545 - val_accuracy: 0.5879\n",
            "Epoch 106/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2608 - accuracy: 0.8651 - val_loss: 2.6642 - val_accuracy: 0.5822\n",
            "Epoch 107/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2547 - accuracy: 0.8696 - val_loss: 2.3804 - val_accuracy: 0.5915\n",
            "Epoch 108/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2597 - accuracy: 0.8661 - val_loss: 2.6135 - val_accuracy: 0.5904\n",
            "Epoch 109/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2618 - accuracy: 0.8636 - val_loss: 2.4486 - val_accuracy: 0.5753\n",
            "Epoch 110/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2764 - accuracy: 0.8590 - val_loss: 2.2850 - val_accuracy: 0.5861\n",
            "Epoch 111/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2713 - accuracy: 0.8629 - val_loss: 2.3471 - val_accuracy: 0.5789\n",
            "Epoch 112/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2520 - accuracy: 0.8686 - val_loss: 2.5627 - val_accuracy: 0.5915\n",
            "Epoch 113/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2571 - accuracy: 0.8669 - val_loss: 2.3650 - val_accuracy: 0.5789\n",
            "Epoch 114/150\n",
            "348/348 [==============================] - 3s 8ms/step - loss: 0.2389 - accuracy: 0.8747 - val_loss: 2.5586 - val_accuracy: 0.5883\n",
            "Epoch 115/150\n",
            "348/348 [==============================] - 3s 7ms/step - loss: 0.2651 - accuracy: 0.8642 - val_loss: 2.6446 - val_accuracy: 0.5915\n",
            "Epoch 116/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2659 - accuracy: 0.8690 - val_loss: 2.6114 - val_accuracy: 0.5696\n",
            "Epoch 117/150\n",
            "348/348 [==============================] - 3s 9ms/step - loss: 0.2521 - accuracy: 0.8708 - val_loss: 2.5980 - val_accuracy: 0.5789\n",
            "Epoch 118/150\n",
            "272/348 [======================>.......] - ETA: 0s - loss: 0.2446 - accuracy: 0.8773"
          ]
        }
      ],
      "source": [
        "# Define the models and their configurations\n",
        "model_configs = []\n",
        "for num_layers in range(2, 6):\n",
        "    for neurons in range(30, 181, 30):\n",
        "        model_configs.append((num_layers, neurons))\n",
        "\n",
        "# Initialize lists to store models and histories\n",
        "models3 = []\n",
        "histories3 = []\n",
        "\n",
        "# Loop over model configurations\n",
        "for i, config in enumerate(model_configs, start=2):\n",
        "    # Extract the number of layers and neurons\n",
        "    num_layers, neurons = config\n",
        "\n",
        "    # Print the number of layers and neurons\n",
        "    print(f\"Trial {i}: Number of layers = {num_layers}, Number of neurons per layer = {neurons}\")\n",
        "\n",
        "    # Define the model\n",
        "    model_layers = [Dense(68, activation='relu', input_shape=input_shape)]\n",
        "    for _ in range(num_layers - 1):\n",
        "        model_layers.append(Dense(neurons, activation='relu'))\n",
        "    model_layers.append(Dense(3, activation='softmax'))\n",
        "\n",
        "    model = tf.keras.models.Sequential(model_layers)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model and store the history\n",
        "    history = model.fit(X_train, y_train_onehot, epochs=150, validation_data=(X_test, y_test_onehot))\n",
        "\n",
        "    # Append the model and its history to the lists\n",
        "    models3.append(model)\n",
        "    histories3.append(history)\n",
        "\n",
        "    # Print the history for reference\n",
        "    print(f\"History for model {i}:\", history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzuIqQCH_Bch"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtDODxlV_Iin"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "6MN9qK81_C8x",
        "outputId": "34c01975-e662-4d7f-add1-562e7ff0b363"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=100)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "RandomForestClassifier(random_state=100)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=100)\n",
        "rf_classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLoeLGbD_N_t",
        "outputId": "a9624755-9432-4fae-9f94-079b6a5db3ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6467098166127292\n",
            "Confusion Matrix:\n",
            "[[584  59 204  80]\n",
            " [ 50 524   6 347]\n",
            " [104   5 803  15]\n",
            " [ 88 337  15 487]]\n"
          ]
        }
      ],
      "source": [
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1nBIaGB_UGP"
      },
      "source": [
        "Importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_miirOx_Vkf",
        "outputId": "c3f7d7e6-463e-4d7c-f910-f50d4f61eb86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature ranking:\n",
            "1. Feature 'Percent Shared Unique Elements': Importance 0.0437\n",
            "2. Feature 'Median C (X)': Importance 0.0363\n",
            "3. Feature 'X_Type': Importance 0.0355\n",
            "4. Feature 'Num_Entries_Y': Importance 0.0336\n",
            "5. Feature 'Sample Standard Deviation (X)': Importance 0.0330\n",
            "6. Feature 'Identical Unique': Importance 0.0321\n",
            "7. Feature 'Y_Type': Importance 0.0318\n",
            "8. Feature 'Normalized Mean (Y)': Importance 0.0317\n",
            "9. Feature 'Coefficient of Variation (X)': Importance 0.0314\n",
            "10. Feature 'Q Entropy (X)': Importance 0.0300\n",
            "11. Feature 'Ks Significant 005': Importance 0.0299\n",
            "12. Feature 'Normalized Range (Y)': Importance 0.0299\n",
            "13. Feature 'Normalized Median (Y)': Importance 0.0298\n",
            "14. Feature 'Mean C (X)': Importance 0.0265\n",
            "15. Feature 'Normalized Range (X)': Importance 0.0256\n",
            "16. Feature 'Perc Mode (Y)': Importance 0.0243\n",
            "17. Feature 'Sample Variance (Y)': Importance 0.0232\n",
            "18. Feature 'Maximum Value (X)': Importance 0.0229\n",
            "19. Feature 'Skewness (X)': Importance 0.0225\n",
            "20. Feature 'Kurtosis (X)': Importance 0.0201\n",
            "21. Feature 'Kurtosis (Y)': Importance 0.0194\n",
            "22. Feature 'Correlation P': Importance 0.0184\n",
            "23. Feature '% Outliers (1.5 IQR) (X)': Importance 0.0183\n",
            "24. Feature 'Has Shared Elements': Importance 0.0170\n",
            "25. Feature 'Normalized Mean (X)': Importance 0.0152\n",
            "26. Feature 'Identical': Importance 0.0148\n",
            "27. Feature 'Percent Shared Elements': Importance 0.0145\n",
            "28. Feature 'Has_None_Y': Importance 0.0144\n",
            "29. Feature 'Num_Entries_X': Importance 0.0142\n",
            "30. Feature 'Minimum Value (Y)': Importance 0.0141\n",
            "31. Feature 'Ks P': Importance 0.0139\n",
            "32. Feature 'Percentage_None_X': Importance 0.0137\n",
            "33. Feature 'Sample Standard Deviation (Y)': Importance 0.0135\n",
            "34. Feature 'Has Shared Unique Elements': Importance 0.0128\n",
            "35. Feature 'Maximum Value (Y)': Importance 0.0126\n",
            "36. Feature 'C Entropy (Y)': Importance 0.0104\n",
            "37. Feature 'Correlation Significant 005': Importance 0.0102\n",
            "38. Feature 'Coefficient of Variation (Y)': Importance 0.0102\n",
            "39. Feature 'Q Entropy (Y)': Importance 0.0095\n",
            "40. Feature 'C Entropy (X)': Importance 0.0088\n",
            "41. Feature 'Perc Mode (X)': Importance 0.0083\n",
            "42. Feature 'Minimum Value (X)': Importance 0.0083\n",
            "43. Feature 'Normalized Median (X)': Importance 0.0072\n",
            "44. Feature '% Outliers (3 IQR) (X)': Importance 0.0070\n",
            "45. Feature 'Skewness (Y)': Importance 0.0067\n",
            "46. Feature 'Sample Variance (X)': Importance 0.0066\n",
            "47. Feature 'Has Outliers (3 IQR) (X)': Importance 0.0065\n",
            "48. Feature 'Num Shared Unique Elements': Importance 0.0061\n",
            "49. Feature 'Percentage_None_Y': Importance 0.0055\n",
            "50. Feature 'Ks Statistic': Importance 0.0038\n",
            "51. Feature 'Num Identical Elements': Importance 0.0033\n",
            "52. Feature '% Outliers (3 Std Dev) (X)': Importance 0.0031\n",
            "53. Feature 'Median C (Y)': Importance 0.0029\n",
            "54. Feature 'Unique C (Y)': Importance 0.0026\n",
            "55. Feature 'Num_None_Y': Importance 0.0023\n",
            "56. Feature 'Correlation Value': Importance 0.0016\n",
            "57. Feature '% Outliers (1-99 Percentile) (X)': Importance 0.0014\n",
            "58. Feature 'Std C (Y)': Importance 0.0013\n",
            "59. Feature 'Min C (X)': Importance 0.0012\n",
            "60. Feature 'Unique C (X)': Importance 0.0011\n",
            "61. Feature 'Max C (Y)': Importance 0.0011\n",
            "62. Feature 'Mean C (Y)': Importance 0.0010\n",
            "63. Feature 'Max C (X)': Importance 0.0010\n",
            "64. Feature 'Std C (X)': Importance 0.0009\n",
            "65. Feature 'Min C (Y)': Importance 0.0009\n",
            "66. Feature '% Outliers (3 Std Dev) (Y)': Importance 0.0007\n",
            "67. Feature 'Has Outliers (1.5 IQR) (Y)': Importance 0.0007\n",
            "68. Feature 'Has Outliers (3 Std Dev) (Y)': Importance 0.0001\n",
            "69. Feature 'Has Outliers (1.5 IQR) (X)': Importance 0.0000\n",
            "70. Feature 'Sortedness (X)': Importance 0.0000\n",
            "71. Feature 'Has Outliers (3 IQR) (Y)': Importance 0.0000\n",
            "72. Feature '% Outliers (1.5 IQR) (Y)': Importance 0.0000\n",
            "73. Feature 'Is Sorted (X)': Importance 0.0000\n",
            "74. Feature 'Has Outliers (1-99 Percentile) (Y)': Importance 0.0000\n",
            "75. Feature 'Has Outliers (3 Std Dev) (X)': Importance 0.0000\n",
            "76. Feature 'Has Outliers (1-99 Percentile) (X)': Importance 0.0000\n",
            "77. Feature '% Outliers (3 IQR) (Y)': Importance 0.0000\n",
            "78. Feature 'Num_Unique_X': Importance 0.0000\n",
            "79. Feature 'Num_Unique_Y': Importance 0.0000\n",
            "80. Feature 'Has_None_X': Importance 0.0000\n",
            "81. Feature 'Num_None_X': Importance 0.0000\n",
            "82. Feature '% Outliers (1-99 Percentile) (Y)': Importance 0.0000\n",
            "83. Feature 'Plot_Type': Importance 0.0000\n"
          ]
        }
      ],
      "source": [
        "feature_names = df.columns.tolist()\n",
        "importances = rf_classifier.feature_importances_\n",
        "\n",
        "feature_importance_list = list(zip(feature_names, importances))\n",
        "feature_importance_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Feature ranking:\")\n",
        "for i, (feature, importance) in enumerate(feature_importance_list):\n",
        "    print(f\"{i+1}. Feature '{feature}': Importance {importance:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbO9DVv0Q_2w"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/thesis/automl/features1/custom.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIZVabd7FOjO",
        "outputId": "5e9fa8f8-1dfc-4713-a1ce-7a32bc837ea3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(13905, 34)\n"
          ]
        }
      ],
      "source": [
        "# features bellow 1% importance\n",
        "weak_features = [feature for feature, importance in feature_importance_list if importance <= 0.01]\n",
        "df = df.drop(weak_features, axis=1)  # Features\n",
        "print(df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFOFz_-oH1I-",
        "outputId": "7c09409c-e598-4398-d634-c2d788c5ee0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(13905, 24)\n"
          ]
        }
      ],
      "source": [
        "# least 10 important features\n",
        "feature_importance_list.sort(key=lambda x: x[1])\n",
        "weak_features = [feature for feature, _ in feature_importance_list[-10:]]\n",
        "df = df.drop(weak_features, axis=1)  # Features\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ol5UZAXRn5m"
      },
      "source": [
        "### Training - Selected Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "cIYFhZ4RNZox",
        "outputId": "f97b88fb-0e47-4750-f574-5439900ab9d8"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'Plot_Type'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Plot_Type'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-167-e75c0c96e789>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# One-hot encode the labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_integer_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-152-a2b806c4d31f>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Plot_Type'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Plot_Type'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mX_categorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3761\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3763\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Plot_Type'"
          ]
        }
      ],
      "source": [
        "X,Y = preprocessing(df)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=100, stratify=Y)\n",
        "# One-hot encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_integer_encoded = label_encoder.fit_transform(y_train)\n",
        "test_integer_encoded = label_encoder.fit_transform(y_test)\n",
        "y_train_onehot = to_categorical(train_integer_encoded)\n",
        "y_test_onehot = to_categorical(test_integer_encoded)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "init_neurons = X_train.shape[1]\n",
        "custom_optimizer = Adam(learning_rate=0.00001)\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    Dense(init_neurons, activation='relu', input_shape=input_shape),\n",
        "    Dense(60, activation='relu'),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#history = model.fit(X_train, y_train, epochs=150, validation_data=(X_test, y_test), callbacks=[reduce_lr])\n",
        "history = model.fit(X_train, y_train_onehot, epochs=60, validation_data=(X_test, y_test_onehot))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "fUdObQ_FZMBf",
        "outputId": "07c428e7-41df-4fdc-863a-bb7b7ea6a879"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Callback' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-168-57c7da623542>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSaveBestWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSaveBestWeights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Callback' is not defined"
          ]
        }
      ],
      "source": [
        "class SaveBestWeights(Callback):\n",
        "    def __init__(self, monitor='val_accuracy', mode='max'):\n",
        "        super(SaveBestWeights, self).__init__()\n",
        "        self.monitor = monitor\n",
        "        self.mode = mode\n",
        "        self.best_weights = None\n",
        "        self.best_metric = float('-inf') if mode == 'max' else float('inf')\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current_metric = logs.get(self.monitor)\n",
        "        if current_metric is None:\n",
        "            return\n",
        "        if (self.mode == 'max' and current_metric > self.best_metric) or \\\n",
        "           (self.mode == 'min' and current_metric < self.best_metric):\n",
        "            self.best_metric = current_metric\n",
        "            self.best_weights = self.model.get_weights()\n",
        "\n",
        "class RestoreBestWeights(Callback):\n",
        "    def __init__(self, save_callback):\n",
        "        super(RestoreBestWeights, self).__init__()\n",
        "        self.save_callback = save_callback\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current_metric = logs.get(self.save_callback.monitor)\n",
        "        if current_metric is None:\n",
        "            return\n",
        "        if (self.save_callback.mode == 'max' and current_metric < self.save_callback.best_metric) or \\\n",
        "           (self.save_callback.mode == 'min' and current_metric > self.save_callback.best_metric):\n",
        "            self.model.set_weights(self.save_callback.best_weights)\n",
        "\n",
        "# Usage example:\n",
        "save_callback = SaveBestWeights(monitor='val_accuracy', mode='max')\n",
        "restore_callback = RestoreBestWeights(save_callback)\n",
        "\n",
        "model.fit(X_train, y_train_onehot, epochs=100, validation_data=(X_test, y_test_onehot), callbacks=[save_callback, restore_callback])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0Sq6rcOToRL",
        "outputId": "fec7d001-522b-4905-80e7-69bdf8fbb649"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.6\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter tuning using Talos"
      ],
      "metadata": {
        "id": "ZfFNZ7rKS55h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,Y = preprocessing(df_lsbp)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, Y, test_size=0.2, random_state=100, stratify=Y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "y_train_onehot, y_test_onehot, y_val_onehot, output_shape = one_hot(y_train,y_test,y_val)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "init_neurons = X_train.shape[1]\n",
        "\n",
        "model_configs = [\n",
        "    (init_neurons, 50),\n",
        "    (init_neurons, 100),\n",
        "    (init_neurons, 150),\n",
        "]\n",
        "\n",
        "batch_sizes = [64, 128, 256]\n",
        "dropout_rates = [0, 0.1, 0.5]\n",
        "\n",
        "# Initialize a list to store results\n",
        "results = []\n",
        "\n",
        "# Loop over model configurations, batch sizes, and dropout rates\n",
        "for neurons, dropout_rate in [(cfg[1], dropout_rate) for cfg in model_configs for dropout_rate in dropout_rates]:\n",
        "    for batch_size in batch_sizes:\n",
        "        print(f\"Trial: Neurons = {neurons}, Dropout Rate = {dropout_rate}, Batch Size = {batch_size}\")\n",
        "\n",
        "        model = tf.keras.models.Sequential([\n",
        "            Dense(init_neurons, activation='relu', input_shape=input_shape),\n",
        "            Dense(neurons, activation='relu'),\n",
        "            Dropout(dropout_rate),\n",
        "            Dense(4, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        # Initialize the Adam optimizer\n",
        "        optimizer = Adam(learning_rate=0.0005)\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        # Train the model\n",
        "        history = model.fit(X_train, y_train_onehot, epochs=50, batch_size=batch_size, validation_data=(X_val, y_val_onehot))\n",
        "\n",
        "        # Store the final validation accuracy and loss\n",
        "        val_loss, val_accuracy = model.evaluate(X_val, y_val_onehot, verbose=0)\n",
        "\n",
        "        # Append the results to the list\n",
        "        results.append({\n",
        "            'Neurons': neurons,\n",
        "            'Dropout Rate': dropout_rate,\n",
        "            'Batch Size': batch_size,\n",
        "            'Validation Loss': val_loss,\n",
        "            'Validation Accuracy': val_accuracy\n",
        "        })\n",
        "\n",
        "        # Print the history for reference\n",
        "        print(f\"History for model:\", history.history)\n",
        "\n",
        "# Convert results to DataFrame and save to CSV\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('model_results.csv', index=False)\n",
        "\n",
        "print(\"Results saved to 'model_results.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd6gjuj4bjjy",
        "outputId": "f2b3119c-d61b-46b1-d13d-9dcf544ccce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'bar', 1: 'line', 2: 'pie', 3: 'scatter'}\n",
            "Trial: Neurons = 50, Dropout Rate = 0, Batch Size = 64\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4661 - loss: 1.1939 - val_accuracy: 0.5831 - val_loss: 0.8970\n",
            "Epoch 2/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6094 - loss: 0.8597 - val_accuracy: 0.6111 - val_loss: 0.8634\n",
            "Epoch 3/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6119 - loss: 0.8437 - val_accuracy: 0.6162 - val_loss: 0.8488\n",
            "Epoch 4/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6154 - loss: 0.8278 - val_accuracy: 0.6160 - val_loss: 0.8443\n",
            "Epoch 5/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6268 - loss: 0.8141 - val_accuracy: 0.6125 - val_loss: 0.8405\n",
            "Epoch 6/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6302 - loss: 0.8057 - val_accuracy: 0.6241 - val_loss: 0.8339\n",
            "Epoch 7/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6436 - loss: 0.7915 - val_accuracy: 0.6297 - val_loss: 0.8286\n",
            "Epoch 8/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6415 - loss: 0.7877 - val_accuracy: 0.6257 - val_loss: 0.8244\n",
            "Epoch 9/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6455 - loss: 0.7777 - val_accuracy: 0.6313 - val_loss: 0.8264\n",
            "Epoch 10/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6559 - loss: 0.7676 - val_accuracy: 0.6276 - val_loss: 0.8276\n",
            "Epoch 11/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6463 - loss: 0.7795 - val_accuracy: 0.6286 - val_loss: 0.8230\n",
            "Epoch 12/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6499 - loss: 0.7705 - val_accuracy: 0.6316 - val_loss: 0.8223\n",
            "Epoch 13/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6502 - loss: 0.7640 - val_accuracy: 0.6316 - val_loss: 0.8201\n",
            "Epoch 14/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6626 - loss: 0.7591 - val_accuracy: 0.6316 - val_loss: 0.8184\n",
            "Epoch 15/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6531 - loss: 0.7575 - val_accuracy: 0.6257 - val_loss: 0.8230\n",
            "Epoch 16/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6521 - loss: 0.7570 - val_accuracy: 0.6311 - val_loss: 0.8190\n",
            "Epoch 17/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6694 - loss: 0.7454 - val_accuracy: 0.6303 - val_loss: 0.8234\n",
            "Epoch 18/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6713 - loss: 0.7367 - val_accuracy: 0.6305 - val_loss: 0.8214\n",
            "Epoch 19/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6729 - loss: 0.7459 - val_accuracy: 0.6313 - val_loss: 0.8212\n",
            "Epoch 20/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6636 - loss: 0.7476 - val_accuracy: 0.6251 - val_loss: 0.8223\n",
            "Epoch 21/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6689 - loss: 0.7448 - val_accuracy: 0.6286 - val_loss: 0.8218\n",
            "Epoch 22/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6765 - loss: 0.7252 - val_accuracy: 0.6316 - val_loss: 0.8260\n",
            "Epoch 23/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6754 - loss: 0.7246 - val_accuracy: 0.6292 - val_loss: 0.8213\n",
            "Epoch 24/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6712 - loss: 0.7182 - val_accuracy: 0.6216 - val_loss: 0.8237\n",
            "Epoch 25/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6766 - loss: 0.7170 - val_accuracy: 0.6227 - val_loss: 0.8256\n",
            "Epoch 26/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6746 - loss: 0.7251 - val_accuracy: 0.6203 - val_loss: 0.8282\n",
            "Epoch 27/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6837 - loss: 0.7251 - val_accuracy: 0.6257 - val_loss: 0.8258\n",
            "Epoch 28/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6794 - loss: 0.7165 - val_accuracy: 0.6254 - val_loss: 0.8266\n",
            "Epoch 29/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6769 - loss: 0.7053 - val_accuracy: 0.6292 - val_loss: 0.8242\n",
            "Epoch 30/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6811 - loss: 0.7107 - val_accuracy: 0.6249 - val_loss: 0.8281\n",
            "Epoch 31/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6857 - loss: 0.7025 - val_accuracy: 0.6313 - val_loss: 0.8268\n",
            "Epoch 32/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6879 - loss: 0.6971 - val_accuracy: 0.6297 - val_loss: 0.8238\n",
            "Epoch 33/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6868 - loss: 0.7083 - val_accuracy: 0.6313 - val_loss: 0.8290\n",
            "Epoch 34/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6880 - loss: 0.6999 - val_accuracy: 0.6294 - val_loss: 0.8266\n",
            "Epoch 35/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6863 - loss: 0.7004 - val_accuracy: 0.6262 - val_loss: 0.8290\n",
            "Epoch 36/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6887 - loss: 0.6949 - val_accuracy: 0.6278 - val_loss: 0.8284\n",
            "Epoch 37/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6971 - loss: 0.6786 - val_accuracy: 0.6268 - val_loss: 0.8342\n",
            "Epoch 38/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6896 - loss: 0.7034 - val_accuracy: 0.6206 - val_loss: 0.8344\n",
            "Epoch 39/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7043 - loss: 0.6738 - val_accuracy: 0.6297 - val_loss: 0.8332\n",
            "Epoch 40/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6885 - loss: 0.6925 - val_accuracy: 0.6303 - val_loss: 0.8384\n",
            "Epoch 41/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6947 - loss: 0.6857 - val_accuracy: 0.6189 - val_loss: 0.8454\n",
            "Epoch 42/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7010 - loss: 0.6727 - val_accuracy: 0.6160 - val_loss: 0.8403\n",
            "Epoch 43/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6996 - loss: 0.6772 - val_accuracy: 0.6219 - val_loss: 0.8424\n",
            "Epoch 44/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6947 - loss: 0.6821 - val_accuracy: 0.6254 - val_loss: 0.8438\n",
            "Epoch 45/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6988 - loss: 0.6755 - val_accuracy: 0.6257 - val_loss: 0.8443\n",
            "Epoch 46/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7043 - loss: 0.6665 - val_accuracy: 0.6259 - val_loss: 0.8466\n",
            "Epoch 47/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7041 - loss: 0.6666 - val_accuracy: 0.6251 - val_loss: 0.8414\n",
            "Epoch 48/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6993 - loss: 0.6798 - val_accuracy: 0.6181 - val_loss: 0.8488\n",
            "Epoch 49/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7031 - loss: 0.6705 - val_accuracy: 0.6292 - val_loss: 0.8498\n",
            "Epoch 50/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7055 - loss: 0.6602 - val_accuracy: 0.6330 - val_loss: 0.8476\n",
            "History for model: {'accuracy': [0.5391945242881775, 0.6050881147384644, 0.6148867607116699, 0.6189320683479309, 0.626303493976593, 0.6298094391822815, 0.6405069828033447, 0.6388888955116272, 0.6438331604003906, 0.6486875414848328, 0.6492269039154053, 0.6511147022247314, 0.653182327747345, 0.6574972867965698, 0.6556094884872437, 0.6581265926361084, 0.6647788286209106, 0.6642394661903381, 0.6679251790046692, 0.6659474968910217, 0.6697230935096741, 0.6689140796661377, 0.670082688331604, 0.6720604300498962, 0.6733189225196838, 0.6747573018074036, 0.6796116232872009, 0.6764652729034424, 0.6782631874084473, 0.676734983921051, 0.6813196539878845, 0.6841064095497131, 0.6832074522972107, 0.6832074522972107, 0.6850053668022156, 0.6857245564460754, 0.6886911392211914, 0.6869831085205078, 0.6930060982704163, 0.6892305016517639, 0.6953433752059937, 0.6977705955505371, 0.696601927280426, 0.6975907683372498, 0.698130190372467, 0.6986695528030396, 0.6979503631591797, 0.7000179886817932, 0.7030744552612305, 0.7017260193824768], 'loss': [1.038784384727478, 0.8657490015029907, 0.8387082815170288, 0.8258830308914185, 0.8144009113311768, 0.8051784038543701, 0.7977203130722046, 0.7911345958709717, 0.78369140625, 0.7786981463432312, 0.7764164209365845, 0.7699896693229675, 0.76612389087677, 0.7605617642402649, 0.7569870352745056, 0.7540434002876282, 0.749464213848114, 0.746148943901062, 0.7432667016983032, 0.7405626177787781, 0.7371613383293152, 0.734035849571228, 0.7299888730049133, 0.7265401482582092, 0.7251722812652588, 0.7213791608810425, 0.7188032865524292, 0.7171450257301331, 0.714882493019104, 0.7144297957420349, 0.708990752696991, 0.7071857452392578, 0.7051063776016235, 0.7040872573852539, 0.700143575668335, 0.6985591650009155, 0.694442629814148, 0.6945068836212158, 0.6902539134025574, 0.690316915512085, 0.6869699954986572, 0.6845855116844177, 0.6819121241569519, 0.6817710995674133, 0.6777696013450623, 0.6775615811347961, 0.6760020852088928, 0.6751017570495605, 0.6713629961013794, 0.6688007116317749], 'val_accuracy': [0.5830636620521545, 0.6111111044883728, 0.6162351965904236, 0.6159654855728149, 0.6124595403671265, 0.6240561008453369, 0.6297194957733154, 0.6256741881370544, 0.6313376426696777, 0.6275620460510254, 0.6286407709121704, 0.6316073536872864, 0.6316073536872864, 0.6316073536872864, 0.6256741881370544, 0.6310679316520691, 0.6302589178085327, 0.6305285692214966, 0.6313376426696777, 0.6251348257064819, 0.6286407709121704, 0.6316073536872864, 0.6291801333427429, 0.6216289401054382, 0.6227076649665833, 0.6202805042266846, 0.6256741881370544, 0.6254045367240906, 0.6291801333427429, 0.6248651742935181, 0.6313376426696777, 0.6297194957733154, 0.6313376426696777, 0.6294498443603516, 0.6262136101722717, 0.6278316974639893, 0.6267529726028442, 0.6205501556396484, 0.6297194957733154, 0.6302589178085327, 0.6189320683479309, 0.6159654855728149, 0.6218985915184021, 0.6254045367240906, 0.6256741881370544, 0.6259438991546631, 0.6251348257064819, 0.6181229948997498, 0.6291801333427429, 0.63295578956604], 'val_loss': [0.8969825506210327, 0.8634029030799866, 0.8488386273384094, 0.8442621231079102, 0.8404849767684937, 0.8339449167251587, 0.828599214553833, 0.8244069218635559, 0.8263521790504456, 0.8276174068450928, 0.8230271935462952, 0.8222512006759644, 0.8200884461402893, 0.8184444308280945, 0.8229715824127197, 0.8190352320671082, 0.8234087824821472, 0.8213624954223633, 0.8211628198623657, 0.8222630023956299, 0.8218293786048889, 0.8259809613227844, 0.8212976455688477, 0.8236962556838989, 0.8255617022514343, 0.828239381313324, 0.8258199691772461, 0.8265777230262756, 0.8242384195327759, 0.828118622303009, 0.8268373608589172, 0.8237734436988831, 0.8289916515350342, 0.826601505279541, 0.828951895236969, 0.8283788561820984, 0.834212601184845, 0.8343930244445801, 0.8332262635231018, 0.83842933177948, 0.8453660011291504, 0.8403258323669434, 0.8423523306846619, 0.8437593579292297, 0.84432452917099, 0.8465765118598938, 0.84140545129776, 0.8487702012062073, 0.8497672080993652, 0.8475675582885742]}\n",
            "Trial: Neurons = 50, Dropout Rate = 0, Batch Size = 128\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.4591 - loss: 1.2573 - val_accuracy: 0.5758 - val_loss: 0.9323\n",
            "Epoch 2/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5961 - loss: 0.8938 - val_accuracy: 0.5984 - val_loss: 0.8785\n",
            "Epoch 3/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6105 - loss: 0.8525 - val_accuracy: 0.6111 - val_loss: 0.8617\n",
            "Epoch 4/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6155 - loss: 0.8334 - val_accuracy: 0.6106 - val_loss: 0.8489\n",
            "Epoch 5/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6216 - loss: 0.8250 - val_accuracy: 0.6127 - val_loss: 0.8437\n",
            "Epoch 6/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6266 - loss: 0.8199 - val_accuracy: 0.6195 - val_loss: 0.8401\n",
            "Epoch 7/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6407 - loss: 0.7941 - val_accuracy: 0.6243 - val_loss: 0.8349\n",
            "Epoch 8/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6423 - loss: 0.8023 - val_accuracy: 0.6200 - val_loss: 0.8370\n",
            "Epoch 9/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6382 - loss: 0.7924 - val_accuracy: 0.6197 - val_loss: 0.8324\n",
            "Epoch 10/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6522 - loss: 0.7857 - val_accuracy: 0.6211 - val_loss: 0.8300\n",
            "Epoch 11/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6433 - loss: 0.7924 - val_accuracy: 0.6184 - val_loss: 0.8309\n",
            "Epoch 12/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6494 - loss: 0.7780 - val_accuracy: 0.6243 - val_loss: 0.8291\n",
            "Epoch 13/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6492 - loss: 0.7741 - val_accuracy: 0.6114 - val_loss: 0.8302\n",
            "Epoch 14/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6436 - loss: 0.7763 - val_accuracy: 0.6313 - val_loss: 0.8231\n",
            "Epoch 15/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6526 - loss: 0.7671 - val_accuracy: 0.6294 - val_loss: 0.8220\n",
            "Epoch 16/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6567 - loss: 0.7625 - val_accuracy: 0.6265 - val_loss: 0.8233\n",
            "Epoch 17/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6568 - loss: 0.7636 - val_accuracy: 0.6300 - val_loss: 0.8224\n",
            "Epoch 18/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6574 - loss: 0.7628 - val_accuracy: 0.6313 - val_loss: 0.8190\n",
            "Epoch 19/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6562 - loss: 0.7544 - val_accuracy: 0.6311 - val_loss: 0.8206\n",
            "Epoch 20/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6656 - loss: 0.7499 - val_accuracy: 0.6241 - val_loss: 0.8219\n",
            "Epoch 21/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6648 - loss: 0.7478 - val_accuracy: 0.6286 - val_loss: 0.8226\n",
            "Epoch 22/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6712 - loss: 0.7428 - val_accuracy: 0.6265 - val_loss: 0.8265\n",
            "Epoch 23/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6705 - loss: 0.7381 - val_accuracy: 0.6292 - val_loss: 0.8217\n",
            "Epoch 24/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6726 - loss: 0.7386 - val_accuracy: 0.6335 - val_loss: 0.8204\n",
            "Epoch 25/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6653 - loss: 0.7458 - val_accuracy: 0.6278 - val_loss: 0.8251\n",
            "Epoch 26/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6764 - loss: 0.7319 - val_accuracy: 0.6284 - val_loss: 0.8199\n",
            "Epoch 27/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6773 - loss: 0.7249 - val_accuracy: 0.6254 - val_loss: 0.8223\n",
            "Epoch 28/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6702 - loss: 0.7327 - val_accuracy: 0.6308 - val_loss: 0.8215\n",
            "Epoch 29/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6806 - loss: 0.7304 - val_accuracy: 0.6281 - val_loss: 0.8243\n",
            "Epoch 30/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6802 - loss: 0.7189 - val_accuracy: 0.6281 - val_loss: 0.8229\n",
            "Epoch 31/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6750 - loss: 0.7340 - val_accuracy: 0.6292 - val_loss: 0.8182\n",
            "Epoch 32/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6764 - loss: 0.7219 - val_accuracy: 0.6367 - val_loss: 0.8207\n",
            "Epoch 33/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6804 - loss: 0.7131 - val_accuracy: 0.6348 - val_loss: 0.8209\n",
            "Epoch 34/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6839 - loss: 0.7179 - val_accuracy: 0.6327 - val_loss: 0.8214\n",
            "Epoch 35/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6801 - loss: 0.7234 - val_accuracy: 0.6340 - val_loss: 0.8218\n",
            "Epoch 36/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6837 - loss: 0.7067 - val_accuracy: 0.6316 - val_loss: 0.8240\n",
            "Epoch 37/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6835 - loss: 0.7127 - val_accuracy: 0.6378 - val_loss: 0.8232\n",
            "Epoch 38/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6832 - loss: 0.7060 - val_accuracy: 0.6308 - val_loss: 0.8283\n",
            "Epoch 39/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6932 - loss: 0.7020 - val_accuracy: 0.6292 - val_loss: 0.8245\n",
            "Epoch 40/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6795 - loss: 0.7129 - val_accuracy: 0.6340 - val_loss: 0.8247\n",
            "Epoch 41/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6904 - loss: 0.7028 - val_accuracy: 0.6357 - val_loss: 0.8239\n",
            "Epoch 42/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6923 - loss: 0.6910 - val_accuracy: 0.6327 - val_loss: 0.8250\n",
            "Epoch 43/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6906 - loss: 0.7043 - val_accuracy: 0.6354 - val_loss: 0.8294\n",
            "Epoch 44/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6946 - loss: 0.6930 - val_accuracy: 0.6327 - val_loss: 0.8241\n",
            "Epoch 45/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6877 - loss: 0.6976 - val_accuracy: 0.6311 - val_loss: 0.8304\n",
            "Epoch 46/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6894 - loss: 0.6979 - val_accuracy: 0.6319 - val_loss: 0.8315\n",
            "Epoch 47/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6948 - loss: 0.6844 - val_accuracy: 0.6311 - val_loss: 0.8279\n",
            "Epoch 48/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6901 - loss: 0.6899 - val_accuracy: 0.6289 - val_loss: 0.8318\n",
            "Epoch 49/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6922 - loss: 0.6893 - val_accuracy: 0.6375 - val_loss: 0.8272\n",
            "Epoch 50/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6934 - loss: 0.6887 - val_accuracy: 0.6305 - val_loss: 0.8330\n",
            "History for model: {'accuracy': [0.5204063057899475, 0.5994246602058411, 0.6080546379089355, 0.6174936890602112, 0.6216289401054382, 0.628820538520813, 0.6305285692214966, 0.6376303434371948, 0.638439416885376, 0.6429342031478882, 0.6405069828033447, 0.6464401483535767, 0.6494965553283691, 0.6504854559898376, 0.6514742970466614, 0.6540812849998474, 0.6574972867965698, 0.6584861278533936, 0.6586659550666809, 0.6606436371803284, 0.6655879020690918, 0.6652283072471619, 0.6654081344604492, 0.6672959327697754, 0.668644368648529, 0.6699928045272827, 0.6714311242103577, 0.6661272644996643, 0.6765552163124084, 0.6724199652671814, 0.6763753890991211, 0.6744875907897949, 0.6761956214904785, 0.6798813343048096, 0.6823084950447083, 0.6793419718742371, 0.6826680898666382, 0.6775440573692322, 0.6849154829978943, 0.6826680898666382, 0.6846458315849304, 0.6854548454284668, 0.6882416605949402, 0.6909385323524475, 0.6896799802780151, 0.6912980675697327, 0.6868932247161865, 0.6912081837654114, 0.6907587051391602, 0.6952534914016724], 'loss': [1.1049319505691528, 0.8860754370689392, 0.8517587184906006, 0.8360176682472229, 0.8247105479240417, 0.8160187602043152, 0.808195173740387, 0.8009088039398193, 0.796556830406189, 0.7915133237838745, 0.7893298268318176, 0.7819109559059143, 0.7790719866752625, 0.7750289440155029, 0.7713839411735535, 0.7676641345024109, 0.763835608959198, 0.7602298259735107, 0.7572163939476013, 0.7552751302719116, 0.7507433891296387, 0.7507449984550476, 0.7463774085044861, 0.744918704032898, 0.7398468852043152, 0.7377586364746094, 0.7352194786071777, 0.736286997795105, 0.7299484014511108, 0.7307606935501099, 0.726701557636261, 0.7240175604820251, 0.7219723463058472, 0.7173030376434326, 0.7180243134498596, 0.7154964804649353, 0.71343594789505, 0.711875319480896, 0.708415150642395, 0.7075462341308594, 0.7054637670516968, 0.7019745111465454, 0.7011451125144958, 0.7004408836364746, 0.6971175074577332, 0.696304440498352, 0.6939592361450195, 0.6928085684776306, 0.6907958388328552, 0.6893676519393921], 'val_accuracy': [0.5757821202278137, 0.5984358191490173, 0.6111111044883728, 0.6105717420578003, 0.6127292513847351, 0.6194714307785034, 0.6243258118629456, 0.6200107932090759, 0.6197410821914673, 0.621089518070221, 0.6183926463127136, 0.6243258118629456, 0.6113808155059814, 0.6313376426696777, 0.6294498443603516, 0.6264832615852356, 0.6299892067909241, 0.6313376426696777, 0.6310679316520691, 0.6240561008453369, 0.6286407709121704, 0.6264832615852356, 0.6291801333427429, 0.6334951519966125, 0.6278316974639893, 0.6283710598945618, 0.6254045367240906, 0.6307982802391052, 0.6281014084815979, 0.6281014084815979, 0.6291801333427429, 0.6367313861846924, 0.6348435878753662, 0.6326860785484314, 0.6340345144271851, 0.6316073536872864, 0.6378101110458374, 0.6307982802391052, 0.6291801333427429, 0.6340345144271851, 0.6356526613235474, 0.6326860785484314, 0.6353829503059387, 0.6326860785484314, 0.6310679316520691, 0.6318770051002502, 0.6310679316520691, 0.628910481929779, 0.6375404596328735, 0.6305285692214966], 'val_loss': [0.9323434829711914, 0.8784997463226318, 0.8617496490478516, 0.8489434123039246, 0.8436984419822693, 0.8400769829750061, 0.834934651851654, 0.8370163440704346, 0.8324238061904907, 0.8299973607063293, 0.8308666944503784, 0.8290632367134094, 0.830158531665802, 0.8230570554733276, 0.8219792246818542, 0.8232629299163818, 0.8223764300346375, 0.8189994096755981, 0.8205956816673279, 0.8219039440155029, 0.8225613832473755, 0.8265446424484253, 0.8217100501060486, 0.8204115629196167, 0.8251369595527649, 0.8199451565742493, 0.8222874402999878, 0.8215194344520569, 0.824315071105957, 0.8228855133056641, 0.8181732892990112, 0.8207182288169861, 0.8208560347557068, 0.8213599324226379, 0.8218153715133667, 0.8240017294883728, 0.8232011795043945, 0.8282665610313416, 0.8245061635971069, 0.8247063159942627, 0.8239333033561707, 0.8250355124473572, 0.8293848037719727, 0.8241156935691833, 0.8304463028907776, 0.8314948678016663, 0.8279232382774353, 0.8317888975143433, 0.8272255659103394, 0.8329845666885376]}\n",
            "Trial: Neurons = 50, Dropout Rate = 0, Batch Size = 256\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.3513 - loss: 1.3410 - val_accuracy: 0.5588 - val_loss: 1.0247\n",
            "Epoch 2/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5772 - loss: 0.9784 - val_accuracy: 0.5847 - val_loss: 0.9085\n",
            "Epoch 3/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5996 - loss: 0.8826 - val_accuracy: 0.5976 - val_loss: 0.8770\n",
            "Epoch 4/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6140 - loss: 0.8541 - val_accuracy: 0.6049 - val_loss: 0.8646\n",
            "Epoch 5/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6179 - loss: 0.8444 - val_accuracy: 0.6087 - val_loss: 0.8543\n",
            "Epoch 6/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6286 - loss: 0.8241 - val_accuracy: 0.6114 - val_loss: 0.8490\n",
            "Epoch 7/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6326 - loss: 0.8217 - val_accuracy: 0.6133 - val_loss: 0.8448\n",
            "Epoch 8/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6295 - loss: 0.8140 - val_accuracy: 0.6206 - val_loss: 0.8412\n",
            "Epoch 9/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6274 - loss: 0.8152 - val_accuracy: 0.6184 - val_loss: 0.8388\n",
            "Epoch 10/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6284 - loss: 0.8024 - val_accuracy: 0.6195 - val_loss: 0.8366\n",
            "Epoch 11/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6328 - loss: 0.8040 - val_accuracy: 0.6222 - val_loss: 0.8360\n",
            "Epoch 12/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6424 - loss: 0.7910 - val_accuracy: 0.6214 - val_loss: 0.8319\n",
            "Epoch 13/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6413 - loss: 0.7881 - val_accuracy: 0.6162 - val_loss: 0.8305\n",
            "Epoch 14/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6557 - loss: 0.7835 - val_accuracy: 0.6235 - val_loss: 0.8275\n",
            "Epoch 15/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6503 - loss: 0.7763 - val_accuracy: 0.6222 - val_loss: 0.8269\n",
            "Epoch 16/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6508 - loss: 0.7790 - val_accuracy: 0.6208 - val_loss: 0.8256\n",
            "Epoch 17/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6419 - loss: 0.7933 - val_accuracy: 0.6262 - val_loss: 0.8242\n",
            "Epoch 18/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6641 - loss: 0.7659 - val_accuracy: 0.6224 - val_loss: 0.8242\n",
            "Epoch 19/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6552 - loss: 0.7671 - val_accuracy: 0.6232 - val_loss: 0.8230\n",
            "Epoch 20/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6588 - loss: 0.7613 - val_accuracy: 0.6192 - val_loss: 0.8234\n",
            "Epoch 21/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6630 - loss: 0.7619 - val_accuracy: 0.6257 - val_loss: 0.8223\n",
            "Epoch 22/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6644 - loss: 0.7507 - val_accuracy: 0.6270 - val_loss: 0.8214\n",
            "Epoch 23/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6635 - loss: 0.7456 - val_accuracy: 0.6257 - val_loss: 0.8258\n",
            "Epoch 24/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6645 - loss: 0.7586 - val_accuracy: 0.6262 - val_loss: 0.8230\n",
            "Epoch 25/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6640 - loss: 0.7567 - val_accuracy: 0.6235 - val_loss: 0.8207\n",
            "Epoch 26/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6596 - loss: 0.7564 - val_accuracy: 0.6243 - val_loss: 0.8201\n",
            "Epoch 27/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6663 - loss: 0.7476 - val_accuracy: 0.6330 - val_loss: 0.8178\n",
            "Epoch 28/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6574 - loss: 0.7534 - val_accuracy: 0.6254 - val_loss: 0.8203\n",
            "Epoch 29/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6746 - loss: 0.7386 - val_accuracy: 0.6281 - val_loss: 0.8202\n",
            "Epoch 30/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6607 - loss: 0.7533 - val_accuracy: 0.6238 - val_loss: 0.8210\n",
            "Epoch 31/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6704 - loss: 0.7431 - val_accuracy: 0.6273 - val_loss: 0.8203\n",
            "Epoch 32/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6764 - loss: 0.7352 - val_accuracy: 0.6281 - val_loss: 0.8201\n",
            "Epoch 33/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6696 - loss: 0.7383 - val_accuracy: 0.6203 - val_loss: 0.8231\n",
            "Epoch 34/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6736 - loss: 0.7374 - val_accuracy: 0.6254 - val_loss: 0.8194\n",
            "Epoch 35/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6773 - loss: 0.7313 - val_accuracy: 0.6230 - val_loss: 0.8282\n",
            "Epoch 36/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6690 - loss: 0.7335 - val_accuracy: 0.6286 - val_loss: 0.8185\n",
            "Epoch 37/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6729 - loss: 0.7375 - val_accuracy: 0.6227 - val_loss: 0.8259\n",
            "Epoch 38/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6803 - loss: 0.7297 - val_accuracy: 0.6281 - val_loss: 0.8195\n",
            "Epoch 39/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6814 - loss: 0.7237 - val_accuracy: 0.6297 - val_loss: 0.8214\n",
            "Epoch 40/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6852 - loss: 0.7206 - val_accuracy: 0.6273 - val_loss: 0.8198\n",
            "Epoch 41/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6746 - loss: 0.7198 - val_accuracy: 0.6195 - val_loss: 0.8304\n",
            "Epoch 42/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6782 - loss: 0.7269 - val_accuracy: 0.6311 - val_loss: 0.8203\n",
            "Epoch 43/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6848 - loss: 0.7121 - val_accuracy: 0.6305 - val_loss: 0.8200\n",
            "Epoch 44/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6829 - loss: 0.7158 - val_accuracy: 0.6278 - val_loss: 0.8196\n",
            "Epoch 45/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6880 - loss: 0.7174 - val_accuracy: 0.6259 - val_loss: 0.8205\n",
            "Epoch 46/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6926 - loss: 0.7056 - val_accuracy: 0.6203 - val_loss: 0.8314\n",
            "Epoch 47/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6856 - loss: 0.7054 - val_accuracy: 0.6243 - val_loss: 0.8235\n",
            "Epoch 48/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6879 - loss: 0.7142 - val_accuracy: 0.6281 - val_loss: 0.8205\n",
            "Epoch 49/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6824 - loss: 0.7131 - val_accuracy: 0.6268 - val_loss: 0.8208\n",
            "Epoch 50/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6896 - loss: 0.7053 - val_accuracy: 0.6270 - val_loss: 0.8270\n",
            "History for model: {'accuracy': [0.445523202419281, 0.5820748209953308, 0.5999640226364136, 0.6127292513847351, 0.621179461479187, 0.6242358684539795, 0.6270226240158081, 0.6302589178085327, 0.632865846157074, 0.6351132392883301, 0.6362819075584412, 0.640956461429596, 0.6450017690658569, 0.6456310749053955, 0.6477885842323303, 0.651654064655304, 0.6532722115516663, 0.6549802422523499, 0.657227635383606, 0.656868040561676, 0.6590255498886108, 0.6586659550666809, 0.6587558388710022, 0.6594750285148621, 0.664958655834198, 0.6621719002723694, 0.667475700378418, 0.6652283072471619, 0.6681050062179565, 0.6676555275917053, 0.6689140796661377, 0.6713412404060364, 0.67268967628479, 0.6721503138542175, 0.6737684011459351, 0.6749370694160461, 0.674127995967865, 0.6761057376861572, 0.6762855052947998, 0.6793419718742371, 0.6711614727973938, 0.6783531308174133, 0.6773642301559448, 0.6806005239486694, 0.6814994812011719, 0.6855447888374329, 0.6806005239486694, 0.6844660043716431, 0.6865336298942566, 0.6861740350723267], 'loss': [1.2220712900161743, 0.9479013085365295, 0.8782764673233032, 0.8533030152320862, 0.8377062082290649, 0.8289849162101746, 0.8201964497566223, 0.8134115934371948, 0.809045135974884, 0.8025730848312378, 0.8007754683494568, 0.7954283952713013, 0.7906622886657715, 0.78774094581604, 0.7846631407737732, 0.7800549268722534, 0.7781093716621399, 0.7737260460853577, 0.7707034349441528, 0.7689154744148254, 0.7655547857284546, 0.763185977935791, 0.7614209055900574, 0.7595140933990479, 0.755744218826294, 0.7546377182006836, 0.7502595782279968, 0.7492280006408691, 0.746569037437439, 0.7457765936851501, 0.7428206205368042, 0.7410852313041687, 0.7383137941360474, 0.7377173900604248, 0.7357236742973328, 0.7329040169715881, 0.7327303886413574, 0.7295242547988892, 0.7274911999702454, 0.7264503240585327, 0.7271130681037903, 0.726212739944458, 0.7211235165596008, 0.7187409996986389, 0.7186444401741028, 0.7171444892883301, 0.7167445421218872, 0.7139059901237488, 0.7121054530143738, 0.7100614309310913], 'val_accuracy': [0.5587918162345886, 0.5846817493438721, 0.5976267457008362, 0.604908287525177, 0.6086839437484741, 0.6113808155059814, 0.6132686138153076, 0.6205501556396484, 0.6183926463127136, 0.6194714307785034, 0.6221683025360107, 0.6213592290878296, 0.6162351965904236, 0.6235167384147644, 0.6221683025360107, 0.6208198666572571, 0.6262136101722717, 0.6224379539489746, 0.6232470273971558, 0.6192017197608948, 0.6256741881370544, 0.6270226240158081, 0.6256741881370544, 0.6262136101722717, 0.6235167384147644, 0.6243258118629456, 0.63295578956604, 0.6254045367240906, 0.6281014084815979, 0.6237863898277283, 0.6272923350334167, 0.6281014084815979, 0.6202805042266846, 0.6254045367240906, 0.6229773759841919, 0.6286407709121704, 0.6227076649665833, 0.6281014084815979, 0.6297194957733154, 0.6272923350334167, 0.6194714307785034, 0.6310679316520691, 0.6305285692214966, 0.6278316974639893, 0.6259438991546631, 0.6202805042266846, 0.6243258118629456, 0.6281014084815979, 0.6267529726028442, 0.6270226240158081], 'val_loss': [1.0246567726135254, 0.9085239171981812, 0.8770285248756409, 0.8645862340927124, 0.8543441891670227, 0.8489506244659424, 0.8447674512863159, 0.8412052989006042, 0.8388494849205017, 0.8365820050239563, 0.836047887802124, 0.8319126963615417, 0.8304511308670044, 0.8275372385978699, 0.8268534541130066, 0.8255537152290344, 0.8242358565330505, 0.8242264986038208, 0.8229993581771851, 0.8233510851860046, 0.822268009185791, 0.8214457631111145, 0.8257747292518616, 0.8229684233665466, 0.8206807971000671, 0.8200965523719788, 0.817759096622467, 0.8203177452087402, 0.8202208280563354, 0.8209956884384155, 0.8202723860740662, 0.8201421499252319, 0.823056161403656, 0.8193970322608948, 0.8281959891319275, 0.8185248970985413, 0.8258691430091858, 0.8195246458053589, 0.8213940262794495, 0.8197997212409973, 0.830352246761322, 0.820301353931427, 0.8200010061264038, 0.8195701241493225, 0.8205328583717346, 0.8314347267150879, 0.8234782218933105, 0.8205094933509827, 0.8207772970199585, 0.8270304203033447]}\n",
            "Trial: Neurons = 50, Dropout Rate = 0.1, Batch Size = 64\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4923 - loss: 1.1347 - val_accuracy: 0.6025 - val_loss: 0.8954\n",
            "Epoch 2/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5967 - loss: 0.8974 - val_accuracy: 0.6098 - val_loss: 0.8642\n",
            "Epoch 3/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6062 - loss: 0.8675 - val_accuracy: 0.6170 - val_loss: 0.8485\n",
            "Epoch 4/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6043 - loss: 0.8496 - val_accuracy: 0.6130 - val_loss: 0.8438\n",
            "Epoch 5/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6298 - loss: 0.8342 - val_accuracy: 0.6219 - val_loss: 0.8384\n",
            "Epoch 6/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6366 - loss: 0.8147 - val_accuracy: 0.6208 - val_loss: 0.8346\n",
            "Epoch 7/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6330 - loss: 0.8113 - val_accuracy: 0.6197 - val_loss: 0.8302\n",
            "Epoch 8/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6348 - loss: 0.8090 - val_accuracy: 0.6257 - val_loss: 0.8318\n",
            "Epoch 9/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6399 - loss: 0.8044 - val_accuracy: 0.6278 - val_loss: 0.8223\n",
            "Epoch 10/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6401 - loss: 0.7997 - val_accuracy: 0.6321 - val_loss: 0.8206\n",
            "Epoch 11/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6543 - loss: 0.7807 - val_accuracy: 0.6294 - val_loss: 0.8212\n",
            "Epoch 12/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6399 - loss: 0.7907 - val_accuracy: 0.6343 - val_loss: 0.8200\n",
            "Epoch 13/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6494 - loss: 0.7807 - val_accuracy: 0.6286 - val_loss: 0.8183\n",
            "Epoch 14/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6488 - loss: 0.7760 - val_accuracy: 0.6303 - val_loss: 0.8177\n",
            "Epoch 15/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6543 - loss: 0.7667 - val_accuracy: 0.6324 - val_loss: 0.8201\n",
            "Epoch 16/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6543 - loss: 0.7688 - val_accuracy: 0.6373 - val_loss: 0.8172\n",
            "Epoch 17/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6481 - loss: 0.7557 - val_accuracy: 0.6268 - val_loss: 0.8217\n",
            "Epoch 18/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6675 - loss: 0.7493 - val_accuracy: 0.6303 - val_loss: 0.8173\n",
            "Epoch 19/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6624 - loss: 0.7548 - val_accuracy: 0.6319 - val_loss: 0.8207\n",
            "Epoch 20/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6615 - loss: 0.7490 - val_accuracy: 0.6405 - val_loss: 0.8133\n",
            "Epoch 21/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6637 - loss: 0.7478 - val_accuracy: 0.6397 - val_loss: 0.8144\n",
            "Epoch 22/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6646 - loss: 0.7403 - val_accuracy: 0.6378 - val_loss: 0.8158\n",
            "Epoch 23/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6674 - loss: 0.7354 - val_accuracy: 0.6383 - val_loss: 0.8160\n",
            "Epoch 24/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6675 - loss: 0.7371 - val_accuracy: 0.6370 - val_loss: 0.8215\n",
            "Epoch 25/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6709 - loss: 0.7336 - val_accuracy: 0.6278 - val_loss: 0.8234\n",
            "Epoch 26/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6734 - loss: 0.7288 - val_accuracy: 0.6340 - val_loss: 0.8253\n",
            "Epoch 27/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6753 - loss: 0.7297 - val_accuracy: 0.6308 - val_loss: 0.8231\n",
            "Epoch 28/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6738 - loss: 0.7215 - val_accuracy: 0.6251 - val_loss: 0.8267\n",
            "Epoch 29/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6706 - loss: 0.7321 - val_accuracy: 0.6354 - val_loss: 0.8213\n",
            "Epoch 30/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6740 - loss: 0.7230 - val_accuracy: 0.6392 - val_loss: 0.8215\n",
            "Epoch 31/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6788 - loss: 0.7253 - val_accuracy: 0.6383 - val_loss: 0.8225\n",
            "Epoch 32/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6807 - loss: 0.7172 - val_accuracy: 0.6281 - val_loss: 0.8256\n",
            "Epoch 33/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6735 - loss: 0.7073 - val_accuracy: 0.6330 - val_loss: 0.8258\n",
            "Epoch 34/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6734 - loss: 0.7180 - val_accuracy: 0.6410 - val_loss: 0.8232\n",
            "Epoch 35/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6841 - loss: 0.7076 - val_accuracy: 0.6224 - val_loss: 0.8318\n",
            "Epoch 36/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6828 - loss: 0.7076 - val_accuracy: 0.6357 - val_loss: 0.8266\n",
            "Epoch 37/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6826 - loss: 0.7044 - val_accuracy: 0.6340 - val_loss: 0.8385\n",
            "Epoch 38/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6861 - loss: 0.7069 - val_accuracy: 0.6278 - val_loss: 0.8346\n",
            "Epoch 39/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6812 - loss: 0.7091 - val_accuracy: 0.6340 - val_loss: 0.8331\n",
            "Epoch 40/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6804 - loss: 0.7085 - val_accuracy: 0.6348 - val_loss: 0.8335\n",
            "Epoch 41/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6904 - loss: 0.6966 - val_accuracy: 0.6303 - val_loss: 0.8331\n",
            "Epoch 42/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6863 - loss: 0.6986 - val_accuracy: 0.6273 - val_loss: 0.8358\n",
            "Epoch 43/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6898 - loss: 0.6955 - val_accuracy: 0.6365 - val_loss: 0.8356\n",
            "Epoch 44/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6847 - loss: 0.6998 - val_accuracy: 0.6311 - val_loss: 0.8380\n",
            "Epoch 45/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6852 - loss: 0.6926 - val_accuracy: 0.6216 - val_loss: 0.8446\n",
            "Epoch 46/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6920 - loss: 0.6967 - val_accuracy: 0.6297 - val_loss: 0.8369\n",
            "Epoch 47/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6907 - loss: 0.6904 - val_accuracy: 0.6359 - val_loss: 0.8371\n",
            "Epoch 48/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6982 - loss: 0.6847 - val_accuracy: 0.6357 - val_loss: 0.8405\n",
            "Epoch 49/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6954 - loss: 0.6850 - val_accuracy: 0.6330 - val_loss: 0.8431\n",
            "Epoch 50/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6922 - loss: 0.6919 - val_accuracy: 0.6289 - val_loss: 0.8441\n",
            "History for model: {'accuracy': [0.5393743515014648, 0.5947501063346863, 0.6087738275527954, 0.6099424958229065, 0.6269327402114868, 0.6264832615852356, 0.6295397281646729, 0.6324163675308228, 0.6356526613235474, 0.6377202272415161, 0.6414059400558472, 0.6446422338485718, 0.6466199159622192, 0.6510248184204102, 0.650215744972229, 0.6518338918685913, 0.6500359773635864, 0.6581265926361084, 0.6586659550666809, 0.6580366492271423, 0.6622617840766907, 0.6610931158065796, 0.6656777858734131, 0.6640596985816956, 0.666037380695343, 0.6721503138542175, 0.6708018779754639, 0.6709816455841064, 0.6697230935096741, 0.6713412404060364, 0.6733189225196838, 0.6763753890991211, 0.6729593873023987, 0.6754764318466187, 0.6790722608566284, 0.6786227822303772, 0.6819489598274231, 0.6826680898666382, 0.6779935359954834, 0.6817691326141357, 0.6841963529586792, 0.6815893650054932, 0.688870906829834, 0.6873427033424377, 0.6843761205673218, 0.6917475461959839, 0.6869831085205078, 0.6937252879142761, 0.6924667358398438, 0.6930060982704163], 'loss': [1.0271342992782593, 0.8883533477783203, 0.8619560599327087, 0.8438844084739685, 0.832809567451477, 0.8237074017524719, 0.814688503742218, 0.8094080686569214, 0.8047779202461243, 0.7992388010025024, 0.7896679043769836, 0.787071168422699, 0.7821426391601562, 0.7767864465713501, 0.7720566987991333, 0.7692810297012329, 0.7639471888542175, 0.7590757608413696, 0.757857620716095, 0.7530171275138855, 0.7511122226715088, 0.7457826137542725, 0.7451660633087158, 0.742059588432312, 0.737697184085846, 0.7346919178962708, 0.7324635982513428, 0.7314059734344482, 0.7284573316574097, 0.7262751460075378, 0.7243033051490784, 0.7224608659744263, 0.7167729139328003, 0.7155817151069641, 0.7175017595291138, 0.7132514119148254, 0.7094843983650208, 0.7092447280883789, 0.7046033143997192, 0.7019128799438477, 0.704301118850708, 0.701596736907959, 0.6971443295478821, 0.6950839757919312, 0.6983468532562256, 0.69239342212677, 0.6924042701721191, 0.6882904171943665, 0.6866486668586731, 0.6869113445281982], 'val_accuracy': [0.6024811267852783, 0.6097626686096191, 0.61704421043396, 0.612998902797699, 0.6218985915184021, 0.6208198666572571, 0.6197410821914673, 0.6256741881370544, 0.6278316974639893, 0.6321467161178589, 0.6294498443603516, 0.6343042254447937, 0.6286407709121704, 0.6302589178085327, 0.6324163675308228, 0.6372707486152649, 0.6267529726028442, 0.6302589178085327, 0.6318770051002502, 0.6405069828033447, 0.6396979689598083, 0.6378101110458374, 0.6383495330810547, 0.637001097202301, 0.6278316974639893, 0.6340345144271851, 0.6307982802391052, 0.6251348257064819, 0.6353829503059387, 0.6391585469245911, 0.6383495330810547, 0.6281014084815979, 0.63295578956604, 0.641046404838562, 0.6224379539489746, 0.6356526613235474, 0.6340345144271851, 0.6278316974639893, 0.6340345144271851, 0.6348435878753662, 0.6302589178085327, 0.6272923350334167, 0.6364616751670837, 0.6310679316520691, 0.6216289401054382, 0.6297194957733154, 0.6359223127365112, 0.6356526613235474, 0.63295578956604, 0.628910481929779], 'val_loss': [0.8953943848609924, 0.8641896843910217, 0.8484888076782227, 0.8438445925712585, 0.8383898735046387, 0.8345552682876587, 0.830165445804596, 0.8317577838897705, 0.8222794532775879, 0.8205652832984924, 0.8211619257926941, 0.8200072646141052, 0.8183161020278931, 0.8176685571670532, 0.8200840950012207, 0.8172205686569214, 0.8216986060142517, 0.817318856716156, 0.820659339427948, 0.813337504863739, 0.8143712282180786, 0.8158281445503235, 0.815970778465271, 0.8214898109436035, 0.8234298825263977, 0.8253473043441772, 0.8231250047683716, 0.8266671895980835, 0.8212874531745911, 0.82154381275177, 0.8225182294845581, 0.8255949020385742, 0.8257662057876587, 0.8232048749923706, 0.8318036198616028, 0.8266218900680542, 0.8384645581245422, 0.8346139788627625, 0.833099901676178, 0.833493173122406, 0.8330983519554138, 0.8358219265937805, 0.8355573415756226, 0.8380328416824341, 0.8445641994476318, 0.8369173407554626, 0.8371323943138123, 0.8404741883277893, 0.8430579900741577, 0.8441324234008789]}\n",
            "Trial: Neurons = 50, Dropout Rate = 0.1, Batch Size = 128\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4220 - loss: 1.2359 - val_accuracy: 0.5806 - val_loss: 0.9307\n",
            "Epoch 2/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5943 - loss: 0.9138 - val_accuracy: 0.5968 - val_loss: 0.8755\n",
            "Epoch 3/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6033 - loss: 0.8657 - val_accuracy: 0.6011 - val_loss: 0.8575\n",
            "Epoch 4/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6092 - loss: 0.8483 - val_accuracy: 0.6119 - val_loss: 0.8497\n",
            "Epoch 5/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6188 - loss: 0.8361 - val_accuracy: 0.6149 - val_loss: 0.8459\n",
            "Epoch 6/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6200 - loss: 0.8283 - val_accuracy: 0.6168 - val_loss: 0.8400\n",
            "Epoch 7/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6202 - loss: 0.8247 - val_accuracy: 0.6189 - val_loss: 0.8358\n",
            "Epoch 8/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6315 - loss: 0.8138 - val_accuracy: 0.6227 - val_loss: 0.8321\n",
            "Epoch 9/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6262 - loss: 0.8108 - val_accuracy: 0.6206 - val_loss: 0.8295\n",
            "Epoch 10/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6368 - loss: 0.7974 - val_accuracy: 0.6273 - val_loss: 0.8290\n",
            "Epoch 11/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6372 - loss: 0.7961 - val_accuracy: 0.6254 - val_loss: 0.8264\n",
            "Epoch 12/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6507 - loss: 0.7770 - val_accuracy: 0.6303 - val_loss: 0.8240\n",
            "Epoch 13/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6467 - loss: 0.7836 - val_accuracy: 0.6297 - val_loss: 0.8226\n",
            "Epoch 14/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6479 - loss: 0.7826 - val_accuracy: 0.6230 - val_loss: 0.8284\n",
            "Epoch 15/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6467 - loss: 0.7803 - val_accuracy: 0.6321 - val_loss: 0.8204\n",
            "Epoch 16/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6446 - loss: 0.7770 - val_accuracy: 0.6351 - val_loss: 0.8188\n",
            "Epoch 17/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6481 - loss: 0.7837 - val_accuracy: 0.6365 - val_loss: 0.8185\n",
            "Epoch 18/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6483 - loss: 0.7796 - val_accuracy: 0.6346 - val_loss: 0.8165\n",
            "Epoch 19/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6489 - loss: 0.7749 - val_accuracy: 0.6319 - val_loss: 0.8201\n",
            "Epoch 20/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6642 - loss: 0.7590 - val_accuracy: 0.6367 - val_loss: 0.8153\n",
            "Epoch 21/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6510 - loss: 0.7639 - val_accuracy: 0.6305 - val_loss: 0.8208\n",
            "Epoch 22/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6609 - loss: 0.7534 - val_accuracy: 0.6335 - val_loss: 0.8167\n",
            "Epoch 23/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6574 - loss: 0.7533 - val_accuracy: 0.6365 - val_loss: 0.8167\n",
            "Epoch 24/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6553 - loss: 0.7570 - val_accuracy: 0.6351 - val_loss: 0.8141\n",
            "Epoch 25/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6650 - loss: 0.7471 - val_accuracy: 0.6365 - val_loss: 0.8132\n",
            "Epoch 26/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6584 - loss: 0.7560 - val_accuracy: 0.6351 - val_loss: 0.8170\n",
            "Epoch 27/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6608 - loss: 0.7466 - val_accuracy: 0.6359 - val_loss: 0.8154\n",
            "Epoch 28/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6644 - loss: 0.7476 - val_accuracy: 0.6346 - val_loss: 0.8151\n",
            "Epoch 29/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6680 - loss: 0.7376 - val_accuracy: 0.6348 - val_loss: 0.8171\n",
            "Epoch 30/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6599 - loss: 0.7424 - val_accuracy: 0.6394 - val_loss: 0.8121\n",
            "Epoch 31/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6687 - loss: 0.7414 - val_accuracy: 0.6373 - val_loss: 0.8135\n",
            "Epoch 32/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6726 - loss: 0.7386 - val_accuracy: 0.6419 - val_loss: 0.8144\n",
            "Epoch 33/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6697 - loss: 0.7393 - val_accuracy: 0.6413 - val_loss: 0.8110\n",
            "Epoch 34/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6663 - loss: 0.7377 - val_accuracy: 0.6359 - val_loss: 0.8130\n",
            "Epoch 35/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6682 - loss: 0.7301 - val_accuracy: 0.6311 - val_loss: 0.8161\n",
            "Epoch 36/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6720 - loss: 0.7234 - val_accuracy: 0.6383 - val_loss: 0.8144\n",
            "Epoch 37/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6722 - loss: 0.7227 - val_accuracy: 0.6378 - val_loss: 0.8146\n",
            "Epoch 38/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6771 - loss: 0.7222 - val_accuracy: 0.6386 - val_loss: 0.8139\n",
            "Epoch 39/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6790 - loss: 0.7223 - val_accuracy: 0.6381 - val_loss: 0.8146\n",
            "Epoch 40/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6747 - loss: 0.7220 - val_accuracy: 0.6402 - val_loss: 0.8156\n",
            "Epoch 41/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6728 - loss: 0.7292 - val_accuracy: 0.6392 - val_loss: 0.8196\n",
            "Epoch 42/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6770 - loss: 0.7218 - val_accuracy: 0.6413 - val_loss: 0.8158\n",
            "Epoch 43/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6874 - loss: 0.7099 - val_accuracy: 0.6270 - val_loss: 0.8208\n",
            "Epoch 44/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6815 - loss: 0.7065 - val_accuracy: 0.6370 - val_loss: 0.8165\n",
            "Epoch 45/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6848 - loss: 0.7083 - val_accuracy: 0.6370 - val_loss: 0.8212\n",
            "Epoch 46/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6796 - loss: 0.7058 - val_accuracy: 0.6386 - val_loss: 0.8171\n",
            "Epoch 47/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6887 - loss: 0.7032 - val_accuracy: 0.6357 - val_loss: 0.8248\n",
            "Epoch 48/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6785 - loss: 0.7149 - val_accuracy: 0.6383 - val_loss: 0.8199\n",
            "Epoch 49/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6893 - loss: 0.7022 - val_accuracy: 0.6375 - val_loss: 0.8208\n",
            "Epoch 50/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6816 - loss: 0.7046 - val_accuracy: 0.6335 - val_loss: 0.8251\n",
            "History for model: {'accuracy': [0.5048543810844421, 0.5885472893714905, 0.5981661081314087, 0.6050881147384644, 0.6151564121246338, 0.6175836324691772, 0.6251348257064819, 0.6270226240158081, 0.6307083964347839, 0.6306184530258179, 0.6335850358009338, 0.640956461429596, 0.6441927552223206, 0.6443725228309631, 0.6454512476921082, 0.6462603211402893, 0.6496763825416565, 0.6496763825416565, 0.6495864987373352, 0.659385085105896, 0.6524631381034851, 0.6546206474304199, 0.6588457226753235, 0.6598345637321472, 0.661992073059082, 0.6601941585540771, 0.6603739857673645, 0.6646889448165894, 0.6689140796661377, 0.6669363379478455, 0.668644368648529, 0.6711614727973938, 0.6693635582923889, 0.6706220507621765, 0.6699928045272827, 0.6748471856117249, 0.6698130369186401, 0.6752966642379761, 0.6769147515296936, 0.6745774745941162, 0.6757461428642273, 0.677813708782196, 0.6794318556785583, 0.679701566696167, 0.677813708782196, 0.6791621446609497, 0.6860841512680054, 0.6789823770523071, 0.6855447888374329, 0.6806005239486694], 'loss': [1.1092294454574585, 0.903815507888794, 0.8658313155174255, 0.851203978061676, 0.8358704447746277, 0.8276461958885193, 0.8200386762619019, 0.8144429326057434, 0.808871865272522, 0.8037315011024475, 0.7995256781578064, 0.7951080799102783, 0.7886582612991333, 0.7875363826751709, 0.7836899757385254, 0.7795807123184204, 0.7763688564300537, 0.7730020880699158, 0.7703155279159546, 0.7669630646705627, 0.7650610208511353, 0.7608651518821716, 0.7582645416259766, 0.7561601996421814, 0.7548232674598694, 0.7514050602912903, 0.7507398724555969, 0.7476286292076111, 0.7440316081047058, 0.7427244782447815, 0.7420138716697693, 0.7387109994888306, 0.7360386252403259, 0.7322562336921692, 0.7337217330932617, 0.7265968918800354, 0.731217622756958, 0.7240839004516602, 0.7236180901527405, 0.7242975234985352, 0.7234153747558594, 0.7219874858856201, 0.7206907272338867, 0.7169852256774902, 0.7163032293319702, 0.7123624086380005, 0.7107563018798828, 0.7117838859558105, 0.7043624520301819, 0.7066511511802673], 'val_accuracy': [0.5806364417076111, 0.596817672252655, 0.6011326909065247, 0.611920177936554, 0.6148867607116699, 0.6167745590209961, 0.6189320683479309, 0.6227076649665833, 0.6205501556396484, 0.6272923350334167, 0.6254045367240906, 0.6302589178085327, 0.6297194957733154, 0.6229773759841919, 0.6321467161178589, 0.6351132392883301, 0.6364616751670837, 0.6345738768577576, 0.6318770051002502, 0.6367313861846924, 0.6305285692214966, 0.6334951519966125, 0.6364616751670837, 0.6351132392883301, 0.6364616751670837, 0.6351132392883301, 0.6359223127365112, 0.6345738768577576, 0.6348435878753662, 0.6394282579421997, 0.6372707486152649, 0.6418554186820984, 0.6413160562515259, 0.6359223127365112, 0.6310679316520691, 0.6383495330810547, 0.6378101110458374, 0.6386191844940186, 0.638079822063446, 0.6402373313903809, 0.6391585469245911, 0.6413160562515259, 0.6270226240158081, 0.637001097202301, 0.637001097202301, 0.6386191844940186, 0.6356526613235474, 0.6383495330810547, 0.6375404596328735, 0.6334951519966125], 'val_loss': [0.930661141872406, 0.8754975199699402, 0.8574686050415039, 0.8497421741485596, 0.845867395401001, 0.8400002121925354, 0.8358457088470459, 0.8320944309234619, 0.8295485973358154, 0.8289820551872253, 0.8264099359512329, 0.8240386843681335, 0.8226029276847839, 0.8283777832984924, 0.8203993439674377, 0.8187970519065857, 0.818480372428894, 0.8165221214294434, 0.8201202154159546, 0.8153477907180786, 0.8208014369010925, 0.8166693449020386, 0.8167332410812378, 0.8141314387321472, 0.8131903409957886, 0.8170152902603149, 0.8153848052024841, 0.8151346445083618, 0.8170746564865112, 0.8120547533035278, 0.8135499954223633, 0.8144468665122986, 0.81104576587677, 0.8129582405090332, 0.8161119222640991, 0.8144292235374451, 0.8145535588264465, 0.8139318823814392, 0.8146350979804993, 0.8155871033668518, 0.8196405172348022, 0.8158486485481262, 0.8207622766494751, 0.8165114521980286, 0.821215808391571, 0.8171162009239197, 0.8248068690299988, 0.8198633790016174, 0.820767343044281, 0.8251081109046936]}\n",
            "Trial: Neurons = 50, Dropout Rate = 0.1, Batch Size = 256\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.3618 - loss: 1.3861 - val_accuracy: 0.5612 - val_loss: 1.0600\n",
            "Epoch 2/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5571 - loss: 1.0409 - val_accuracy: 0.5936 - val_loss: 0.9185\n",
            "Epoch 3/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5885 - loss: 0.9137 - val_accuracy: 0.6028 - val_loss: 0.8785\n",
            "Epoch 4/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5982 - loss: 0.8764 - val_accuracy: 0.6036 - val_loss: 0.8681\n",
            "Epoch 5/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6066 - loss: 0.8653 - val_accuracy: 0.6098 - val_loss: 0.8548\n",
            "Epoch 6/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6081 - loss: 0.8364 - val_accuracy: 0.6170 - val_loss: 0.8508\n",
            "Epoch 7/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6163 - loss: 0.8462 - val_accuracy: 0.6152 - val_loss: 0.8458\n",
            "Epoch 8/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6166 - loss: 0.8361 - val_accuracy: 0.6216 - val_loss: 0.8408\n",
            "Epoch 9/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6180 - loss: 0.8290 - val_accuracy: 0.6232 - val_loss: 0.8386\n",
            "Epoch 10/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6223 - loss: 0.8193 - val_accuracy: 0.6230 - val_loss: 0.8354\n",
            "Epoch 11/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6327 - loss: 0.8121 - val_accuracy: 0.6311 - val_loss: 0.8314\n",
            "Epoch 12/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6320 - loss: 0.8095 - val_accuracy: 0.6246 - val_loss: 0.8313\n",
            "Epoch 13/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6375 - loss: 0.8030 - val_accuracy: 0.6294 - val_loss: 0.8289\n",
            "Epoch 14/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6259 - loss: 0.8092 - val_accuracy: 0.6276 - val_loss: 0.8279\n",
            "Epoch 15/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6304 - loss: 0.8045 - val_accuracy: 0.6316 - val_loss: 0.8246\n",
            "Epoch 16/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6350 - loss: 0.8007 - val_accuracy: 0.6324 - val_loss: 0.8224\n",
            "Epoch 17/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6412 - loss: 0.7911 - val_accuracy: 0.6321 - val_loss: 0.8229\n",
            "Epoch 18/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6427 - loss: 0.7909 - val_accuracy: 0.6278 - val_loss: 0.8217\n",
            "Epoch 19/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6482 - loss: 0.7831 - val_accuracy: 0.6308 - val_loss: 0.8205\n",
            "Epoch 20/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6455 - loss: 0.7887 - val_accuracy: 0.6335 - val_loss: 0.8194\n",
            "Epoch 21/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6489 - loss: 0.7871 - val_accuracy: 0.6332 - val_loss: 0.8185\n",
            "Epoch 22/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6525 - loss: 0.7821 - val_accuracy: 0.6335 - val_loss: 0.8195\n",
            "Epoch 23/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6546 - loss: 0.7696 - val_accuracy: 0.6289 - val_loss: 0.8168\n",
            "Epoch 24/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6600 - loss: 0.7640 - val_accuracy: 0.6294 - val_loss: 0.8188\n",
            "Epoch 25/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6513 - loss: 0.7669 - val_accuracy: 0.6300 - val_loss: 0.8174\n",
            "Epoch 26/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6573 - loss: 0.7717 - val_accuracy: 0.6246 - val_loss: 0.8166\n",
            "Epoch 27/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6620 - loss: 0.7636 - val_accuracy: 0.6338 - val_loss: 0.8163\n",
            "Epoch 28/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6574 - loss: 0.7773 - val_accuracy: 0.6311 - val_loss: 0.8145\n",
            "Epoch 29/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6627 - loss: 0.7714 - val_accuracy: 0.6270 - val_loss: 0.8155\n",
            "Epoch 30/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6654 - loss: 0.7610 - val_accuracy: 0.6338 - val_loss: 0.8124\n",
            "Epoch 31/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6691 - loss: 0.7469 - val_accuracy: 0.6324 - val_loss: 0.8128\n",
            "Epoch 32/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6603 - loss: 0.7536 - val_accuracy: 0.6378 - val_loss: 0.8134\n",
            "Epoch 33/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6564 - loss: 0.7545 - val_accuracy: 0.6278 - val_loss: 0.8139\n",
            "Epoch 34/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6736 - loss: 0.7357 - val_accuracy: 0.6338 - val_loss: 0.8116\n",
            "Epoch 35/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6715 - loss: 0.7494 - val_accuracy: 0.6348 - val_loss: 0.8131\n",
            "Epoch 36/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6701 - loss: 0.7426 - val_accuracy: 0.6378 - val_loss: 0.8110\n",
            "Epoch 37/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6715 - loss: 0.7326 - val_accuracy: 0.6351 - val_loss: 0.8124\n",
            "Epoch 38/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6716 - loss: 0.7373 - val_accuracy: 0.6351 - val_loss: 0.8108\n",
            "Epoch 39/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6714 - loss: 0.7394 - val_accuracy: 0.6408 - val_loss: 0.8107\n",
            "Epoch 40/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6655 - loss: 0.7441 - val_accuracy: 0.6330 - val_loss: 0.8105\n",
            "Epoch 41/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6720 - loss: 0.7380 - val_accuracy: 0.6305 - val_loss: 0.8170\n",
            "Epoch 42/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6768 - loss: 0.7316 - val_accuracy: 0.6397 - val_loss: 0.8112\n",
            "Epoch 43/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6709 - loss: 0.7428 - val_accuracy: 0.6359 - val_loss: 0.8096\n",
            "Epoch 44/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6738 - loss: 0.7320 - val_accuracy: 0.6400 - val_loss: 0.8107\n",
            "Epoch 45/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6729 - loss: 0.7340 - val_accuracy: 0.6351 - val_loss: 0.8159\n",
            "Epoch 46/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6725 - loss: 0.7380 - val_accuracy: 0.6373 - val_loss: 0.8109\n",
            "Epoch 47/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6779 - loss: 0.7170 - val_accuracy: 0.6346 - val_loss: 0.8113\n",
            "Epoch 48/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6832 - loss: 0.7177 - val_accuracy: 0.6354 - val_loss: 0.8121\n",
            "Epoch 49/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6744 - loss: 0.7270 - val_accuracy: 0.6340 - val_loss: 0.8138\n",
            "Epoch 50/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6747 - loss: 0.7279 - val_accuracy: 0.6346 - val_loss: 0.8132\n",
            "History for model: {'accuracy': [0.4489392340183258, 0.5693994760513306, 0.5879180431365967, 0.5991550087928772, 0.6060769557952881, 0.6045487523078918, 0.6121898889541626, 0.6183926463127136, 0.6212693452835083, 0.6245055794715881, 0.6320568323135376, 0.63295578956604, 0.6335850358009338, 0.6340345144271851, 0.6357425451278687, 0.639518141746521, 0.6403272151947021, 0.6422150135040283, 0.6403272151947021, 0.6498561501502991, 0.6476986408233643, 0.6535418629646301, 0.6486875414848328, 0.6512045860290527, 0.6514742970466614, 0.6550701260566711, 0.6586659550666809, 0.6556993722915649, 0.6608234643936157, 0.6633405089378357, 0.6619021892547607, 0.6606436371803284, 0.6596547961235046, 0.6666666865348816, 0.6672959327697754, 0.6621719002723694, 0.6671161651611328, 0.6663070917129517, 0.6679251790046692, 0.6691837310791016, 0.6692736148834229, 0.6717907190322876, 0.6699928045272827, 0.6752067804336548, 0.6753865480422974, 0.6731391549110413, 0.6752966642379761, 0.6748471856117249, 0.6729593873023987, 0.6769147515296936], 'loss': [1.2648444175720215, 0.9992694854736328, 0.9083982706069946, 0.8797100186347961, 0.8598315715789795, 0.8497937917709351, 0.8424253463745117, 0.832992434501648, 0.8277456760406494, 0.8214754462242126, 0.8134562969207764, 0.8104638457298279, 0.8087785243988037, 0.8026719689369202, 0.8010868430137634, 0.7957898378372192, 0.7939162254333496, 0.7899556756019592, 0.7897828817367554, 0.784242570400238, 0.7812438607215881, 0.7783887982368469, 0.7778804302215576, 0.7738580703735352, 0.7737557291984558, 0.7719801068305969, 0.7644661068916321, 0.7668332457542419, 0.7610595226287842, 0.7624750733375549, 0.7601024508476257, 0.7571902275085449, 0.7576112747192383, 0.7511703968048096, 0.7518908977508545, 0.7492048144340515, 0.7479192018508911, 0.7457577586174011, 0.7450231909751892, 0.7436699867248535, 0.7416912913322449, 0.7378560900688171, 0.7385215163230896, 0.7361518740653992, 0.7335432767868042, 0.7347522974014282, 0.7306042313575745, 0.7321632504463196, 0.7294974327087402, 0.7277273535728455], 'val_accuracy': [0.5612189769744873, 0.5935814380645752, 0.602750837802887, 0.6035598516464233, 0.6097626686096191, 0.61704421043396, 0.6151564121246338, 0.6216289401054382, 0.6232470273971558, 0.6229773759841919, 0.6310679316520691, 0.6245954632759094, 0.6294498443603516, 0.6275620460510254, 0.6316073536872864, 0.6324163675308228, 0.6321467161178589, 0.6278316974639893, 0.6307982802391052, 0.6334951519966125, 0.6332254409790039, 0.6334951519966125, 0.628910481929779, 0.6294498443603516, 0.6299892067909241, 0.6245954632759094, 0.6337648034095764, 0.6310679316520691, 0.6270226240158081, 0.6337648034095764, 0.6324163675308228, 0.6378101110458374, 0.6278316974639893, 0.6337648034095764, 0.6348435878753662, 0.6378101110458374, 0.6351132392883301, 0.6351132392883301, 0.6407766938209534, 0.63295578956604, 0.6305285692214966, 0.6396979689598083, 0.6359223127365112, 0.6399676203727722, 0.6351132392883301, 0.6372707486152649, 0.6345738768577576, 0.6353829503059387, 0.6340345144271851, 0.6345738768577576], 'val_loss': [1.060049057006836, 0.9185147285461426, 0.8784540295600891, 0.868108868598938, 0.8548154234886169, 0.8508410453796387, 0.8457881212234497, 0.8408458828926086, 0.8385676741600037, 0.8353660702705383, 0.8314452171325684, 0.8312782645225525, 0.8289186954498291, 0.8279315829277039, 0.8245691657066345, 0.8223825097084045, 0.8229108452796936, 0.8217121362686157, 0.8205033540725708, 0.819357693195343, 0.8185276389122009, 0.8194953203201294, 0.8168351054191589, 0.8187798261642456, 0.8173776865005493, 0.8166345357894897, 0.8163039684295654, 0.8145033121109009, 0.8154792189598083, 0.812427818775177, 0.8128232955932617, 0.8134171366691589, 0.8138994574546814, 0.8115523457527161, 0.813133180141449, 0.8110486268997192, 0.8124268054962158, 0.8107753992080688, 0.810678243637085, 0.8104773759841919, 0.8169988989830017, 0.8112327456474304, 0.8095807433128357, 0.8107208013534546, 0.8158932328224182, 0.8108996152877808, 0.8112988471984863, 0.8121462464332581, 0.8138293027877808, 0.8131642937660217]}\n",
            "Trial: Neurons = 50, Dropout Rate = 0.5, Batch Size = 64\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.3877 - loss: 1.3021 - val_accuracy: 0.5804 - val_loss: 0.9374\n",
            "Epoch 2/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5483 - loss: 0.9738 - val_accuracy: 0.5971 - val_loss: 0.8863\n",
            "Epoch 3/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5611 - loss: 0.9371 - val_accuracy: 0.6049 - val_loss: 0.8721\n",
            "Epoch 4/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5791 - loss: 0.8984 - val_accuracy: 0.6095 - val_loss: 0.8545\n",
            "Epoch 5/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5905 - loss: 0.8829 - val_accuracy: 0.6192 - val_loss: 0.8454\n",
            "Epoch 6/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6068 - loss: 0.8734 - val_accuracy: 0.6219 - val_loss: 0.8397\n",
            "Epoch 7/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6090 - loss: 0.8616 - val_accuracy: 0.6238 - val_loss: 0.8352\n",
            "Epoch 8/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6262 - loss: 0.8462 - val_accuracy: 0.6235 - val_loss: 0.8331\n",
            "Epoch 9/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6215 - loss: 0.8607 - val_accuracy: 0.6251 - val_loss: 0.8293\n",
            "Epoch 10/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6204 - loss: 0.8341 - val_accuracy: 0.6284 - val_loss: 0.8260\n",
            "Epoch 11/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6235 - loss: 0.8437 - val_accuracy: 0.6294 - val_loss: 0.8218\n",
            "Epoch 12/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6268 - loss: 0.8383 - val_accuracy: 0.6324 - val_loss: 0.8233\n",
            "Epoch 13/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6279 - loss: 0.8194 - val_accuracy: 0.6259 - val_loss: 0.8208\n",
            "Epoch 14/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6307 - loss: 0.8163 - val_accuracy: 0.6319 - val_loss: 0.8182\n",
            "Epoch 15/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6389 - loss: 0.8168 - val_accuracy: 0.6357 - val_loss: 0.8172\n",
            "Epoch 16/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6282 - loss: 0.8185 - val_accuracy: 0.6343 - val_loss: 0.8150\n",
            "Epoch 17/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6350 - loss: 0.8059 - val_accuracy: 0.6357 - val_loss: 0.8171\n",
            "Epoch 18/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6414 - loss: 0.8040 - val_accuracy: 0.6324 - val_loss: 0.8165\n",
            "Epoch 19/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6413 - loss: 0.7955 - val_accuracy: 0.6324 - val_loss: 0.8188\n",
            "Epoch 20/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6388 - loss: 0.8044 - val_accuracy: 0.6392 - val_loss: 0.8148\n",
            "Epoch 21/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6349 - loss: 0.7934 - val_accuracy: 0.6335 - val_loss: 0.8136\n",
            "Epoch 22/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6407 - loss: 0.7898 - val_accuracy: 0.6386 - val_loss: 0.8152\n",
            "Epoch 23/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6495 - loss: 0.7801 - val_accuracy: 0.6324 - val_loss: 0.8146\n",
            "Epoch 24/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6493 - loss: 0.7913 - val_accuracy: 0.6362 - val_loss: 0.8107\n",
            "Epoch 25/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6517 - loss: 0.7869 - val_accuracy: 0.6338 - val_loss: 0.8131\n",
            "Epoch 26/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6474 - loss: 0.7806 - val_accuracy: 0.6308 - val_loss: 0.8142\n",
            "Epoch 27/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6529 - loss: 0.7787 - val_accuracy: 0.6308 - val_loss: 0.8107\n",
            "Epoch 28/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6466 - loss: 0.7857 - val_accuracy: 0.6348 - val_loss: 0.8118\n",
            "Epoch 29/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6636 - loss: 0.7588 - val_accuracy: 0.6359 - val_loss: 0.8104\n",
            "Epoch 30/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6429 - loss: 0.7888 - val_accuracy: 0.6365 - val_loss: 0.8116\n",
            "Epoch 31/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6581 - loss: 0.7661 - val_accuracy: 0.6365 - val_loss: 0.8144\n",
            "Epoch 32/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6539 - loss: 0.7658 - val_accuracy: 0.6365 - val_loss: 0.8107\n",
            "Epoch 33/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6566 - loss: 0.7644 - val_accuracy: 0.6348 - val_loss: 0.8102\n",
            "Epoch 34/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6609 - loss: 0.7595 - val_accuracy: 0.6332 - val_loss: 0.8131\n",
            "Epoch 35/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6507 - loss: 0.7675 - val_accuracy: 0.6354 - val_loss: 0.8168\n",
            "Epoch 36/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6680 - loss: 0.7569 - val_accuracy: 0.6386 - val_loss: 0.8123\n",
            "Epoch 37/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6486 - loss: 0.7786 - val_accuracy: 0.6370 - val_loss: 0.8170\n",
            "Epoch 38/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6540 - loss: 0.7652 - val_accuracy: 0.6354 - val_loss: 0.8127\n",
            "Epoch 39/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6536 - loss: 0.7714 - val_accuracy: 0.6335 - val_loss: 0.8121\n",
            "Epoch 40/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6596 - loss: 0.7499 - val_accuracy: 0.6386 - val_loss: 0.8147\n",
            "Epoch 41/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6586 - loss: 0.7562 - val_accuracy: 0.6330 - val_loss: 0.8162\n",
            "Epoch 42/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6648 - loss: 0.7445 - val_accuracy: 0.6346 - val_loss: 0.8113\n",
            "Epoch 43/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6608 - loss: 0.7556 - val_accuracy: 0.6348 - val_loss: 0.8162\n",
            "Epoch 44/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6619 - loss: 0.7540 - val_accuracy: 0.6448 - val_loss: 0.8135\n",
            "Epoch 45/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6641 - loss: 0.7426 - val_accuracy: 0.6327 - val_loss: 0.8178\n",
            "Epoch 46/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6662 - loss: 0.7373 - val_accuracy: 0.6381 - val_loss: 0.8177\n",
            "Epoch 47/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6627 - loss: 0.7483 - val_accuracy: 0.6354 - val_loss: 0.8130\n",
            "Epoch 48/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6672 - loss: 0.7392 - val_accuracy: 0.6343 - val_loss: 0.8217\n",
            "Epoch 49/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6605 - loss: 0.7401 - val_accuracy: 0.6375 - val_loss: 0.8255\n",
            "Epoch 50/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6742 - loss: 0.7328 - val_accuracy: 0.6359 - val_loss: 0.8195\n",
            "History for model: {'accuracy': [0.4585580825805664, 0.5480043292045593, 0.5697590708732605, 0.5854009389877319, 0.5969076156616211, 0.6058072447776794, 0.6058072447776794, 0.617134153842926, 0.618212878704071, 0.621089518070221, 0.6260337829589844, 0.6331355571746826, 0.6299892067909241, 0.6294498443603516, 0.63547283411026, 0.6330456733703613, 0.6357425451278687, 0.6377202272415161, 0.6342142820358276, 0.6396979689598083, 0.638079822063446, 0.6411362886428833, 0.6481481194496155, 0.6495864987373352, 0.6451815962791443, 0.6501258611679077, 0.6471592783927917, 0.6505753397941589, 0.6519237756729126, 0.651654064655304, 0.6533620953559875, 0.6540812849998474, 0.6550701260566711, 0.6543509364128113, 0.6556094884872437, 0.6574972867965698, 0.6565983295440674, 0.6591154336929321, 0.6599245071411133, 0.6592952013015747, 0.6608234643936157, 0.6581265926361084, 0.6637899875640869, 0.6617224216461182, 0.6627112627029419, 0.666037380695343, 0.6625314354896545, 0.6663070917129517, 0.6655879020690918, 0.6682847738265991], 'loss': [1.162603735923767, 0.9655944108963013, 0.9241166710853577, 0.8955639004707336, 0.8825505971908569, 0.8676808476448059, 0.8615351915359497, 0.8516302108764648, 0.8530848622322083, 0.8368054628372192, 0.8360481262207031, 0.8325810432434082, 0.8224579095840454, 0.82105553150177, 0.811741054058075, 0.813542902469635, 0.8081361055374146, 0.8071478605270386, 0.8031302094459534, 0.8038158416748047, 0.7967363595962524, 0.792778491973877, 0.7882481813430786, 0.7913357615470886, 0.7908223867416382, 0.7833037972450256, 0.7838625907897949, 0.7832067012786865, 0.7796249389648438, 0.7812085747718811, 0.774816632270813, 0.7729200124740601, 0.7710286378860474, 0.7668094635009766, 0.7695906162261963, 0.768189549446106, 0.7619751691818237, 0.7590125799179077, 0.760778546333313, 0.7560619115829468, 0.7516850829124451, 0.7531465888023376, 0.7495056390762329, 0.751487672328949, 0.7438973784446716, 0.7461971640586853, 0.7503176331520081, 0.742164671421051, 0.7437335252761841, 0.741016149520874], 'val_accuracy': [0.5803667902946472, 0.5970873832702637, 0.604908287525177, 0.6094930171966553, 0.6192017197608948, 0.6218985915184021, 0.6237863898277283, 0.6235167384147644, 0.6251348257064819, 0.6283710598945618, 0.6294498443603516, 0.6324163675308228, 0.6259438991546631, 0.6318770051002502, 0.6356526613235474, 0.6343042254447937, 0.6356526613235474, 0.6324163675308228, 0.6324163675308228, 0.6391585469245911, 0.6334951519966125, 0.6386191844940186, 0.6324163675308228, 0.6361920237541199, 0.6337648034095764, 0.6307982802391052, 0.6307982802391052, 0.6348435878753662, 0.6359223127365112, 0.6364616751670837, 0.6364616751670837, 0.6364616751670837, 0.6348435878753662, 0.6332254409790039, 0.6353829503059387, 0.6386191844940186, 0.637001097202301, 0.6353829503059387, 0.6334951519966125, 0.6386191844940186, 0.63295578956604, 0.6345738768577576, 0.6348435878753662, 0.6448220014572144, 0.6326860785484314, 0.638079822063446, 0.6353829503059387, 0.6343042254447937, 0.6375404596328735, 0.6359223127365112], 'val_loss': [0.9374005198478699, 0.8863002061843872, 0.8721065521240234, 0.8544907569885254, 0.845422089099884, 0.8396834135055542, 0.8352251052856445, 0.8331132531166077, 0.8292510509490967, 0.8259501457214355, 0.821808397769928, 0.8232645988464355, 0.8207970857620239, 0.8181524872779846, 0.817223846912384, 0.8149598836898804, 0.817143976688385, 0.816475510597229, 0.81881183385849, 0.8148337006568909, 0.8136219382286072, 0.815177857875824, 0.8146491646766663, 0.8106509447097778, 0.8130873441696167, 0.8141859769821167, 0.8107385635375977, 0.811751127243042, 0.810392439365387, 0.8116052150726318, 0.8144338130950928, 0.8107456564903259, 0.8101502060890198, 0.8130728602409363, 0.8167654871940613, 0.8123345375061035, 0.8170463442802429, 0.8127168416976929, 0.812074601650238, 0.8146994113922119, 0.8161719441413879, 0.8113364577293396, 0.8162075877189636, 0.8134722709655762, 0.8177549242973328, 0.8177092671394348, 0.813011109828949, 0.8216835856437683, 0.82549649477005, 0.8194504976272583]}\n",
            "Trial: Neurons = 50, Dropout Rate = 0.5, Batch Size = 128\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.3850 - loss: 1.2904 - val_accuracy: 0.5836 - val_loss: 0.9635\n",
            "Epoch 2/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5462 - loss: 0.9993 - val_accuracy: 0.5987 - val_loss: 0.9032\n",
            "Epoch 3/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5632 - loss: 0.9440 - val_accuracy: 0.6098 - val_loss: 0.8765\n",
            "Epoch 4/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5879 - loss: 0.9124 - val_accuracy: 0.6090 - val_loss: 0.8636\n",
            "Epoch 5/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5948 - loss: 0.9036 - val_accuracy: 0.6149 - val_loss: 0.8574\n",
            "Epoch 6/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5980 - loss: 0.8877 - val_accuracy: 0.6162 - val_loss: 0.8500\n",
            "Epoch 7/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5996 - loss: 0.8818 - val_accuracy: 0.6184 - val_loss: 0.8453\n",
            "Epoch 8/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6068 - loss: 0.8617 - val_accuracy: 0.6197 - val_loss: 0.8425\n",
            "Epoch 9/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6080 - loss: 0.8649 - val_accuracy: 0.6189 - val_loss: 0.8371\n",
            "Epoch 10/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6078 - loss: 0.8556 - val_accuracy: 0.6200 - val_loss: 0.8356\n",
            "Epoch 11/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6202 - loss: 0.8522 - val_accuracy: 0.6184 - val_loss: 0.8327\n",
            "Epoch 12/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6179 - loss: 0.8481 - val_accuracy: 0.6195 - val_loss: 0.8306\n",
            "Epoch 13/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6211 - loss: 0.8347 - val_accuracy: 0.6211 - val_loss: 0.8279\n",
            "Epoch 14/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6213 - loss: 0.8329 - val_accuracy: 0.6297 - val_loss: 0.8270\n",
            "Epoch 15/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6190 - loss: 0.8419 - val_accuracy: 0.6249 - val_loss: 0.8251\n",
            "Epoch 16/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6349 - loss: 0.8303 - val_accuracy: 0.6294 - val_loss: 0.8260\n",
            "Epoch 17/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6328 - loss: 0.8198 - val_accuracy: 0.6273 - val_loss: 0.8234\n",
            "Epoch 18/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6206 - loss: 0.8261 - val_accuracy: 0.6265 - val_loss: 0.8240\n",
            "Epoch 19/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6327 - loss: 0.8173 - val_accuracy: 0.6268 - val_loss: 0.8216\n",
            "Epoch 20/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6291 - loss: 0.8143 - val_accuracy: 0.6292 - val_loss: 0.8218\n",
            "Epoch 21/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6296 - loss: 0.8088 - val_accuracy: 0.6251 - val_loss: 0.8205\n",
            "Epoch 22/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6406 - loss: 0.8050 - val_accuracy: 0.6286 - val_loss: 0.8200\n",
            "Epoch 23/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6300 - loss: 0.8130 - val_accuracy: 0.6305 - val_loss: 0.8183\n",
            "Epoch 24/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6379 - loss: 0.7995 - val_accuracy: 0.6332 - val_loss: 0.8190\n",
            "Epoch 25/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6463 - loss: 0.8040 - val_accuracy: 0.6276 - val_loss: 0.8158\n",
            "Epoch 26/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6414 - loss: 0.7929 - val_accuracy: 0.6300 - val_loss: 0.8164\n",
            "Epoch 27/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6419 - loss: 0.7955 - val_accuracy: 0.6286 - val_loss: 0.8162\n",
            "Epoch 28/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6496 - loss: 0.7918 - val_accuracy: 0.6321 - val_loss: 0.8164\n",
            "Epoch 29/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6419 - loss: 0.7962 - val_accuracy: 0.6335 - val_loss: 0.8149\n",
            "Epoch 30/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6400 - loss: 0.7887 - val_accuracy: 0.6300 - val_loss: 0.8175\n",
            "Epoch 31/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6478 - loss: 0.7847 - val_accuracy: 0.6319 - val_loss: 0.8147\n",
            "Epoch 32/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6458 - loss: 0.7914 - val_accuracy: 0.6286 - val_loss: 0.8192\n",
            "Epoch 33/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6459 - loss: 0.7945 - val_accuracy: 0.6286 - val_loss: 0.8155\n",
            "Epoch 34/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6478 - loss: 0.7824 - val_accuracy: 0.6332 - val_loss: 0.8152\n",
            "Epoch 35/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6507 - loss: 0.7863 - val_accuracy: 0.6311 - val_loss: 0.8157\n",
            "Epoch 36/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6515 - loss: 0.7762 - val_accuracy: 0.6313 - val_loss: 0.8152\n",
            "Epoch 37/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6526 - loss: 0.7886 - val_accuracy: 0.6305 - val_loss: 0.8160\n",
            "Epoch 38/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6490 - loss: 0.7946 - val_accuracy: 0.6319 - val_loss: 0.8142\n",
            "Epoch 39/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6584 - loss: 0.7772 - val_accuracy: 0.6330 - val_loss: 0.8161\n",
            "Epoch 40/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6506 - loss: 0.7766 - val_accuracy: 0.6343 - val_loss: 0.8140\n",
            "Epoch 41/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6486 - loss: 0.7773 - val_accuracy: 0.6348 - val_loss: 0.8137\n",
            "Epoch 42/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6535 - loss: 0.7849 - val_accuracy: 0.6365 - val_loss: 0.8127\n",
            "Epoch 43/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6572 - loss: 0.7787 - val_accuracy: 0.6365 - val_loss: 0.8119\n",
            "Epoch 44/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6536 - loss: 0.7687 - val_accuracy: 0.6346 - val_loss: 0.8133\n",
            "Epoch 45/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6598 - loss: 0.7624 - val_accuracy: 0.6313 - val_loss: 0.8132\n",
            "Epoch 46/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6598 - loss: 0.7660 - val_accuracy: 0.6343 - val_loss: 0.8130\n",
            "Epoch 47/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6615 - loss: 0.7625 - val_accuracy: 0.6386 - val_loss: 0.8113\n",
            "Epoch 48/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6521 - loss: 0.7732 - val_accuracy: 0.6365 - val_loss: 0.8141\n",
            "Epoch 49/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6574 - loss: 0.7749 - val_accuracy: 0.6340 - val_loss: 0.8176\n",
            "Epoch 50/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6522 - loss: 0.7657 - val_accuracy: 0.6419 - val_loss: 0.8139\n",
            "History for model: {'accuracy': [0.4585580825805664, 0.5472851395606995, 0.5644552111625671, 0.5829737782478333, 0.591693639755249, 0.599784255027771, 0.6036497950553894, 0.6096727848052979, 0.6143473386764526, 0.6103020310401917, 0.6184825897216797, 0.6192916035652161, 0.618572473526001, 0.6218087077140808, 0.6255843043327332, 0.6287306547164917, 0.6268428564071655, 0.6257641315460205, 0.63295578956604, 0.6319668889045715, 0.6325063109397888, 0.6344839930534363, 0.6337648034095764, 0.6358324289321899, 0.6408665776252747, 0.6386191844940186, 0.6341243982315063, 0.6451815962791443, 0.6412261724472046, 0.6428443193435669, 0.6423048973083496, 0.6446422338485718, 0.6459906697273254, 0.6494066715240479, 0.647608757019043, 0.648777425289154, 0.6506652235984802, 0.6511147022247314, 0.6549802422523499, 0.6508449912071228, 0.6518338918685913, 0.6550701260566711, 0.655339777469635, 0.6562387347221375, 0.6565983295440674, 0.656868040561676, 0.6614527106285095, 0.6507551074028015, 0.6565983295440674, 0.6533620953559875], 'loss': [1.1783983707427979, 0.9913774132728577, 0.9388107061386108, 0.910653829574585, 0.8945168256759644, 0.887450635433197, 0.8743921518325806, 0.865498960018158, 0.8576517105102539, 0.8552682399749756, 0.8535546660423279, 0.8472489714622498, 0.8394808173179626, 0.8345786333084106, 0.8321714401245117, 0.8288063406944275, 0.8250438570976257, 0.8193666338920593, 0.8222530484199524, 0.8141213059425354, 0.8151942491531372, 0.8111881613731384, 0.8157463669776917, 0.8084944486618042, 0.8064965605735779, 0.8007771372795105, 0.8080280423164368, 0.7955941557884216, 0.7997593283653259, 0.7970058917999268, 0.7930324673652649, 0.7912554740905762, 0.7945762276649475, 0.7856355905532837, 0.787041425704956, 0.7872995138168335, 0.7828296422958374, 0.7821061015129089, 0.7768909335136414, 0.7799427509307861, 0.780367910861969, 0.7764754295349121, 0.7778269648551941, 0.7695749402046204, 0.7711723446846008, 0.7716194987297058, 0.7672539353370667, 0.7670284509658813, 0.7718349099159241, 0.7687150835990906], 'val_accuracy': [0.583603024482727, 0.598705530166626, 0.6097626686096191, 0.608953595161438, 0.6148867607116699, 0.6162351965904236, 0.6183926463127136, 0.6197410821914673, 0.6189320683479309, 0.6200107932090759, 0.6183926463127136, 0.6194714307785034, 0.621089518070221, 0.6297194957733154, 0.6248651742935181, 0.6294498443603516, 0.6272923350334167, 0.6264832615852356, 0.6267529726028442, 0.6291801333427429, 0.6251348257064819, 0.6286407709121704, 0.6305285692214966, 0.6332254409790039, 0.6275620460510254, 0.6299892067909241, 0.6286407709121704, 0.6321467161178589, 0.6334951519966125, 0.6299892067909241, 0.6318770051002502, 0.6286407709121704, 0.6286407709121704, 0.6332254409790039, 0.6310679316520691, 0.6313376426696777, 0.6305285692214966, 0.6318770051002502, 0.63295578956604, 0.6343042254447937, 0.6348435878753662, 0.6364616751670837, 0.6364616751670837, 0.6345738768577576, 0.6313376426696777, 0.6343042254447937, 0.6386191844940186, 0.6364616751670837, 0.6340345144271851, 0.6418554186820984], 'val_loss': [0.963546097278595, 0.9031751155853271, 0.8764962553977966, 0.8636395335197449, 0.8573566675186157, 0.8499740958213806, 0.845291793346405, 0.8425233960151672, 0.8371068835258484, 0.8355587124824524, 0.8327227234840393, 0.8306487202644348, 0.8278886079788208, 0.8269815444946289, 0.8251405358314514, 0.8260340690612793, 0.8234248161315918, 0.8239864706993103, 0.8215575814247131, 0.8217999935150146, 0.8204829692840576, 0.8200039267539978, 0.8182843327522278, 0.8189610838890076, 0.8157978653907776, 0.8163687586784363, 0.8162189722061157, 0.8164232969284058, 0.8148836493492126, 0.817520260810852, 0.8147497177124023, 0.8192131519317627, 0.8154901266098022, 0.8152108192443848, 0.8157137036323547, 0.815167248249054, 0.8160290718078613, 0.8141648769378662, 0.8160812854766846, 0.8140143156051636, 0.8137404322624207, 0.8126798868179321, 0.811922550201416, 0.813289225101471, 0.8131838440895081, 0.8130282759666443, 0.8112674355506897, 0.8140679597854614, 0.8175973296165466, 0.8139278292655945]}\n",
            "Trial: Neurons = 50, Dropout Rate = 0.5, Batch Size = 256\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.3349 - loss: 1.4005 - val_accuracy: 0.5467 - val_loss: 1.0676\n",
            "Epoch 2/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5147 - loss: 1.1024 - val_accuracy: 0.5798 - val_loss: 0.9494\n",
            "Epoch 3/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5351 - loss: 1.0155 - val_accuracy: 0.5876 - val_loss: 0.9069\n",
            "Epoch 4/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5555 - loss: 0.9641 - val_accuracy: 0.5912 - val_loss: 0.8855\n",
            "Epoch 5/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5765 - loss: 0.9204 - val_accuracy: 0.6014 - val_loss: 0.8745\n",
            "Epoch 6/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5660 - loss: 0.9363 - val_accuracy: 0.6030 - val_loss: 0.8676\n",
            "Epoch 7/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5863 - loss: 0.9054 - val_accuracy: 0.6063 - val_loss: 0.8610\n",
            "Epoch 8/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5873 - loss: 0.8928 - val_accuracy: 0.6081 - val_loss: 0.8547\n",
            "Epoch 9/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5996 - loss: 0.8768 - val_accuracy: 0.6098 - val_loss: 0.8522\n",
            "Epoch 10/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6057 - loss: 0.8714 - val_accuracy: 0.6114 - val_loss: 0.8484\n",
            "Epoch 11/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6014 - loss: 0.8690 - val_accuracy: 0.6149 - val_loss: 0.8458\n",
            "Epoch 12/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5997 - loss: 0.8689 - val_accuracy: 0.6216 - val_loss: 0.8433\n",
            "Epoch 13/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6067 - loss: 0.8611 - val_accuracy: 0.6230 - val_loss: 0.8406\n",
            "Epoch 14/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6030 - loss: 0.8573 - val_accuracy: 0.6206 - val_loss: 0.8385\n",
            "Epoch 15/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6139 - loss: 0.8611 - val_accuracy: 0.6211 - val_loss: 0.8367\n",
            "Epoch 16/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6140 - loss: 0.8570 - val_accuracy: 0.6259 - val_loss: 0.8338\n",
            "Epoch 17/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6174 - loss: 0.8530 - val_accuracy: 0.6230 - val_loss: 0.8332\n",
            "Epoch 18/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6226 - loss: 0.8387 - val_accuracy: 0.6268 - val_loss: 0.8320\n",
            "Epoch 19/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6216 - loss: 0.8463 - val_accuracy: 0.6284 - val_loss: 0.8306\n",
            "Epoch 20/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6230 - loss: 0.8311 - val_accuracy: 0.6300 - val_loss: 0.8309\n",
            "Epoch 21/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6251 - loss: 0.8422 - val_accuracy: 0.6268 - val_loss: 0.8265\n",
            "Epoch 22/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6337 - loss: 0.8231 - val_accuracy: 0.6319 - val_loss: 0.8263\n",
            "Epoch 23/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6322 - loss: 0.8283 - val_accuracy: 0.6319 - val_loss: 0.8245\n",
            "Epoch 24/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6289 - loss: 0.8231 - val_accuracy: 0.6297 - val_loss: 0.8244\n",
            "Epoch 25/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6293 - loss: 0.8262 - val_accuracy: 0.6303 - val_loss: 0.8240\n",
            "Epoch 26/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6266 - loss: 0.8258 - val_accuracy: 0.6324 - val_loss: 0.8208\n",
            "Epoch 27/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6382 - loss: 0.8271 - val_accuracy: 0.6335 - val_loss: 0.8207\n",
            "Epoch 28/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6392 - loss: 0.8150 - val_accuracy: 0.6338 - val_loss: 0.8212\n",
            "Epoch 29/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6386 - loss: 0.8186 - val_accuracy: 0.6400 - val_loss: 0.8185\n",
            "Epoch 30/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6377 - loss: 0.8122 - val_accuracy: 0.6332 - val_loss: 0.8186\n",
            "Epoch 31/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6320 - loss: 0.8044 - val_accuracy: 0.6343 - val_loss: 0.8184\n",
            "Epoch 32/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6405 - loss: 0.8112 - val_accuracy: 0.6367 - val_loss: 0.8170\n",
            "Epoch 33/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6367 - loss: 0.8086 - val_accuracy: 0.6378 - val_loss: 0.8172\n",
            "Epoch 34/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6330 - loss: 0.8147 - val_accuracy: 0.6378 - val_loss: 0.8167\n",
            "Epoch 35/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6401 - loss: 0.8120 - val_accuracy: 0.6389 - val_loss: 0.8152\n",
            "Epoch 36/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6338 - loss: 0.8042 - val_accuracy: 0.6424 - val_loss: 0.8142\n",
            "Epoch 37/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6490 - loss: 0.8010 - val_accuracy: 0.6365 - val_loss: 0.8131\n",
            "Epoch 38/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6467 - loss: 0.7877 - val_accuracy: 0.6397 - val_loss: 0.8130\n",
            "Epoch 39/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6387 - loss: 0.7970 - val_accuracy: 0.6386 - val_loss: 0.8126\n",
            "Epoch 40/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6398 - loss: 0.8093 - val_accuracy: 0.6424 - val_loss: 0.8123\n",
            "Epoch 41/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6400 - loss: 0.7930 - val_accuracy: 0.6405 - val_loss: 0.8137\n",
            "Epoch 42/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6458 - loss: 0.7898 - val_accuracy: 0.6389 - val_loss: 0.8115\n",
            "Epoch 43/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6497 - loss: 0.7855 - val_accuracy: 0.6381 - val_loss: 0.8133\n",
            "Epoch 44/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6396 - loss: 0.8053 - val_accuracy: 0.6405 - val_loss: 0.8119\n",
            "Epoch 45/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6419 - loss: 0.7874 - val_accuracy: 0.6354 - val_loss: 0.8127\n",
            "Epoch 46/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6446 - loss: 0.7949 - val_accuracy: 0.6370 - val_loss: 0.8105\n",
            "Epoch 47/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6408 - loss: 0.7877 - val_accuracy: 0.6394 - val_loss: 0.8123\n",
            "Epoch 48/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6512 - loss: 0.7890 - val_accuracy: 0.6346 - val_loss: 0.8117\n",
            "Epoch 49/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6498 - loss: 0.7732 - val_accuracy: 0.6365 - val_loss: 0.8103\n",
            "Epoch 50/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6381 - loss: 0.7944 - val_accuracy: 0.6343 - val_loss: 0.8119\n",
            "History for model: {'accuracy': [0.3974289894104004, 0.5159115195274353, 0.5460265874862671, 0.5569939017295837, 0.5745235681533813, 0.5785688757896423, 0.5889068841934204, 0.5868392586708069, 0.5965480208396912, 0.6048184037208557, 0.6040093302726746, 0.6049982309341431, 0.6079647541046143, 0.6111111044883728, 0.6157857179641724, 0.6174038052558899, 0.6149766445159912, 0.6167745590209961, 0.6227076649665833, 0.6219884753227234, 0.6280115246772766, 0.6316073536872864, 0.6307083964347839, 0.6299892067909241, 0.6307083964347839, 0.6297194957733154, 0.6338547468185425, 0.6373606324195862, 0.6375404596328735, 0.6337648034095764, 0.6334951519966125, 0.63547283411026, 0.636911153793335, 0.6376303434371948, 0.640956461429596, 0.6392484903335571, 0.6449118852615356, 0.6379899382591248, 0.6403272151947021, 0.6419453620910645, 0.6408665776252747, 0.6413160562515259, 0.6483279466629028, 0.6423948407173157, 0.6451815962791443, 0.6484178304672241, 0.6485975980758667, 0.6466199159622192, 0.643563449382782, 0.6437432765960693], 'loss': [1.30405855178833, 1.0791833400726318, 0.9946231245994568, 0.9581121802330017, 0.9282097220420837, 0.9207164645195007, 0.9031020402908325, 0.889440655708313, 0.8820555210113525, 0.8746058344841003, 0.869804859161377, 0.8684747219085693, 0.8638627529144287, 0.8542779684066772, 0.8546964526176453, 0.8508196473121643, 0.8488325476646423, 0.8430280089378357, 0.8354713320732117, 0.8353267312049866, 0.8362430930137634, 0.8318896293640137, 0.8286372423171997, 0.8228487372398376, 0.8235641121864319, 0.823276937007904, 0.8217324018478394, 0.8180164098739624, 0.815034806728363, 0.8159112334251404, 0.8131331205368042, 0.8137863874435425, 0.8058250546455383, 0.809735894203186, 0.8060455918312073, 0.8055184483528137, 0.8023564219474792, 0.8026400208473206, 0.7974535226821899, 0.7988123297691345, 0.7952364087104797, 0.793018639087677, 0.790849506855011, 0.7952308058738708, 0.7915627360343933, 0.7866492867469788, 0.7829450964927673, 0.7883126139640808, 0.789279580116272, 0.7841477394104004], 'val_accuracy': [0.5466558933258057, 0.5798274278640747, 0.587648332118988, 0.5911542773246765, 0.6014024019241333, 0.6030204892158508, 0.6062567234039307, 0.6081445813179016, 0.6097626686096191, 0.6113808155059814, 0.6148867607116699, 0.6216289401054382, 0.6229773759841919, 0.6205501556396484, 0.621089518070221, 0.6259438991546631, 0.6229773759841919, 0.6267529726028442, 0.6283710598945618, 0.6299892067909241, 0.6267529726028442, 0.6318770051002502, 0.6318770051002502, 0.6297194957733154, 0.6302589178085327, 0.6324163675308228, 0.6334951519966125, 0.6337648034095764, 0.6399676203727722, 0.6332254409790039, 0.6343042254447937, 0.6367313861846924, 0.6378101110458374, 0.6378101110458374, 0.6388888955116272, 0.6423948407173157, 0.6364616751670837, 0.6396979689598083, 0.6386191844940186, 0.6423948407173157, 0.6405069828033447, 0.6388888955116272, 0.638079822063446, 0.6405069828033447, 0.6353829503059387, 0.637001097202301, 0.6394282579421997, 0.6345738768577576, 0.6364616751670837, 0.6343042254447937], 'val_loss': [1.0676257610321045, 0.9494129419326782, 0.9068734645843506, 0.885502278804779, 0.874457061290741, 0.8675979971885681, 0.8610361814498901, 0.8547495603561401, 0.8522414565086365, 0.8484096527099609, 0.8457818031311035, 0.8433328866958618, 0.8406342267990112, 0.8385311365127563, 0.8366666436195374, 0.8338070511817932, 0.833246648311615, 0.8319504261016846, 0.8305973410606384, 0.8308690190315247, 0.8265261054039001, 0.8262872099876404, 0.8244866132736206, 0.824374794960022, 0.8240167498588562, 0.8208475112915039, 0.8206866383552551, 0.8211889266967773, 0.8185171484947205, 0.8186436295509338, 0.8183642029762268, 0.8170361518859863, 0.8172245025634766, 0.8167275786399841, 0.8151986598968506, 0.8142306208610535, 0.8130857944488525, 0.8130227327346802, 0.8125999569892883, 0.8122584819793701, 0.8137271404266357, 0.8115456700325012, 0.8132845163345337, 0.8118656873703003, 0.8127207159996033, 0.8104872107505798, 0.8123126029968262, 0.8117440938949585, 0.8102760910987854, 0.8119482398033142]}\n",
            "Trial: Neurons = 100, Dropout Rate = 0, Batch Size = 64\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4642 - loss: 1.1445 - val_accuracy: 0.6046 - val_loss: 0.8752\n",
            "Epoch 2/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5984 - loss: 0.8592 - val_accuracy: 0.6111 - val_loss: 0.8538\n",
            "Epoch 3/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6107 - loss: 0.8363 - val_accuracy: 0.6079 - val_loss: 0.8508\n",
            "Epoch 4/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6234 - loss: 0.8207 - val_accuracy: 0.6200 - val_loss: 0.8371\n",
            "Epoch 5/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6307 - loss: 0.8137 - val_accuracy: 0.6117 - val_loss: 0.8414\n",
            "Epoch 6/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6321 - loss: 0.8047 - val_accuracy: 0.6270 - val_loss: 0.8306\n",
            "Epoch 7/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6370 - loss: 0.7971 - val_accuracy: 0.6227 - val_loss: 0.8351\n",
            "Epoch 8/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6366 - loss: 0.7876 - val_accuracy: 0.6235 - val_loss: 0.8355\n",
            "Epoch 9/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6450 - loss: 0.7782 - val_accuracy: 0.6338 - val_loss: 0.8218\n",
            "Epoch 10/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6503 - loss: 0.7661 - val_accuracy: 0.6216 - val_loss: 0.8247\n",
            "Epoch 11/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6542 - loss: 0.7705 - val_accuracy: 0.6181 - val_loss: 0.8278\n",
            "Epoch 12/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6526 - loss: 0.7656 - val_accuracy: 0.6321 - val_loss: 0.8210\n",
            "Epoch 13/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6629 - loss: 0.7433 - val_accuracy: 0.6351 - val_loss: 0.8241\n",
            "Epoch 14/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6574 - loss: 0.7531 - val_accuracy: 0.6243 - val_loss: 0.8238\n",
            "Epoch 15/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6641 - loss: 0.7475 - val_accuracy: 0.6292 - val_loss: 0.8213\n",
            "Epoch 16/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6701 - loss: 0.7412 - val_accuracy: 0.6316 - val_loss: 0.8238\n",
            "Epoch 17/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6679 - loss: 0.7470 - val_accuracy: 0.6343 - val_loss: 0.8181\n",
            "Epoch 18/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6647 - loss: 0.7454 - val_accuracy: 0.6354 - val_loss: 0.8170\n",
            "Epoch 19/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6677 - loss: 0.7451 - val_accuracy: 0.6303 - val_loss: 0.8169\n",
            "Epoch 20/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6702 - loss: 0.7315 - val_accuracy: 0.6362 - val_loss: 0.8199\n",
            "Epoch 21/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6761 - loss: 0.7245 - val_accuracy: 0.6294 - val_loss: 0.8224\n",
            "Epoch 22/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6785 - loss: 0.7170 - val_accuracy: 0.6319 - val_loss: 0.8172\n",
            "Epoch 23/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6744 - loss: 0.7243 - val_accuracy: 0.6340 - val_loss: 0.8266\n",
            "Epoch 24/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6860 - loss: 0.7106 - val_accuracy: 0.6327 - val_loss: 0.8264\n",
            "Epoch 25/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6794 - loss: 0.7162 - val_accuracy: 0.6346 - val_loss: 0.8210\n",
            "Epoch 26/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6795 - loss: 0.7137 - val_accuracy: 0.6316 - val_loss: 0.8299\n",
            "Epoch 27/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6795 - loss: 0.6961 - val_accuracy: 0.6367 - val_loss: 0.8267\n",
            "Epoch 28/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6867 - loss: 0.6945 - val_accuracy: 0.6281 - val_loss: 0.8277\n",
            "Epoch 29/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6848 - loss: 0.6967 - val_accuracy: 0.6286 - val_loss: 0.8277\n",
            "Epoch 30/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6831 - loss: 0.7073 - val_accuracy: 0.6316 - val_loss: 0.8289\n",
            "Epoch 31/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6888 - loss: 0.7019 - val_accuracy: 0.6313 - val_loss: 0.8341\n",
            "Epoch 32/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6951 - loss: 0.6907 - val_accuracy: 0.6303 - val_loss: 0.8344\n",
            "Epoch 33/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6944 - loss: 0.6823 - val_accuracy: 0.6254 - val_loss: 0.8345\n",
            "Epoch 34/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6977 - loss: 0.6862 - val_accuracy: 0.6303 - val_loss: 0.8320\n",
            "Epoch 35/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6876 - loss: 0.6885 - val_accuracy: 0.6311 - val_loss: 0.8298\n",
            "Epoch 36/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7057 - loss: 0.6725 - val_accuracy: 0.6203 - val_loss: 0.8347\n",
            "Epoch 37/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7039 - loss: 0.6773 - val_accuracy: 0.6335 - val_loss: 0.8321\n",
            "Epoch 38/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7026 - loss: 0.6726 - val_accuracy: 0.6208 - val_loss: 0.8486\n",
            "Epoch 39/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7021 - loss: 0.6737 - val_accuracy: 0.6292 - val_loss: 0.8351\n",
            "Epoch 40/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7043 - loss: 0.6698 - val_accuracy: 0.6192 - val_loss: 0.8363\n",
            "Epoch 41/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7014 - loss: 0.6640 - val_accuracy: 0.6257 - val_loss: 0.8423\n",
            "Epoch 42/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7043 - loss: 0.6714 - val_accuracy: 0.6249 - val_loss: 0.8423\n",
            "Epoch 43/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7050 - loss: 0.6647 - val_accuracy: 0.6251 - val_loss: 0.8414\n",
            "Epoch 44/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7077 - loss: 0.6641 - val_accuracy: 0.6259 - val_loss: 0.8530\n",
            "Epoch 45/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7145 - loss: 0.6504 - val_accuracy: 0.6381 - val_loss: 0.8495\n",
            "Epoch 46/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7096 - loss: 0.6547 - val_accuracy: 0.6284 - val_loss: 0.8478\n",
            "Epoch 47/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7153 - loss: 0.6589 - val_accuracy: 0.6324 - val_loss: 0.8491\n",
            "Epoch 48/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7132 - loss: 0.6515 - val_accuracy: 0.6308 - val_loss: 0.8472\n",
            "Epoch 49/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7093 - loss: 0.6506 - val_accuracy: 0.6152 - val_loss: 0.8666\n",
            "Epoch 50/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7108 - loss: 0.6519 - val_accuracy: 0.6270 - val_loss: 0.8542\n",
            "History for model: {'accuracy': [0.5440489053726196, 0.6047285199165344, 0.61452716588974, 0.6237863898277283, 0.6344839930534363, 0.6351132392883301, 0.6356526613235474, 0.6394282579421997, 0.6453613638877869, 0.6484178304672241, 0.6532722115516663, 0.6529126167297363, 0.6561488509178162, 0.6589356064796448, 0.6596547961235046, 0.6638798713684082, 0.6657677292823792, 0.6681948900222778, 0.6725099086761475, 0.6706220507621765, 0.675656259059906, 0.6761956214904785, 0.6765552163124084, 0.6790722608566284, 0.6801510453224182, 0.679701566696167, 0.6784430146217346, 0.6832074522972107, 0.6830276846885681, 0.685904324054718, 0.687792181968689, 0.6886911392211914, 0.692556619644165, 0.692556619644165, 0.6890506744384766, 0.6956130862236023, 0.6947141289710999, 0.6983099579811096, 0.6945343613624573, 0.6997482776641846, 0.7008270621299744, 0.7023552656173706, 0.6999281048774719, 0.7054117321968079, 0.7044228911399841, 0.7046026587486267, 0.7058612108230591, 0.7054117321968079, 0.7094570398330688, 0.7111650705337524], 'loss': [1.0017181634902954, 0.8539748191833496, 0.831626296043396, 0.8180843591690063, 0.8077601194381714, 0.8001911044120789, 0.791866660118103, 0.7874360084533691, 0.7808398604393005, 0.7744584083557129, 0.768750786781311, 0.7651172280311584, 0.761507511138916, 0.7560151219367981, 0.7525659799575806, 0.7505441308021545, 0.7453243732452393, 0.7399082183837891, 0.737302839756012, 0.7325255274772644, 0.728786289691925, 0.7259142398834229, 0.7233917713165283, 0.7185632586479187, 0.7173923254013062, 0.7130382061004639, 0.7105752825737, 0.7077735662460327, 0.7060806751251221, 0.7017718553543091, 0.7005772590637207, 0.6950811743736267, 0.6919049620628357, 0.6921006441116333, 0.6886435151100159, 0.6868451237678528, 0.6844944953918457, 0.681130588054657, 0.6804414987564087, 0.6763595938682556, 0.6738879680633545, 0.6715006828308105, 0.6690096855163574, 0.6677113175392151, 0.662527859210968, 0.6636995077133179, 0.6615636944770813, 0.6607323288917542, 0.6581742167472839, 0.6544692516326904], 'val_accuracy': [0.6046386361122131, 0.6111111044883728, 0.607874870300293, 0.6200107932090759, 0.6116504669189453, 0.6270226240158081, 0.6227076649665833, 0.6235167384147644, 0.6337648034095764, 0.6216289401054382, 0.6181229948997498, 0.6321467161178589, 0.6351132392883301, 0.6243258118629456, 0.6291801333427429, 0.6316073536872864, 0.6343042254447937, 0.6353829503059387, 0.6302589178085327, 0.6361920237541199, 0.6294498443603516, 0.6318770051002502, 0.6340345144271851, 0.6326860785484314, 0.6345738768577576, 0.6316073536872864, 0.6367313861846924, 0.6281014084815979, 0.6286407709121704, 0.6316073536872864, 0.6313376426696777, 0.6302589178085327, 0.6254045367240906, 0.6302589178085327, 0.6310679316520691, 0.6202805042266846, 0.6334951519966125, 0.6208198666572571, 0.6291801333427429, 0.6192017197608948, 0.6256741881370544, 0.6248651742935181, 0.6251348257064819, 0.6259438991546631, 0.638079822063446, 0.6283710598945618, 0.6324163675308228, 0.6307982802391052, 0.6151564121246338, 0.6270226240158081], 'val_loss': [0.8752486705780029, 0.8537724614143372, 0.8507916331291199, 0.83710116147995, 0.8414220213890076, 0.8306133151054382, 0.8350844383239746, 0.8355384469032288, 0.8217650055885315, 0.8247492909431458, 0.8278083205223083, 0.8209564685821533, 0.8241163492202759, 0.82377028465271, 0.8212876915931702, 0.8237502574920654, 0.8180623054504395, 0.8169771432876587, 0.8168694376945496, 0.819911539554596, 0.822446346282959, 0.8172115683555603, 0.8266458511352539, 0.8263968825340271, 0.820972740650177, 0.8298640847206116, 0.826711118221283, 0.8277398943901062, 0.8276605010032654, 0.8288687467575073, 0.8340598344802856, 0.8344034552574158, 0.8344553112983704, 0.8319715857505798, 0.8298474550247192, 0.8346760272979736, 0.8320930004119873, 0.8485745787620544, 0.8350721597671509, 0.8362751603126526, 0.8422517776489258, 0.8423400521278381, 0.8414155840873718, 0.852993369102478, 0.8495154976844788, 0.8478331565856934, 0.849092423915863, 0.8471748232841492, 0.8666295409202576, 0.8541792035102844]}\n",
            "Trial: Neurons = 100, Dropout Rate = 0, Batch Size = 128\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.4712 - loss: 1.1954 - val_accuracy: 0.5928 - val_loss: 0.9048\n",
            "Epoch 2/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6052 - loss: 0.8773 - val_accuracy: 0.6065 - val_loss: 0.8644\n",
            "Epoch 3/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6125 - loss: 0.8417 - val_accuracy: 0.6068 - val_loss: 0.8579\n",
            "Epoch 4/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6147 - loss: 0.8305 - val_accuracy: 0.6189 - val_loss: 0.8399\n",
            "Epoch 5/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6339 - loss: 0.8152 - val_accuracy: 0.6187 - val_loss: 0.8395\n",
            "Epoch 6/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6441 - loss: 0.7981 - val_accuracy: 0.6262 - val_loss: 0.8329\n",
            "Epoch 7/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6340 - loss: 0.8008 - val_accuracy: 0.6243 - val_loss: 0.8300\n",
            "Epoch 8/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6452 - loss: 0.7869 - val_accuracy: 0.6251 - val_loss: 0.8292\n",
            "Epoch 9/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6548 - loss: 0.7635 - val_accuracy: 0.6313 - val_loss: 0.8251\n",
            "Epoch 10/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6575 - loss: 0.7783 - val_accuracy: 0.6222 - val_loss: 0.8272\n",
            "Epoch 11/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6618 - loss: 0.7630 - val_accuracy: 0.6268 - val_loss: 0.8306\n",
            "Epoch 12/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6592 - loss: 0.7679 - val_accuracy: 0.6251 - val_loss: 0.8220\n",
            "Epoch 13/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6539 - loss: 0.7692 - val_accuracy: 0.6246 - val_loss: 0.8269\n",
            "Epoch 14/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6531 - loss: 0.7654 - val_accuracy: 0.6292 - val_loss: 0.8209\n",
            "Epoch 15/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6654 - loss: 0.7584 - val_accuracy: 0.6303 - val_loss: 0.8237\n",
            "Epoch 16/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6628 - loss: 0.7494 - val_accuracy: 0.6249 - val_loss: 0.8241\n",
            "Epoch 17/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6600 - loss: 0.7554 - val_accuracy: 0.6143 - val_loss: 0.8324\n",
            "Epoch 18/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6684 - loss: 0.7481 - val_accuracy: 0.6149 - val_loss: 0.8298\n",
            "Epoch 19/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6767 - loss: 0.7450 - val_accuracy: 0.6343 - val_loss: 0.8214\n",
            "Epoch 20/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6733 - loss: 0.7329 - val_accuracy: 0.6251 - val_loss: 0.8247\n",
            "Epoch 21/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6695 - loss: 0.7422 - val_accuracy: 0.6294 - val_loss: 0.8237\n",
            "Epoch 22/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6829 - loss: 0.7154 - val_accuracy: 0.6292 - val_loss: 0.8243\n",
            "Epoch 23/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6716 - loss: 0.7322 - val_accuracy: 0.6195 - val_loss: 0.8329\n",
            "Epoch 24/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6716 - loss: 0.7314 - val_accuracy: 0.6246 - val_loss: 0.8230\n",
            "Epoch 25/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6857 - loss: 0.7170 - val_accuracy: 0.6305 - val_loss: 0.8226\n",
            "Epoch 26/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6774 - loss: 0.7202 - val_accuracy: 0.6367 - val_loss: 0.8217\n",
            "Epoch 27/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6756 - loss: 0.7183 - val_accuracy: 0.6303 - val_loss: 0.8196\n",
            "Epoch 28/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6847 - loss: 0.7135 - val_accuracy: 0.6327 - val_loss: 0.8238\n",
            "Epoch 29/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6857 - loss: 0.7129 - val_accuracy: 0.6265 - val_loss: 0.8258\n",
            "Epoch 30/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6852 - loss: 0.7087 - val_accuracy: 0.6308 - val_loss: 0.8257\n",
            "Epoch 31/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6825 - loss: 0.7107 - val_accuracy: 0.6241 - val_loss: 0.8306\n",
            "Epoch 32/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6743 - loss: 0.7106 - val_accuracy: 0.6251 - val_loss: 0.8331\n",
            "Epoch 33/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6890 - loss: 0.7030 - val_accuracy: 0.6235 - val_loss: 0.8308\n",
            "Epoch 34/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6891 - loss: 0.7025 - val_accuracy: 0.6281 - val_loss: 0.8307\n",
            "Epoch 35/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6956 - loss: 0.6972 - val_accuracy: 0.6292 - val_loss: 0.8280\n",
            "Epoch 36/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6940 - loss: 0.6935 - val_accuracy: 0.6324 - val_loss: 0.8314\n",
            "Epoch 37/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6946 - loss: 0.6905 - val_accuracy: 0.6251 - val_loss: 0.8395\n",
            "Epoch 38/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6972 - loss: 0.6924 - val_accuracy: 0.6330 - val_loss: 0.8302\n",
            "Epoch 39/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6940 - loss: 0.6909 - val_accuracy: 0.6273 - val_loss: 0.8326\n",
            "Epoch 40/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6968 - loss: 0.6883 - val_accuracy: 0.6284 - val_loss: 0.8391\n",
            "Epoch 41/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6907 - loss: 0.6857 - val_accuracy: 0.6311 - val_loss: 0.8359\n",
            "Epoch 42/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6997 - loss: 0.6780 - val_accuracy: 0.6251 - val_loss: 0.8370\n",
            "Epoch 43/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6993 - loss: 0.6827 - val_accuracy: 0.6106 - val_loss: 0.8475\n",
            "Epoch 44/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6995 - loss: 0.6762 - val_accuracy: 0.6316 - val_loss: 0.8337\n",
            "Epoch 45/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7001 - loss: 0.6775 - val_accuracy: 0.6327 - val_loss: 0.8423\n",
            "Epoch 46/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6985 - loss: 0.6753 - val_accuracy: 0.6241 - val_loss: 0.8422\n",
            "Epoch 47/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7041 - loss: 0.6696 - val_accuracy: 0.6227 - val_loss: 0.8459\n",
            "Epoch 48/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6999 - loss: 0.6744 - val_accuracy: 0.6281 - val_loss: 0.8453\n",
            "Epoch 49/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7030 - loss: 0.6662 - val_accuracy: 0.6127 - val_loss: 0.8582\n",
            "Epoch 50/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7061 - loss: 0.6704 - val_accuracy: 0.6278 - val_loss: 0.8467\n",
            "History for model: {'accuracy': [0.5299353003501892, 0.6045487523078918, 0.6146170496940613, 0.623336911201477, 0.627741813659668, 0.6356526613235474, 0.6374505758285522, 0.642125129699707, 0.6460805535316467, 0.6496763825416565, 0.6549802422523499, 0.6557892560958862, 0.6565084457397461, 0.6585760712623596, 0.6628910303115845, 0.6617224216461182, 0.6636102199554443, 0.6659474968910217, 0.6721503138542175, 0.667565643787384, 0.6707119941711426, 0.6705321669578552, 0.6708917617797852, 0.6738583445549011, 0.6783531308174133, 0.6755663156509399, 0.6755663156509399, 0.6815893650054932, 0.6789823770523071, 0.6834771633148193, 0.6847357153892517, 0.6802409291267395, 0.6858144402503967, 0.6871628761291504, 0.689949631690979, 0.689949631690979, 0.6904890537261963, 0.6934555768966675, 0.69183748960495, 0.6921071410179138, 0.695523202419281, 0.6961524486541748, 0.696601927280426, 0.698130190372467, 0.6957928538322449, 0.695523202419281, 0.6972312331199646, 0.7014563083648682, 0.7010068297386169, 0.7026249766349792], 'loss': [1.05995512008667, 0.868643045425415, 0.8398615121841431, 0.8237273097038269, 0.8114410042762756, 0.8018133044242859, 0.7958537340164185, 0.7898551225662231, 0.7836483120918274, 0.7784151434898376, 0.7752959132194519, 0.7706434726715088, 0.7670539617538452, 0.7606926560401917, 0.7578973174095154, 0.7549214363098145, 0.7504628896713257, 0.7481443881988525, 0.7445733547210693, 0.7401646971702576, 0.7392850518226624, 0.7365960478782654, 0.733185350894928, 0.7297504544258118, 0.727114200592041, 0.7244802117347717, 0.7226390838623047, 0.7175549268722534, 0.7152648568153381, 0.7122864723205566, 0.7092478275299072, 0.7090004682540894, 0.7061487436294556, 0.7034486532211304, 0.7024165391921997, 0.6983524560928345, 0.6953765153884888, 0.6919557452201843, 0.6910634636878967, 0.6889004707336426, 0.6858965754508972, 0.6846209168434143, 0.6824314594268799, 0.6802956461906433, 0.6796281337738037, 0.6788496971130371, 0.675507664680481, 0.6727649569511414, 0.6705712080001831, 0.6696718335151672], 'val_accuracy': [0.592772364616394, 0.6065264344215393, 0.606796145439148, 0.6189320683479309, 0.6186623573303223, 0.6262136101722717, 0.6243258118629456, 0.6251348257064819, 0.6313376426696777, 0.6221683025360107, 0.6267529726028442, 0.6251348257064819, 0.6245954632759094, 0.6291801333427429, 0.6302589178085327, 0.6248651742935181, 0.6143473386764526, 0.6148867607116699, 0.6343042254447937, 0.6251348257064819, 0.6294498443603516, 0.6291801333427429, 0.6194714307785034, 0.6245954632759094, 0.6305285692214966, 0.6367313861846924, 0.6302589178085327, 0.6326860785484314, 0.6264832615852356, 0.6307982802391052, 0.6240561008453369, 0.6251348257064819, 0.6235167384147644, 0.6281014084815979, 0.6291801333427429, 0.6324163675308228, 0.6251348257064819, 0.63295578956604, 0.6272923350334167, 0.6283710598945618, 0.6310679316520691, 0.6251348257064819, 0.6105717420578003, 0.6316073536872864, 0.6326860785484314, 0.6240561008453369, 0.6227076649665833, 0.6281014084815979, 0.6127292513847351, 0.6278316974639893], 'val_loss': [0.9047895669937134, 0.8644086718559265, 0.8578954935073853, 0.8399141430854797, 0.839467465877533, 0.8328853845596313, 0.8300431370735168, 0.8291715383529663, 0.8250856995582581, 0.827154278755188, 0.8305665254592896, 0.8220204710960388, 0.8269050121307373, 0.8209295272827148, 0.8236749172210693, 0.8240662813186646, 0.8324205279350281, 0.8297616839408875, 0.8213821649551392, 0.8246875405311584, 0.8236895203590393, 0.824271559715271, 0.8329207301139832, 0.8230493664741516, 0.8226089477539062, 0.8217270374298096, 0.8195928335189819, 0.8238267302513123, 0.8257880210876465, 0.8256726264953613, 0.8305819034576416, 0.8331348896026611, 0.8307946920394897, 0.8306944966316223, 0.8280465602874756, 0.8314483165740967, 0.8394939303398132, 0.8302215933799744, 0.8325831294059753, 0.8391023278236389, 0.8358941078186035, 0.8369855880737305, 0.8475175499916077, 0.8337404727935791, 0.842276394367218, 0.8421788215637207, 0.8459109663963318, 0.8452823162078857, 0.8582467436790466, 0.8466972708702087]}\n",
            "Trial: Neurons = 100, Dropout Rate = 0, Batch Size = 256\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4555 - loss: 1.2346 - val_accuracy: 0.5847 - val_loss: 0.9640\n",
            "Epoch 2/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5892 - loss: 0.9331 - val_accuracy: 0.5971 - val_loss: 0.8840\n",
            "Epoch 3/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6109 - loss: 0.8540 - val_accuracy: 0.6041 - val_loss: 0.8604\n",
            "Epoch 4/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6128 - loss: 0.8473 - val_accuracy: 0.6141 - val_loss: 0.8500\n",
            "Epoch 5/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6222 - loss: 0.8363 - val_accuracy: 0.6127 - val_loss: 0.8456\n",
            "Epoch 6/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6219 - loss: 0.8256 - val_accuracy: 0.6195 - val_loss: 0.8378\n",
            "Epoch 7/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6206 - loss: 0.8233 - val_accuracy: 0.6235 - val_loss: 0.8342\n",
            "Epoch 8/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6339 - loss: 0.8048 - val_accuracy: 0.6241 - val_loss: 0.8302\n",
            "Epoch 9/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6400 - loss: 0.7862 - val_accuracy: 0.6286 - val_loss: 0.8312\n",
            "Epoch 10/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6411 - loss: 0.7896 - val_accuracy: 0.6238 - val_loss: 0.8274\n",
            "Epoch 11/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6441 - loss: 0.7889 - val_accuracy: 0.6262 - val_loss: 0.8261\n",
            "Epoch 12/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6484 - loss: 0.7824 - val_accuracy: 0.6222 - val_loss: 0.8283\n",
            "Epoch 13/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6486 - loss: 0.7754 - val_accuracy: 0.6281 - val_loss: 0.8245\n",
            "Epoch 14/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6534 - loss: 0.7818 - val_accuracy: 0.6219 - val_loss: 0.8248\n",
            "Epoch 15/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6474 - loss: 0.7687 - val_accuracy: 0.6357 - val_loss: 0.8204\n",
            "Epoch 16/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6573 - loss: 0.7667 - val_accuracy: 0.6208 - val_loss: 0.8257\n",
            "Epoch 17/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6578 - loss: 0.7624 - val_accuracy: 0.6270 - val_loss: 0.8209\n",
            "Epoch 18/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6554 - loss: 0.7668 - val_accuracy: 0.6297 - val_loss: 0.8207\n",
            "Epoch 19/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6550 - loss: 0.7597 - val_accuracy: 0.6357 - val_loss: 0.8201\n",
            "Epoch 20/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6700 - loss: 0.7548 - val_accuracy: 0.6276 - val_loss: 0.8241\n",
            "Epoch 21/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6674 - loss: 0.7558 - val_accuracy: 0.6335 - val_loss: 0.8182\n",
            "Epoch 22/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6709 - loss: 0.7546 - val_accuracy: 0.6316 - val_loss: 0.8180\n",
            "Epoch 23/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6710 - loss: 0.7370 - val_accuracy: 0.6303 - val_loss: 0.8183\n",
            "Epoch 24/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6624 - loss: 0.7494 - val_accuracy: 0.6262 - val_loss: 0.8181\n",
            "Epoch 25/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6702 - loss: 0.7497 - val_accuracy: 0.6313 - val_loss: 0.8208\n",
            "Epoch 26/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6724 - loss: 0.7317 - val_accuracy: 0.6362 - val_loss: 0.8164\n",
            "Epoch 27/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6707 - loss: 0.7394 - val_accuracy: 0.6243 - val_loss: 0.8216\n",
            "Epoch 28/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6764 - loss: 0.7267 - val_accuracy: 0.6303 - val_loss: 0.8162\n",
            "Epoch 29/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6719 - loss: 0.7259 - val_accuracy: 0.6330 - val_loss: 0.8180\n",
            "Epoch 30/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6714 - loss: 0.7335 - val_accuracy: 0.6238 - val_loss: 0.8208\n",
            "Epoch 31/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6746 - loss: 0.7252 - val_accuracy: 0.6362 - val_loss: 0.8239\n",
            "Epoch 32/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6797 - loss: 0.7208 - val_accuracy: 0.6351 - val_loss: 0.8195\n",
            "Epoch 33/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6763 - loss: 0.7275 - val_accuracy: 0.6294 - val_loss: 0.8176\n",
            "Epoch 34/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6839 - loss: 0.7230 - val_accuracy: 0.6321 - val_loss: 0.8160\n",
            "Epoch 35/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6815 - loss: 0.7175 - val_accuracy: 0.6230 - val_loss: 0.8204\n",
            "Epoch 36/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6768 - loss: 0.7125 - val_accuracy: 0.6375 - val_loss: 0.8215\n",
            "Epoch 37/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6841 - loss: 0.7175 - val_accuracy: 0.6284 - val_loss: 0.8203\n",
            "Epoch 38/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6902 - loss: 0.7047 - val_accuracy: 0.6311 - val_loss: 0.8196\n",
            "Epoch 39/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6859 - loss: 0.7187 - val_accuracy: 0.6308 - val_loss: 0.8202\n",
            "Epoch 40/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6919 - loss: 0.7003 - val_accuracy: 0.6327 - val_loss: 0.8213\n",
            "Epoch 41/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6846 - loss: 0.7094 - val_accuracy: 0.6346 - val_loss: 0.8193\n",
            "Epoch 42/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6814 - loss: 0.7090 - val_accuracy: 0.6222 - val_loss: 0.8237\n",
            "Epoch 43/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6844 - loss: 0.7140 - val_accuracy: 0.6346 - val_loss: 0.8276\n",
            "Epoch 44/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6844 - loss: 0.6958 - val_accuracy: 0.6241 - val_loss: 0.8289\n",
            "Epoch 45/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6916 - loss: 0.7025 - val_accuracy: 0.6354 - val_loss: 0.8209\n",
            "Epoch 46/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6964 - loss: 0.6926 - val_accuracy: 0.6208 - val_loss: 0.8308\n",
            "Epoch 47/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6840 - loss: 0.7075 - val_accuracy: 0.6270 - val_loss: 0.8263\n",
            "Epoch 48/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6900 - loss: 0.6911 - val_accuracy: 0.6273 - val_loss: 0.8253\n",
            "Epoch 49/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7014 - loss: 0.6855 - val_accuracy: 0.6330 - val_loss: 0.8306\n",
            "Epoch 50/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6916 - loss: 0.6987 - val_accuracy: 0.6316 - val_loss: 0.8250\n",
            "History for model: {'accuracy': [0.5186983346939087, 0.5963681936264038, 0.6076051592826843, 0.6146170496940613, 0.6243258118629456, 0.6272923350334167, 0.6307982802391052, 0.6370909810066223, 0.6388888955116272, 0.6414958834648132, 0.6439230442047119, 0.6467996835708618, 0.6499460339546204, 0.6521934270858765, 0.6521035432815552, 0.6556993722915649, 0.6567781567573547, 0.6582164764404297, 0.658306360244751, 0.6628910303115845, 0.6628011465072632, 0.666037380695343, 0.6641495823860168, 0.6681050062179565, 0.669003963470459, 0.6673858165740967, 0.6691837310791016, 0.6724199652671814, 0.6684646010398865, 0.6755663156509399, 0.6732290387153625, 0.6721503138542175, 0.6785328984260559, 0.6791621446609497, 0.6797914505004883, 0.6810500025749207, 0.6797914505004883, 0.6814095377922058, 0.6842862367630005, 0.684825599193573, 0.6872527599334717, 0.6820388436317444, 0.6870729923248291, 0.682218611240387, 0.6898597478866577, 0.6895900964736938, 0.6863538026809692, 0.6902193427085876, 0.6911182999610901, 0.6912980675697327], 'loss': [1.1239761114120483, 0.9109156131744385, 0.8591368198394775, 0.8377511501312256, 0.8264217376708984, 0.8171595931053162, 0.8096837997436523, 0.8016703724861145, 0.7967056632041931, 0.7923211455345154, 0.787433385848999, 0.7835349440574646, 0.78097003698349, 0.7755550742149353, 0.7726192474365234, 0.7684791088104248, 0.7666921019554138, 0.7622312903404236, 0.7611194849014282, 0.7593803405761719, 0.7546374201774597, 0.7506086230278015, 0.7483062744140625, 0.7465274333953857, 0.7439277172088623, 0.7414879202842712, 0.7394141554832458, 0.735831081867218, 0.7348310351371765, 0.7300868034362793, 0.7282342910766602, 0.7309803366661072, 0.7257749438285828, 0.7221186757087708, 0.7202032804489136, 0.7178935408592224, 0.7166696786880493, 0.7169701457023621, 0.7128122448921204, 0.7118595242500305, 0.7092843651771545, 0.7106524705886841, 0.7060903906822205, 0.7047086358070374, 0.7032929062843323, 0.7013714909553528, 0.7023659348487854, 0.6963663101196289, 0.698658287525177, 0.6958784461021423], 'val_accuracy': [0.5846817493438721, 0.5970873832702637, 0.6040992736816406, 0.6140776872634888, 0.6127292513847351, 0.6194714307785034, 0.6235167384147644, 0.6240561008453369, 0.6286407709121704, 0.6237863898277283, 0.6262136101722717, 0.6221683025360107, 0.6281014084815979, 0.6218985915184021, 0.6356526613235474, 0.6208198666572571, 0.6270226240158081, 0.6297194957733154, 0.6356526613235474, 0.6275620460510254, 0.6334951519966125, 0.6316073536872864, 0.6302589178085327, 0.6262136101722717, 0.6313376426696777, 0.6361920237541199, 0.6243258118629456, 0.6302589178085327, 0.63295578956604, 0.6237863898277283, 0.6361920237541199, 0.6351132392883301, 0.6294498443603516, 0.6321467161178589, 0.6229773759841919, 0.6375404596328735, 0.6283710598945618, 0.6310679316520691, 0.6307982802391052, 0.6326860785484314, 0.6345738768577576, 0.6221683025360107, 0.6345738768577576, 0.6240561008453369, 0.6353829503059387, 0.6208198666572571, 0.6270226240158081, 0.6272923350334167, 0.63295578956604, 0.6316073536872864], 'val_loss': [0.96400386095047, 0.8840138912200928, 0.8604369163513184, 0.8500398397445679, 0.8456308841705322, 0.8377915620803833, 0.8341642618179321, 0.8301543593406677, 0.8312423825263977, 0.827431857585907, 0.8260632753372192, 0.8283407688140869, 0.8245008587837219, 0.8248177170753479, 0.8203862905502319, 0.8256712555885315, 0.8209341764450073, 0.8207361698150635, 0.8200774192810059, 0.8240852355957031, 0.8181819915771484, 0.8180108666419983, 0.8183228969573975, 0.8181474804878235, 0.8207724094390869, 0.8163560628890991, 0.8216418623924255, 0.8162083029747009, 0.8179685473442078, 0.8208349347114563, 0.8238967061042786, 0.8194717168807983, 0.8176227807998657, 0.8160117864608765, 0.8203761577606201, 0.8215391039848328, 0.8202512860298157, 0.8195821046829224, 0.8202243447303772, 0.8213180303573608, 0.8193389773368835, 0.823714554309845, 0.8276151418685913, 0.8288700580596924, 0.8209396004676819, 0.8307890295982361, 0.8263041973114014, 0.8252896070480347, 0.8306043744087219, 0.8249585628509521]}\n",
            "Trial: Neurons = 100, Dropout Rate = 0.1, Batch Size = 64\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4634 - loss: 1.1421 - val_accuracy: 0.5930 - val_loss: 0.8732\n",
            "Epoch 2/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5997 - loss: 0.8728 - val_accuracy: 0.6114 - val_loss: 0.8476\n",
            "Epoch 3/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6152 - loss: 0.8335 - val_accuracy: 0.6127 - val_loss: 0.8398\n",
            "Epoch 4/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6246 - loss: 0.8303 - val_accuracy: 0.6079 - val_loss: 0.8386\n",
            "Epoch 5/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6280 - loss: 0.8163 - val_accuracy: 0.6259 - val_loss: 0.8293\n",
            "Epoch 6/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6228 - loss: 0.8182 - val_accuracy: 0.6133 - val_loss: 0.8274\n",
            "Epoch 7/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6355 - loss: 0.7983 - val_accuracy: 0.6278 - val_loss: 0.8224\n",
            "Epoch 8/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6391 - loss: 0.7877 - val_accuracy: 0.6262 - val_loss: 0.8236\n",
            "Epoch 9/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6371 - loss: 0.7865 - val_accuracy: 0.6235 - val_loss: 0.8189\n",
            "Epoch 10/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6481 - loss: 0.7776 - val_accuracy: 0.6243 - val_loss: 0.8207\n",
            "Epoch 11/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6435 - loss: 0.7819 - val_accuracy: 0.6343 - val_loss: 0.8201\n",
            "Epoch 12/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6499 - loss: 0.7652 - val_accuracy: 0.6197 - val_loss: 0.8185\n",
            "Epoch 13/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6533 - loss: 0.7731 - val_accuracy: 0.6292 - val_loss: 0.8168\n",
            "Epoch 14/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6579 - loss: 0.7558 - val_accuracy: 0.6227 - val_loss: 0.8192\n",
            "Epoch 15/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6568 - loss: 0.7607 - val_accuracy: 0.6332 - val_loss: 0.8179\n",
            "Epoch 16/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6629 - loss: 0.7512 - val_accuracy: 0.6346 - val_loss: 0.8164\n",
            "Epoch 17/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6606 - loss: 0.7444 - val_accuracy: 0.6270 - val_loss: 0.8139\n",
            "Epoch 18/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6789 - loss: 0.7253 - val_accuracy: 0.6330 - val_loss: 0.8167\n",
            "Epoch 19/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6696 - loss: 0.7342 - val_accuracy: 0.6300 - val_loss: 0.8209\n",
            "Epoch 20/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6712 - loss: 0.7346 - val_accuracy: 0.6330 - val_loss: 0.8196\n",
            "Epoch 21/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6713 - loss: 0.7397 - val_accuracy: 0.6338 - val_loss: 0.8152\n",
            "Epoch 22/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6698 - loss: 0.7318 - val_accuracy: 0.6216 - val_loss: 0.8238\n",
            "Epoch 23/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6734 - loss: 0.7290 - val_accuracy: 0.6340 - val_loss: 0.8234\n",
            "Epoch 24/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6830 - loss: 0.7178 - val_accuracy: 0.6321 - val_loss: 0.8188\n",
            "Epoch 25/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6726 - loss: 0.7216 - val_accuracy: 0.6354 - val_loss: 0.8244\n",
            "Epoch 26/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6756 - loss: 0.7198 - val_accuracy: 0.6370 - val_loss: 0.8211\n",
            "Epoch 27/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6740 - loss: 0.7143 - val_accuracy: 0.6305 - val_loss: 0.8251\n",
            "Epoch 28/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6774 - loss: 0.7209 - val_accuracy: 0.6286 - val_loss: 0.8207\n",
            "Epoch 29/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6775 - loss: 0.7057 - val_accuracy: 0.6281 - val_loss: 0.8241\n",
            "Epoch 30/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6760 - loss: 0.7244 - val_accuracy: 0.6316 - val_loss: 0.8267\n",
            "Epoch 31/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6772 - loss: 0.7170 - val_accuracy: 0.6292 - val_loss: 0.8245\n",
            "Epoch 32/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6883 - loss: 0.6902 - val_accuracy: 0.6289 - val_loss: 0.8295\n",
            "Epoch 33/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6798 - loss: 0.7056 - val_accuracy: 0.6319 - val_loss: 0.8308\n",
            "Epoch 34/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6855 - loss: 0.6975 - val_accuracy: 0.6278 - val_loss: 0.8399\n",
            "Epoch 35/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6907 - loss: 0.6931 - val_accuracy: 0.6311 - val_loss: 0.8281\n",
            "Epoch 36/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6862 - loss: 0.6905 - val_accuracy: 0.6259 - val_loss: 0.8283\n",
            "Epoch 37/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6942 - loss: 0.6769 - val_accuracy: 0.6284 - val_loss: 0.8345\n",
            "Epoch 38/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6984 - loss: 0.6779 - val_accuracy: 0.6362 - val_loss: 0.8324\n",
            "Epoch 39/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6916 - loss: 0.6874 - val_accuracy: 0.6238 - val_loss: 0.8392\n",
            "Epoch 40/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7042 - loss: 0.6700 - val_accuracy: 0.6324 - val_loss: 0.8346\n",
            "Epoch 41/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6901 - loss: 0.6843 - val_accuracy: 0.6362 - val_loss: 0.8361\n",
            "Epoch 42/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7024 - loss: 0.6596 - val_accuracy: 0.6184 - val_loss: 0.8549\n",
            "Epoch 43/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6991 - loss: 0.6787 - val_accuracy: 0.6346 - val_loss: 0.8365\n",
            "Epoch 44/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6960 - loss: 0.6742 - val_accuracy: 0.6311 - val_loss: 0.8494\n",
            "Epoch 45/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6987 - loss: 0.6747 - val_accuracy: 0.6381 - val_loss: 0.8375\n",
            "Epoch 46/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6985 - loss: 0.6697 - val_accuracy: 0.6340 - val_loss: 0.8404\n",
            "Epoch 47/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6989 - loss: 0.6646 - val_accuracy: 0.6338 - val_loss: 0.8421\n",
            "Epoch 48/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7067 - loss: 0.6602 - val_accuracy: 0.6338 - val_loss: 0.8461\n",
            "Epoch 49/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7040 - loss: 0.6577 - val_accuracy: 0.6357 - val_loss: 0.8429\n",
            "Epoch 50/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7066 - loss: 0.6589 - val_accuracy: 0.6335 - val_loss: 0.8453\n",
            "History for model: {'accuracy': [0.5408126711845398, 0.6037396788597107, 0.6112909317016602, 0.6195613145828247, 0.628910481929779, 0.6269327402114868, 0.634394109249115, 0.6361021399497986, 0.6381697058677673, 0.6459906697273254, 0.647249162197113, 0.6499460339546204, 0.6550701260566711, 0.6541711688041687, 0.6582164764404297, 0.6608234643936157, 0.6613628268241882, 0.6650485396385193, 0.6625314354896545, 0.6656777858734131, 0.6721503138542175, 0.6669363379478455, 0.6739482283592224, 0.6759259104728699, 0.6755663156509399, 0.6747573018074036, 0.67304927110672, 0.6795217394828796, 0.6759259104728699, 0.6823084950447083, 0.6814095377922058, 0.6814095377922058, 0.6773642301559448, 0.6863538026809692, 0.6879719495773315, 0.6851851940155029, 0.6906688213348389, 0.6908485889434814, 0.6920172572135925, 0.6968716382980347, 0.6897698640823364, 0.694084882736206, 0.6936354041099548, 0.6964221596717834, 0.6988493204116821, 0.6965120434761047, 0.7018159031867981, 0.7003775835037231, 0.7003775835037231, 0.7055016160011292], 'loss': [0.9999374151229858, 0.8634167313575745, 0.8427648544311523, 0.8287391662597656, 0.8149358034133911, 0.8090246319770813, 0.7986898422241211, 0.7929201126098633, 0.7869095802307129, 0.7798822522163391, 0.7759691476821899, 0.7710503339767456, 0.7682089805603027, 0.7608238458633423, 0.758683443069458, 0.753751277923584, 0.7485546469688416, 0.7473012804985046, 0.7440650463104248, 0.7392709851264954, 0.7326789498329163, 0.7328700423240662, 0.7278941869735718, 0.7256780862808228, 0.7227453589439392, 0.7234117984771729, 0.719057023525238, 0.7152647972106934, 0.71286940574646, 0.71324622631073, 0.7085386514663696, 0.7065982222557068, 0.705678403377533, 0.696792721748352, 0.6955661773681641, 0.6958718299865723, 0.6903904676437378, 0.6897503137588501, 0.6863594651222229, 0.6807985305786133, 0.6837707161903381, 0.6782326698303223, 0.6816915273666382, 0.6762809753417969, 0.6737986207008362, 0.6715215444564819, 0.6709528565406799, 0.6701648831367493, 0.6639548540115356, 0.6621837615966797], 'val_accuracy': [0.5930420756340027, 0.6113808155059814, 0.6127292513847351, 0.607874870300293, 0.6259438991546631, 0.6132686138153076, 0.6278316974639893, 0.6262136101722717, 0.6235167384147644, 0.6243258118629456, 0.6343042254447937, 0.6197410821914673, 0.6291801333427429, 0.6227076649665833, 0.6332254409790039, 0.6345738768577576, 0.6270226240158081, 0.63295578956604, 0.6299892067909241, 0.63295578956604, 0.6337648034095764, 0.6216289401054382, 0.6340345144271851, 0.6321467161178589, 0.6353829503059387, 0.637001097202301, 0.6305285692214966, 0.6286407709121704, 0.6281014084815979, 0.6316073536872864, 0.6291801333427429, 0.628910481929779, 0.6318770051002502, 0.6278316974639893, 0.6310679316520691, 0.6259438991546631, 0.6283710598945618, 0.6361920237541199, 0.6237863898277283, 0.6324163675308228, 0.6361920237541199, 0.6183926463127136, 0.6345738768577576, 0.6310679316520691, 0.638079822063446, 0.6340345144271851, 0.6337648034095764, 0.6337648034095764, 0.6356526613235474, 0.6334951519966125], 'val_loss': [0.8731907606124878, 0.847618043422699, 0.8398476839065552, 0.8385801911354065, 0.8293038010597229, 0.8274021744728088, 0.822380542755127, 0.8235838413238525, 0.8188772201538086, 0.8206751942634583, 0.8201329708099365, 0.8185465335845947, 0.8168323636054993, 0.8192203640937805, 0.8178886771202087, 0.8163554072380066, 0.8139005899429321, 0.8167281746864319, 0.8208928108215332, 0.8195875883102417, 0.8151726126670837, 0.8237752318382263, 0.8233940005302429, 0.8188015818595886, 0.8244240283966064, 0.8210512399673462, 0.8251073360443115, 0.8206759691238403, 0.8240528702735901, 0.8267384171485901, 0.8244842886924744, 0.8295498490333557, 0.8308368921279907, 0.8399240970611572, 0.8281213641166687, 0.8283376097679138, 0.834511935710907, 0.8324236869812012, 0.8392156362533569, 0.8346090316772461, 0.8360677361488342, 0.8548761606216431, 0.836533784866333, 0.8494468927383423, 0.8375451564788818, 0.8403742909431458, 0.8421037793159485, 0.8460593819618225, 0.8429123759269714, 0.8453085422515869]}\n",
            "Trial: Neurons = 100, Dropout Rate = 0.1, Batch Size = 128\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4231 - loss: 1.2406 - val_accuracy: 0.5839 - val_loss: 0.9138\n",
            "Epoch 2/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5826 - loss: 0.8964 - val_accuracy: 0.5987 - val_loss: 0.8692\n",
            "Epoch 3/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6000 - loss: 0.8640 - val_accuracy: 0.6079 - val_loss: 0.8531\n",
            "Epoch 4/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6135 - loss: 0.8437 - val_accuracy: 0.6122 - val_loss: 0.8460\n",
            "Epoch 5/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6140 - loss: 0.8363 - val_accuracy: 0.6211 - val_loss: 0.8378\n",
            "Epoch 6/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6254 - loss: 0.8214 - val_accuracy: 0.6208 - val_loss: 0.8356\n",
            "Epoch 7/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6284 - loss: 0.8123 - val_accuracy: 0.6200 - val_loss: 0.8325\n",
            "Epoch 8/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6314 - loss: 0.8094 - val_accuracy: 0.6119 - val_loss: 0.8334\n",
            "Epoch 9/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6290 - loss: 0.8154 - val_accuracy: 0.6235 - val_loss: 0.8264\n",
            "Epoch 10/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6353 - loss: 0.7937 - val_accuracy: 0.6241 - val_loss: 0.8251\n",
            "Epoch 11/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6406 - loss: 0.7902 - val_accuracy: 0.6294 - val_loss: 0.8206\n",
            "Epoch 12/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6424 - loss: 0.7809 - val_accuracy: 0.6340 - val_loss: 0.8199\n",
            "Epoch 13/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6510 - loss: 0.7769 - val_accuracy: 0.6270 - val_loss: 0.8190\n",
            "Epoch 14/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6497 - loss: 0.7776 - val_accuracy: 0.6313 - val_loss: 0.8178\n",
            "Epoch 15/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6578 - loss: 0.7647 - val_accuracy: 0.6308 - val_loss: 0.8180\n",
            "Epoch 16/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6447 - loss: 0.7786 - val_accuracy: 0.6370 - val_loss: 0.8191\n",
            "Epoch 17/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6518 - loss: 0.7672 - val_accuracy: 0.6343 - val_loss: 0.8150\n",
            "Epoch 18/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6604 - loss: 0.7577 - val_accuracy: 0.6321 - val_loss: 0.8161\n",
            "Epoch 19/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6605 - loss: 0.7485 - val_accuracy: 0.6268 - val_loss: 0.8134\n",
            "Epoch 20/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6690 - loss: 0.7479 - val_accuracy: 0.6324 - val_loss: 0.8133\n",
            "Epoch 21/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6641 - loss: 0.7522 - val_accuracy: 0.6311 - val_loss: 0.8166\n",
            "Epoch 22/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6751 - loss: 0.7449 - val_accuracy: 0.6343 - val_loss: 0.8146\n",
            "Epoch 23/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6779 - loss: 0.7318 - val_accuracy: 0.6313 - val_loss: 0.8136\n",
            "Epoch 24/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6636 - loss: 0.7539 - val_accuracy: 0.6338 - val_loss: 0.8136\n",
            "Epoch 25/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6653 - loss: 0.7404 - val_accuracy: 0.6316 - val_loss: 0.8123\n",
            "Epoch 26/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6678 - loss: 0.7493 - val_accuracy: 0.6259 - val_loss: 0.8180\n",
            "Epoch 27/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6621 - loss: 0.7338 - val_accuracy: 0.6413 - val_loss: 0.8133\n",
            "Epoch 28/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6726 - loss: 0.7315 - val_accuracy: 0.6335 - val_loss: 0.8128\n",
            "Epoch 29/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6758 - loss: 0.7267 - val_accuracy: 0.6373 - val_loss: 0.8128\n",
            "Epoch 30/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6806 - loss: 0.7183 - val_accuracy: 0.6284 - val_loss: 0.8197\n",
            "Epoch 31/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6813 - loss: 0.7261 - val_accuracy: 0.6340 - val_loss: 0.8204\n",
            "Epoch 32/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6735 - loss: 0.7293 - val_accuracy: 0.6354 - val_loss: 0.8190\n",
            "Epoch 33/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6821 - loss: 0.7066 - val_accuracy: 0.6346 - val_loss: 0.8157\n",
            "Epoch 34/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6779 - loss: 0.7184 - val_accuracy: 0.6319 - val_loss: 0.8205\n",
            "Epoch 35/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6916 - loss: 0.7043 - val_accuracy: 0.6370 - val_loss: 0.8171\n",
            "Epoch 36/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6791 - loss: 0.7065 - val_accuracy: 0.6346 - val_loss: 0.8220\n",
            "Epoch 37/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6768 - loss: 0.7127 - val_accuracy: 0.6259 - val_loss: 0.8245\n",
            "Epoch 38/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6929 - loss: 0.6946 - val_accuracy: 0.6357 - val_loss: 0.8197\n",
            "Epoch 39/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6858 - loss: 0.7004 - val_accuracy: 0.6373 - val_loss: 0.8225\n",
            "Epoch 40/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6928 - loss: 0.7008 - val_accuracy: 0.6319 - val_loss: 0.8237\n",
            "Epoch 41/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6901 - loss: 0.7027 - val_accuracy: 0.6254 - val_loss: 0.8272\n",
            "Epoch 42/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6843 - loss: 0.6991 - val_accuracy: 0.6330 - val_loss: 0.8228\n",
            "Epoch 43/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6853 - loss: 0.6994 - val_accuracy: 0.6343 - val_loss: 0.8252\n",
            "Epoch 44/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6875 - loss: 0.7007 - val_accuracy: 0.6338 - val_loss: 0.8248\n",
            "Epoch 45/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6916 - loss: 0.6942 - val_accuracy: 0.6362 - val_loss: 0.8237\n",
            "Epoch 46/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6893 - loss: 0.6872 - val_accuracy: 0.6354 - val_loss: 0.8265\n",
            "Epoch 47/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6903 - loss: 0.6940 - val_accuracy: 0.6219 - val_loss: 0.8364\n",
            "Epoch 48/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6926 - loss: 0.6894 - val_accuracy: 0.6262 - val_loss: 0.8334\n",
            "Epoch 49/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6872 - loss: 0.6933 - val_accuracy: 0.6300 - val_loss: 0.8338\n",
            "Epoch 50/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6910 - loss: 0.7011 - val_accuracy: 0.6308 - val_loss: 0.8376\n",
            "History for model: {'accuracy': [0.5061129331588745, 0.5848615765571594, 0.6023013591766357, 0.6097626686096191, 0.621179461479187, 0.6206400394439697, 0.6254045367240906, 0.6294498443603516, 0.6326860785484314, 0.6347537040710449, 0.6403272151947021, 0.6415857672691345, 0.6459007263183594, 0.6489571928977966, 0.6512045860290527, 0.6484178304672241, 0.6507551074028015, 0.6595649123191833, 0.6590255498886108, 0.6598345637321472, 0.6615425944328308, 0.6654980182647705, 0.6665767431259155, 0.6637899875640869, 0.6651384234428406, 0.6670262217521667, 0.6658576130867004, 0.6701725721359253, 0.6713412404060364, 0.6760157942771912, 0.6746673583984375, 0.6744875907897949, 0.6769147515296936, 0.6770046949386597, 0.6816792488098145, 0.6780834197998047, 0.6787126660346985, 0.6832074522972107, 0.6857245564460754, 0.6839266419410706, 0.6854548454284668, 0.6829378008842468, 0.685904324054718, 0.6892305016517639, 0.6858144402503967, 0.687792181968689, 0.6912081837654114, 0.6891406178474426, 0.6881517171859741, 0.6886011958122253], 'loss': [1.094900131225586, 0.8887919783592224, 0.8569197058677673, 0.8425037264823914, 0.8282443881034851, 0.8233485221862793, 0.8155837655067444, 0.8119721412658691, 0.8067426085472107, 0.7974861860275269, 0.7891668081283569, 0.7858775854110718, 0.7826404571533203, 0.7754786014556885, 0.7755086421966553, 0.7713753581047058, 0.7680256962776184, 0.7615586519241333, 0.7594377398490906, 0.7568327784538269, 0.7535758018493652, 0.7501413226127625, 0.7450533509254456, 0.7456021308898926, 0.7449905276298523, 0.7373141646385193, 0.7376153469085693, 0.7321089506149292, 0.7323921322822571, 0.7276581525802612, 0.7264888286590576, 0.7266254425048828, 0.7216070294380188, 0.7196962833404541, 0.715900719165802, 0.7145007848739624, 0.7140079140663147, 0.7115688323974609, 0.7081067562103271, 0.706733763217926, 0.7054620385169983, 0.699324369430542, 0.7015511393547058, 0.6979124546051025, 0.6984139680862427, 0.695547878742218, 0.6932040452957153, 0.6925183534622192, 0.6921129822731018, 0.6874648928642273], 'val_accuracy': [0.5838727355003357, 0.598705530166626, 0.607874870300293, 0.6121898889541626, 0.621089518070221, 0.6208198666572571, 0.6200107932090759, 0.611920177936554, 0.6235167384147644, 0.6240561008453369, 0.6294498443603516, 0.6340345144271851, 0.6270226240158081, 0.6313376426696777, 0.6307982802391052, 0.637001097202301, 0.6343042254447937, 0.6321467161178589, 0.6267529726028442, 0.6324163675308228, 0.6310679316520691, 0.6343042254447937, 0.6313376426696777, 0.6337648034095764, 0.6316073536872864, 0.6259438991546631, 0.6413160562515259, 0.6334951519966125, 0.6372707486152649, 0.6283710598945618, 0.6340345144271851, 0.6353829503059387, 0.6345738768577576, 0.6318770051002502, 0.637001097202301, 0.6345738768577576, 0.6259438991546631, 0.6356526613235474, 0.6372707486152649, 0.6318770051002502, 0.6254045367240906, 0.63295578956604, 0.6343042254447937, 0.6337648034095764, 0.6361920237541199, 0.6353829503059387, 0.6218985915184021, 0.6262136101722717, 0.6299892067909241, 0.6307982802391052], 'val_loss': [0.9138296246528625, 0.8692218065261841, 0.8530519008636475, 0.8459895849227905, 0.837776780128479, 0.8355512022972107, 0.8324522972106934, 0.8333916664123535, 0.8263512253761292, 0.8250797390937805, 0.820636510848999, 0.819852888584137, 0.8190325498580933, 0.817759096622467, 0.8179841041564941, 0.8190982341766357, 0.8150143027305603, 0.8160701394081116, 0.8133826851844788, 0.8132614493370056, 0.8165932297706604, 0.8146347403526306, 0.8136163353919983, 0.8136042356491089, 0.8122641444206238, 0.8179713487625122, 0.8132721781730652, 0.8128064274787903, 0.8127578496932983, 0.8197427988052368, 0.8203610777854919, 0.819014847278595, 0.8157209753990173, 0.8205368518829346, 0.8171331882476807, 0.8219515085220337, 0.8244854807853699, 0.819735050201416, 0.8225411772727966, 0.8236973881721497, 0.827194333076477, 0.8228006958961487, 0.8252410292625427, 0.8247724175453186, 0.8237079977989197, 0.8265400528907776, 0.8364370465278625, 0.8334373831748962, 0.8337588310241699, 0.8376464247703552]}\n",
            "Trial: Neurons = 100, Dropout Rate = 0.1, Batch Size = 256\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3332 - loss: 1.4228 - val_accuracy: 0.5569 - val_loss: 1.0217\n",
            "Epoch 2/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5685 - loss: 0.9768 - val_accuracy: 0.5858 - val_loss: 0.9008\n",
            "Epoch 3/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5815 - loss: 0.8949 - val_accuracy: 0.6003 - val_loss: 0.8722\n",
            "Epoch 4/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6054 - loss: 0.8621 - val_accuracy: 0.6090 - val_loss: 0.8594\n",
            "Epoch 5/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5998 - loss: 0.8535 - val_accuracy: 0.6111 - val_loss: 0.8541\n",
            "Epoch 6/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6138 - loss: 0.8372 - val_accuracy: 0.6165 - val_loss: 0.8466\n",
            "Epoch 7/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6190 - loss: 0.8242 - val_accuracy: 0.6192 - val_loss: 0.8405\n",
            "Epoch 8/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6209 - loss: 0.8220 - val_accuracy: 0.6195 - val_loss: 0.8401\n",
            "Epoch 9/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6273 - loss: 0.8204 - val_accuracy: 0.6238 - val_loss: 0.8362\n",
            "Epoch 10/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6231 - loss: 0.8102 - val_accuracy: 0.6251 - val_loss: 0.8333\n",
            "Epoch 11/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6302 - loss: 0.8012 - val_accuracy: 0.6268 - val_loss: 0.8318\n",
            "Epoch 12/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6373 - loss: 0.7995 - val_accuracy: 0.6286 - val_loss: 0.8304\n",
            "Epoch 13/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6392 - loss: 0.7912 - val_accuracy: 0.6262 - val_loss: 0.8270\n",
            "Epoch 14/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6324 - loss: 0.7931 - val_accuracy: 0.6284 - val_loss: 0.8267\n",
            "Epoch 15/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6415 - loss: 0.7893 - val_accuracy: 0.6286 - val_loss: 0.8242\n",
            "Epoch 16/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6505 - loss: 0.7846 - val_accuracy: 0.6305 - val_loss: 0.8256\n",
            "Epoch 17/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6443 - loss: 0.7811 - val_accuracy: 0.6313 - val_loss: 0.8206\n",
            "Epoch 18/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6520 - loss: 0.7853 - val_accuracy: 0.6235 - val_loss: 0.8230\n",
            "Epoch 19/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6387 - loss: 0.7842 - val_accuracy: 0.6332 - val_loss: 0.8221\n",
            "Epoch 20/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6517 - loss: 0.7680 - val_accuracy: 0.6297 - val_loss: 0.8179\n",
            "Epoch 21/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6573 - loss: 0.7700 - val_accuracy: 0.6270 - val_loss: 0.8205\n",
            "Epoch 22/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6561 - loss: 0.7670 - val_accuracy: 0.6324 - val_loss: 0.8193\n",
            "Epoch 23/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6610 - loss: 0.7570 - val_accuracy: 0.6327 - val_loss: 0.8169\n",
            "Epoch 24/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6539 - loss: 0.7668 - val_accuracy: 0.6338 - val_loss: 0.8185\n",
            "Epoch 25/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6540 - loss: 0.7696 - val_accuracy: 0.6389 - val_loss: 0.8140\n",
            "Epoch 26/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6599 - loss: 0.7558 - val_accuracy: 0.6365 - val_loss: 0.8156\n",
            "Epoch 27/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6654 - loss: 0.7523 - val_accuracy: 0.6386 - val_loss: 0.8156\n",
            "Epoch 28/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6624 - loss: 0.7497 - val_accuracy: 0.6348 - val_loss: 0.8167\n",
            "Epoch 29/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6691 - loss: 0.7383 - val_accuracy: 0.6316 - val_loss: 0.8156\n",
            "Epoch 30/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6681 - loss: 0.7370 - val_accuracy: 0.6316 - val_loss: 0.8150\n",
            "Epoch 31/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6649 - loss: 0.7436 - val_accuracy: 0.6340 - val_loss: 0.8153\n",
            "Epoch 32/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6655 - loss: 0.7445 - val_accuracy: 0.6338 - val_loss: 0.8148\n",
            "Epoch 33/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6660 - loss: 0.7416 - val_accuracy: 0.6381 - val_loss: 0.8132\n",
            "Epoch 34/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6665 - loss: 0.7419 - val_accuracy: 0.6300 - val_loss: 0.8147\n",
            "Epoch 35/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6685 - loss: 0.7387 - val_accuracy: 0.6321 - val_loss: 0.8172\n",
            "Epoch 36/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6682 - loss: 0.7401 - val_accuracy: 0.6348 - val_loss: 0.8117\n",
            "Epoch 37/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6749 - loss: 0.7281 - val_accuracy: 0.6338 - val_loss: 0.8147\n",
            "Epoch 38/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6762 - loss: 0.7337 - val_accuracy: 0.6281 - val_loss: 0.8172\n",
            "Epoch 39/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6757 - loss: 0.7330 - val_accuracy: 0.6300 - val_loss: 0.8160\n",
            "Epoch 40/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6743 - loss: 0.7217 - val_accuracy: 0.6354 - val_loss: 0.8162\n",
            "Epoch 41/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6706 - loss: 0.7267 - val_accuracy: 0.6335 - val_loss: 0.8141\n",
            "Epoch 42/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6764 - loss: 0.7265 - val_accuracy: 0.6324 - val_loss: 0.8181\n",
            "Epoch 43/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6749 - loss: 0.7286 - val_accuracy: 0.6357 - val_loss: 0.8166\n",
            "Epoch 44/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6805 - loss: 0.7074 - val_accuracy: 0.6305 - val_loss: 0.8190\n",
            "Epoch 45/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6798 - loss: 0.7145 - val_accuracy: 0.6383 - val_loss: 0.8184\n",
            "Epoch 46/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6848 - loss: 0.7061 - val_accuracy: 0.6241 - val_loss: 0.8160\n",
            "Epoch 47/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6754 - loss: 0.7189 - val_accuracy: 0.6327 - val_loss: 0.8199\n",
            "Epoch 48/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6791 - loss: 0.7131 - val_accuracy: 0.6316 - val_loss: 0.8181\n",
            "Epoch 49/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6777 - loss: 0.7218 - val_accuracy: 0.6241 - val_loss: 0.8223\n",
            "Epoch 50/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6783 - loss: 0.7204 - val_accuracy: 0.6308 - val_loss: 0.8200\n",
            "History for model: {'accuracy': [0.4268248975276947, 0.5728155374526978, 0.5872887372970581, 0.600862979888916, 0.6076051592826843, 0.6111111044883728, 0.6152462959289551, 0.6225278973579407, 0.621089518070221, 0.6246853470802307, 0.6272024512290955, 0.6318770051002502, 0.6367313861846924, 0.6359223127365112, 0.6391585469245911, 0.6441927552223206, 0.6467996835708618, 0.647608757019043, 0.6444624066352844, 0.6494066715240479, 0.6547105312347412, 0.6538115739822388, 0.6545307636260986, 0.6550701260566711, 0.6569579243659973, 0.657227635383606, 0.6565983295440674, 0.6625314354896545, 0.6645091772079468, 0.660913348197937, 0.6636102199554443, 0.6645990610122681, 0.6606436371803284, 0.6667565703392029, 0.6673858165740967, 0.6681050062179565, 0.6713412404060364, 0.6731391549110413, 0.6698130369186401, 0.6697230935096741, 0.6698130369186401, 0.6761057376861572, 0.6734987497329712, 0.6750269532203674, 0.6779036521911621, 0.6782631874084473, 0.6804206967353821, 0.6768248677253723, 0.6771844625473022, 0.6789823770523071], 'loss': [1.2625759840011597, 0.9540790915489197, 0.8859475255012512, 0.8619760870933533, 0.8504206538200378, 0.8390782475471497, 0.8284160494804382, 0.8199995160102844, 0.8183010816574097, 0.8114274144172668, 0.8067094683647156, 0.8010043501853943, 0.7945441007614136, 0.7931806445121765, 0.788914680480957, 0.7873217463493347, 0.7816787362098694, 0.778551459312439, 0.7762306928634644, 0.7753286361694336, 0.7703448534011841, 0.7712740302085876, 0.7645320892333984, 0.761939525604248, 0.7604406476020813, 0.7630500793457031, 0.758948028087616, 0.7529067993164062, 0.7528113722801208, 0.7485830783843994, 0.7468066811561584, 0.746244490146637, 0.7457300424575806, 0.7421765327453613, 0.7382782697677612, 0.7395035624504089, 0.7353566884994507, 0.7333884835243225, 0.7338827848434448, 0.7303953766822815, 0.7292592525482178, 0.7246677875518799, 0.7252059578895569, 0.7234693765640259, 0.7181212306022644, 0.7181838750839233, 0.7163731455802917, 0.7193220257759094, 0.7197253108024597, 0.7144351601600647], 'val_accuracy': [0.5569040179252625, 0.5857605338096619, 0.6003236174583435, 0.608953595161438, 0.6111111044883728, 0.6165048480033875, 0.6192017197608948, 0.6194714307785034, 0.6237863898277283, 0.6251348257064819, 0.6267529726028442, 0.6286407709121704, 0.6262136101722717, 0.6283710598945618, 0.6286407709121704, 0.6305285692214966, 0.6313376426696777, 0.6235167384147644, 0.6332254409790039, 0.6297194957733154, 0.6270226240158081, 0.6324163675308228, 0.6326860785484314, 0.6337648034095764, 0.6388888955116272, 0.6364616751670837, 0.6386191844940186, 0.6348435878753662, 0.6316073536872864, 0.6316073536872864, 0.6340345144271851, 0.6337648034095764, 0.638079822063446, 0.6299892067909241, 0.6321467161178589, 0.6348435878753662, 0.6337648034095764, 0.6281014084815979, 0.6299892067909241, 0.6353829503059387, 0.6334951519966125, 0.6324163675308228, 0.6356526613235474, 0.6305285692214966, 0.6383495330810547, 0.6240561008453369, 0.6326860785484314, 0.6316073536872864, 0.6240561008453369, 0.6307982802391052], 'val_loss': [1.021699070930481, 0.9007683396339417, 0.8721696734428406, 0.8593642115592957, 0.8541191220283508, 0.8465781807899475, 0.8405435085296631, 0.8401328325271606, 0.8362040519714355, 0.8332502841949463, 0.8318380117416382, 0.8304083943367004, 0.827023446559906, 0.8267351984977722, 0.8241766095161438, 0.825597882270813, 0.820607602596283, 0.8230096101760864, 0.8220885396003723, 0.8178709149360657, 0.8205138444900513, 0.8192955851554871, 0.816904604434967, 0.8185452222824097, 0.8140144944190979, 0.8156242370605469, 0.815560519695282, 0.8166993260383606, 0.8156489133834839, 0.814980149269104, 0.8153061270713806, 0.8147603273391724, 0.8131536245346069, 0.814667820930481, 0.8171569108963013, 0.8117028474807739, 0.8146721124649048, 0.8171715140342712, 0.8160142302513123, 0.8161659836769104, 0.814091682434082, 0.8180900812149048, 0.8165712952613831, 0.8190222978591919, 0.818424642086029, 0.8159889578819275, 0.8199250102043152, 0.818123996257782, 0.8222882151603699, 0.819994330406189]}\n",
            "Trial: Neurons = 100, Dropout Rate = 0.5, Batch Size = 64\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4098 - loss: 1.2808 - val_accuracy: 0.5914 - val_loss: 0.9005\n",
            "Epoch 2/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5776 - loss: 0.9263 - val_accuracy: 0.6087 - val_loss: 0.8612\n",
            "Epoch 3/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5908 - loss: 0.8884 - val_accuracy: 0.6181 - val_loss: 0.8489\n",
            "Epoch 4/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6060 - loss: 0.8680 - val_accuracy: 0.6257 - val_loss: 0.8415\n",
            "Epoch 5/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6107 - loss: 0.8655 - val_accuracy: 0.6211 - val_loss: 0.8369\n",
            "Epoch 6/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6212 - loss: 0.8383 - val_accuracy: 0.6270 - val_loss: 0.8359\n",
            "Epoch 7/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6209 - loss: 0.8325 - val_accuracy: 0.6268 - val_loss: 0.8298\n",
            "Epoch 8/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6322 - loss: 0.8303 - val_accuracy: 0.6303 - val_loss: 0.8257\n",
            "Epoch 9/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6112 - loss: 0.8339 - val_accuracy: 0.6262 - val_loss: 0.8234\n",
            "Epoch 10/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6349 - loss: 0.8102 - val_accuracy: 0.6278 - val_loss: 0.8219\n",
            "Epoch 11/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6341 - loss: 0.8036 - val_accuracy: 0.6311 - val_loss: 0.8204\n",
            "Epoch 12/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6434 - loss: 0.7999 - val_accuracy: 0.6321 - val_loss: 0.8191\n",
            "Epoch 13/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6416 - loss: 0.7956 - val_accuracy: 0.6330 - val_loss: 0.8196\n",
            "Epoch 14/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6317 - loss: 0.8041 - val_accuracy: 0.6354 - val_loss: 0.8161\n",
            "Epoch 15/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6380 - loss: 0.8018 - val_accuracy: 0.6319 - val_loss: 0.8171\n",
            "Epoch 16/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6540 - loss: 0.7822 - val_accuracy: 0.6340 - val_loss: 0.8165\n",
            "Epoch 17/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6466 - loss: 0.7872 - val_accuracy: 0.6327 - val_loss: 0.8146\n",
            "Epoch 18/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6450 - loss: 0.7964 - val_accuracy: 0.6389 - val_loss: 0.8147\n",
            "Epoch 19/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6481 - loss: 0.7805 - val_accuracy: 0.6332 - val_loss: 0.8137\n",
            "Epoch 20/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6477 - loss: 0.7731 - val_accuracy: 0.6365 - val_loss: 0.8120\n",
            "Epoch 21/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6527 - loss: 0.7750 - val_accuracy: 0.6370 - val_loss: 0.8120\n",
            "Epoch 22/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6535 - loss: 0.7653 - val_accuracy: 0.6308 - val_loss: 0.8148\n",
            "Epoch 23/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6555 - loss: 0.7723 - val_accuracy: 0.6289 - val_loss: 0.8144\n",
            "Epoch 24/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6584 - loss: 0.7582 - val_accuracy: 0.6343 - val_loss: 0.8112\n",
            "Epoch 25/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6566 - loss: 0.7611 - val_accuracy: 0.6359 - val_loss: 0.8097\n",
            "Epoch 26/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6594 - loss: 0.7627 - val_accuracy: 0.6373 - val_loss: 0.8111\n",
            "Epoch 27/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6670 - loss: 0.7450 - val_accuracy: 0.6378 - val_loss: 0.8113\n",
            "Epoch 28/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6560 - loss: 0.7526 - val_accuracy: 0.6367 - val_loss: 0.8118\n",
            "Epoch 29/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6607 - loss: 0.7546 - val_accuracy: 0.6351 - val_loss: 0.8104\n",
            "Epoch 30/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6559 - loss: 0.7586 - val_accuracy: 0.6405 - val_loss: 0.8108\n",
            "Epoch 31/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6683 - loss: 0.7456 - val_accuracy: 0.6332 - val_loss: 0.8145\n",
            "Epoch 32/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6577 - loss: 0.7554 - val_accuracy: 0.6383 - val_loss: 0.8134\n",
            "Epoch 33/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6629 - loss: 0.7543 - val_accuracy: 0.6362 - val_loss: 0.8134\n",
            "Epoch 34/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6619 - loss: 0.7454 - val_accuracy: 0.6440 - val_loss: 0.8121\n",
            "Epoch 35/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6718 - loss: 0.7349 - val_accuracy: 0.6297 - val_loss: 0.8152\n",
            "Epoch 36/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6741 - loss: 0.7346 - val_accuracy: 0.6351 - val_loss: 0.8178\n",
            "Epoch 37/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6779 - loss: 0.7244 - val_accuracy: 0.6410 - val_loss: 0.8148\n",
            "Epoch 38/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6692 - loss: 0.7282 - val_accuracy: 0.6362 - val_loss: 0.8120\n",
            "Epoch 39/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6687 - loss: 0.7282 - val_accuracy: 0.6346 - val_loss: 0.8146\n",
            "Epoch 40/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6696 - loss: 0.7370 - val_accuracy: 0.6400 - val_loss: 0.8198\n",
            "Epoch 41/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6731 - loss: 0.7245 - val_accuracy: 0.6378 - val_loss: 0.8151\n",
            "Epoch 42/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6795 - loss: 0.7200 - val_accuracy: 0.6381 - val_loss: 0.8133\n",
            "Epoch 43/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6781 - loss: 0.7128 - val_accuracy: 0.6357 - val_loss: 0.8208\n",
            "Epoch 44/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6760 - loss: 0.7223 - val_accuracy: 0.6362 - val_loss: 0.8195\n",
            "Epoch 45/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6759 - loss: 0.7285 - val_accuracy: 0.6365 - val_loss: 0.8258\n",
            "Epoch 46/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6716 - loss: 0.7211 - val_accuracy: 0.6346 - val_loss: 0.8213\n",
            "Epoch 47/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6840 - loss: 0.7019 - val_accuracy: 0.6394 - val_loss: 0.8188\n",
            "Epoch 48/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6761 - loss: 0.7212 - val_accuracy: 0.6365 - val_loss: 0.8231\n",
            "Epoch 49/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6757 - loss: 0.7236 - val_accuracy: 0.6429 - val_loss: 0.8209\n",
            "Epoch 50/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6742 - loss: 0.7160 - val_accuracy: 0.6378 - val_loss: 0.8188\n",
            "History for model: {'accuracy': [0.4942466616630554, 0.5765911340713501, 0.5966379046440125, 0.6069759130477905, 0.6103919744491577, 0.6232470273971558, 0.6217188239097595, 0.6268428564071655, 0.6284610033035278, 0.6334052681922913, 0.6349334716796875, 0.639518141746521, 0.6405069828033447, 0.6420352458953857, 0.6425746083259583, 0.6417655348777771, 0.6433836817741394, 0.6463502049446106, 0.6467996835708618, 0.6471592783927917, 0.651294469833374, 0.6485975980758667, 0.6551600098609924, 0.6540812849998474, 0.6563286781311035, 0.6582164764404297, 0.659385085105896, 0.6543509364128113, 0.6559690833091736, 0.6567781567573547, 0.6641495823860168, 0.6656777858734131, 0.6621719002723694, 0.663520336151123, 0.6642394661903381, 0.6650485396385193, 0.6681948900222778, 0.668644368648529, 0.668644368648529, 0.6690938472747803, 0.6703523993492126, 0.6696332097053528, 0.6691837310791016, 0.6745774745941162, 0.6717907190322876, 0.6720604300498962, 0.6725997924804688, 0.6731391549110413, 0.6779935359954834, 0.6725099086761475], 'loss': [1.1161901950836182, 0.920320987701416, 0.8827170133590698, 0.8644585013389587, 0.8518367409706116, 0.844192624092102, 0.8344430923461914, 0.8282418251037598, 0.8238067626953125, 0.816786527633667, 0.8069734573364258, 0.8075147271156311, 0.8021562099456787, 0.799201250076294, 0.791550874710083, 0.7891088724136353, 0.7863077521324158, 0.7870498299598694, 0.7837411761283875, 0.7797377705574036, 0.7748212218284607, 0.7751947641372681, 0.773729681968689, 0.7669223546981812, 0.7655355334281921, 0.7604594826698303, 0.7586215734481812, 0.7580761313438416, 0.7578762173652649, 0.7555786967277527, 0.7512219548225403, 0.7478131651878357, 0.7475247979164124, 0.7443301677703857, 0.744062066078186, 0.7429898977279663, 0.7372419238090515, 0.7341068983078003, 0.7344793081283569, 0.7330920696258545, 0.7331926822662354, 0.7312660813331604, 0.7263666987419128, 0.7264698147773743, 0.7295675277709961, 0.7251293063163757, 0.7218067049980164, 0.7185048460960388, 0.7173029184341431, 0.7219645380973816], 'val_accuracy': [0.5914239287376404, 0.6086839437484741, 0.6181229948997498, 0.6256741881370544, 0.621089518070221, 0.6270226240158081, 0.6267529726028442, 0.6302589178085327, 0.6262136101722717, 0.6278316974639893, 0.6310679316520691, 0.6321467161178589, 0.63295578956604, 0.6353829503059387, 0.6318770051002502, 0.6340345144271851, 0.6326860785484314, 0.6388888955116272, 0.6332254409790039, 0.6364616751670837, 0.637001097202301, 0.6307982802391052, 0.628910481929779, 0.6343042254447937, 0.6359223127365112, 0.6372707486152649, 0.6378101110458374, 0.6367313861846924, 0.6351132392883301, 0.6405069828033447, 0.6332254409790039, 0.6383495330810547, 0.6361920237541199, 0.6440129280090332, 0.6297194957733154, 0.6351132392883301, 0.641046404838562, 0.6361920237541199, 0.6345738768577576, 0.6399676203727722, 0.6378101110458374, 0.638079822063446, 0.6356526613235474, 0.6361920237541199, 0.6364616751670837, 0.6345738768577576, 0.6394282579421997, 0.6364616751670837, 0.6429342031478882, 0.6378101110458374], 'val_loss': [0.9005455374717712, 0.8611569404602051, 0.8488913774490356, 0.8414503931999207, 0.8368908166885376, 0.8358916640281677, 0.8297786712646484, 0.8256794214248657, 0.8234308362007141, 0.8219067454338074, 0.8203629851341248, 0.8190526366233826, 0.8196324706077576, 0.8161329030990601, 0.8171451091766357, 0.8165232539176941, 0.8145876526832581, 0.8146867156028748, 0.8136526346206665, 0.8119924068450928, 0.8119757175445557, 0.8147728443145752, 0.8144030570983887, 0.8112322092056274, 0.809716522693634, 0.8111234307289124, 0.8112705945968628, 0.8118399977684021, 0.8103587627410889, 0.8107753396034241, 0.8144598007202148, 0.8134276270866394, 0.8133931159973145, 0.8121033310890198, 0.815157949924469, 0.817768931388855, 0.8147873878479004, 0.8120157718658447, 0.814582109451294, 0.8198269009590149, 0.8150854706764221, 0.8132537007331848, 0.8208011388778687, 0.8195295333862305, 0.825824499130249, 0.8212913870811462, 0.8188456892967224, 0.8230595588684082, 0.8209255933761597, 0.8188105225563049]}\n",
            "Trial: Neurons = 100, Dropout Rate = 0.5, Batch Size = 128\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.3555 - loss: 1.3903 - val_accuracy: 0.5796 - val_loss: 0.9499\n",
            "Epoch 2/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5448 - loss: 0.9892 - val_accuracy: 0.6054 - val_loss: 0.8804\n",
            "Epoch 3/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5680 - loss: 0.9224 - val_accuracy: 0.6138 - val_loss: 0.8602\n",
            "Epoch 4/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5827 - loss: 0.9030 - val_accuracy: 0.6152 - val_loss: 0.8493\n",
            "Epoch 5/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6042 - loss: 0.8700 - val_accuracy: 0.6230 - val_loss: 0.8436\n",
            "Epoch 6/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6096 - loss: 0.8601 - val_accuracy: 0.6154 - val_loss: 0.8387\n",
            "Epoch 7/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6114 - loss: 0.8618 - val_accuracy: 0.6238 - val_loss: 0.8338\n",
            "Epoch 8/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6211 - loss: 0.8312 - val_accuracy: 0.6224 - val_loss: 0.8317\n",
            "Epoch 9/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6217 - loss: 0.8331 - val_accuracy: 0.6230 - val_loss: 0.8280\n",
            "Epoch 10/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6230 - loss: 0.8263 - val_accuracy: 0.6273 - val_loss: 0.8253\n",
            "Epoch 11/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6228 - loss: 0.8383 - val_accuracy: 0.6251 - val_loss: 0.8265\n",
            "Epoch 12/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6212 - loss: 0.8268 - val_accuracy: 0.6276 - val_loss: 0.8221\n",
            "Epoch 13/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6331 - loss: 0.8127 - val_accuracy: 0.6305 - val_loss: 0.8202\n",
            "Epoch 14/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6319 - loss: 0.8157 - val_accuracy: 0.6362 - val_loss: 0.8184\n",
            "Epoch 15/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6344 - loss: 0.7971 - val_accuracy: 0.6330 - val_loss: 0.8163\n",
            "Epoch 16/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6386 - loss: 0.7998 - val_accuracy: 0.6348 - val_loss: 0.8172\n",
            "Epoch 17/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6441 - loss: 0.7986 - val_accuracy: 0.6324 - val_loss: 0.8153\n",
            "Epoch 18/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6429 - loss: 0.7966 - val_accuracy: 0.6370 - val_loss: 0.8149\n",
            "Epoch 19/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6390 - loss: 0.7884 - val_accuracy: 0.6357 - val_loss: 0.8124\n",
            "Epoch 20/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6490 - loss: 0.7939 - val_accuracy: 0.6351 - val_loss: 0.8139\n",
            "Epoch 21/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6524 - loss: 0.7955 - val_accuracy: 0.6330 - val_loss: 0.8160\n",
            "Epoch 22/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6489 - loss: 0.7919 - val_accuracy: 0.6392 - val_loss: 0.8112\n",
            "Epoch 23/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6473 - loss: 0.7844 - val_accuracy: 0.6348 - val_loss: 0.8116\n",
            "Epoch 24/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6510 - loss: 0.7821 - val_accuracy: 0.6402 - val_loss: 0.8108\n",
            "Epoch 25/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6568 - loss: 0.7719 - val_accuracy: 0.6367 - val_loss: 0.8113\n",
            "Epoch 26/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6560 - loss: 0.7655 - val_accuracy: 0.6324 - val_loss: 0.8118\n",
            "Epoch 27/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6544 - loss: 0.7817 - val_accuracy: 0.6375 - val_loss: 0.8105\n",
            "Epoch 28/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6609 - loss: 0.7687 - val_accuracy: 0.6386 - val_loss: 0.8105\n",
            "Epoch 29/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6560 - loss: 0.7569 - val_accuracy: 0.6402 - val_loss: 0.8066\n",
            "Epoch 30/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6510 - loss: 0.7621 - val_accuracy: 0.6357 - val_loss: 0.8130\n",
            "Epoch 31/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6478 - loss: 0.7733 - val_accuracy: 0.6378 - val_loss: 0.8097\n",
            "Epoch 32/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6588 - loss: 0.7568 - val_accuracy: 0.6311 - val_loss: 0.8112\n",
            "Epoch 33/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6533 - loss: 0.7684 - val_accuracy: 0.6405 - val_loss: 0.8088\n",
            "Epoch 34/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6626 - loss: 0.7552 - val_accuracy: 0.6419 - val_loss: 0.8107\n",
            "Epoch 35/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6606 - loss: 0.7586 - val_accuracy: 0.6405 - val_loss: 0.8095\n",
            "Epoch 36/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6609 - loss: 0.7532 - val_accuracy: 0.6362 - val_loss: 0.8067\n",
            "Epoch 37/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6603 - loss: 0.7569 - val_accuracy: 0.6394 - val_loss: 0.8090\n",
            "Epoch 38/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6588 - loss: 0.7598 - val_accuracy: 0.6383 - val_loss: 0.8120\n",
            "Epoch 39/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6703 - loss: 0.7406 - val_accuracy: 0.6383 - val_loss: 0.8132\n",
            "Epoch 40/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6674 - loss: 0.7528 - val_accuracy: 0.6362 - val_loss: 0.8072\n",
            "Epoch 41/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6576 - loss: 0.7474 - val_accuracy: 0.6378 - val_loss: 0.8073\n",
            "Epoch 42/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6769 - loss: 0.7334 - val_accuracy: 0.6346 - val_loss: 0.8083\n",
            "Epoch 43/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6667 - loss: 0.7417 - val_accuracy: 0.6408 - val_loss: 0.8109\n",
            "Epoch 44/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6612 - loss: 0.7487 - val_accuracy: 0.6421 - val_loss: 0.8117\n",
            "Epoch 45/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6659 - loss: 0.7362 - val_accuracy: 0.6424 - val_loss: 0.8059\n",
            "Epoch 46/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6712 - loss: 0.7433 - val_accuracy: 0.6354 - val_loss: 0.8083\n",
            "Epoch 47/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6724 - loss: 0.7401 - val_accuracy: 0.6392 - val_loss: 0.8108\n",
            "Epoch 48/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6713 - loss: 0.7291 - val_accuracy: 0.6386 - val_loss: 0.8107\n",
            "Epoch 49/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6747 - loss: 0.7356 - val_accuracy: 0.6370 - val_loss: 0.8109\n",
            "Epoch 50/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6778 - loss: 0.7203 - val_accuracy: 0.6400 - val_loss: 0.8081\n",
            "History for model: {'accuracy': [0.43743258714675903, 0.5497123599052429, 0.5764113664627075, 0.5891765356063843, 0.6021215319633484, 0.6072456240653992, 0.6131787300109863, 0.6156957745552063, 0.618572473526001, 0.6230672597885132, 0.6238763332366943, 0.6297194957733154, 0.631787121295929, 0.6305285692214966, 0.6357425451278687, 0.6400575041770935, 0.6361021399497986, 0.6413160562515259, 0.6426644921302795, 0.6442826390266418, 0.6484178304672241, 0.6459906697273254, 0.6477885842323303, 0.6485975980758667, 0.649137020111084, 0.6508449912071228, 0.6499460339546204, 0.6556094884872437, 0.6522833704948425, 0.6507551074028015, 0.6541711688041687, 0.6537216901779175, 0.6580366492271423, 0.662351667881012, 0.6615425944328308, 0.6603739857673645, 0.6618123054504395, 0.6621719002723694, 0.6647788286209106, 0.6624415516853333, 0.6638798713684082, 0.6672060489654541, 0.6637001037597656, 0.6637001037597656, 0.6654081344604492, 0.6687342524528503, 0.6699029207229614, 0.6717008352279663, 0.6720604300498962, 0.6713412404060364], 'loss': [1.2269893884658813, 0.9745796322822571, 0.9174396395683289, 0.8933369517326355, 0.8672588467597961, 0.8605587482452393, 0.8528836369514465, 0.842926561832428, 0.8387088179588318, 0.8304018974304199, 0.8256312012672424, 0.8182908892631531, 0.8160334229469299, 0.8120945692062378, 0.8084822297096252, 0.8033291697502136, 0.803835928440094, 0.7994272112846375, 0.7940639853477478, 0.7926134467124939, 0.7904284000396729, 0.7875003218650818, 0.7842718362808228, 0.781402587890625, 0.7802152037620544, 0.7770156860351562, 0.7790915966033936, 0.7717177271842957, 0.7677270174026489, 0.7686540484428406, 0.7706449627876282, 0.7679524421691895, 0.7617810964584351, 0.7587246894836426, 0.7595564126968384, 0.7594241499900818, 0.7565711736679077, 0.7558282017707825, 0.7531054615974426, 0.751076877117157, 0.7464154958724976, 0.7428239583969116, 0.7475569844245911, 0.7410100102424622, 0.7414329648017883, 0.7446987628936768, 0.7408633232116699, 0.7335034608840942, 0.7358357906341553, 0.7323110699653625], 'val_accuracy': [0.5795577168464661, 0.6054477095603943, 0.6138079762458801, 0.6151564121246338, 0.6229773759841919, 0.6154261231422424, 0.6237863898277283, 0.6224379539489746, 0.6229773759841919, 0.6272923350334167, 0.6251348257064819, 0.6275620460510254, 0.6305285692214966, 0.6361920237541199, 0.63295578956604, 0.6348435878753662, 0.6324163675308228, 0.637001097202301, 0.6356526613235474, 0.6351132392883301, 0.63295578956604, 0.6391585469245911, 0.6348435878753662, 0.6402373313903809, 0.6367313861846924, 0.6324163675308228, 0.6375404596328735, 0.6386191844940186, 0.6402373313903809, 0.6356526613235474, 0.6378101110458374, 0.6310679316520691, 0.6405069828033447, 0.6418554186820984, 0.6405069828033447, 0.6361920237541199, 0.6394282579421997, 0.6383495330810547, 0.6383495330810547, 0.6361920237541199, 0.6378101110458374, 0.6345738768577576, 0.6407766938209534, 0.642125129699707, 0.6423948407173157, 0.6353829503059387, 0.6391585469245911, 0.6386191844940186, 0.637001097202301, 0.6399676203727722], 'val_loss': [0.9499337077140808, 0.880393922328949, 0.8601534366607666, 0.8493130803108215, 0.8436225056648254, 0.8386858701705933, 0.8337863087654114, 0.8317356705665588, 0.8279779553413391, 0.8252644538879395, 0.8265367746353149, 0.8220783472061157, 0.8201661705970764, 0.818406343460083, 0.8163403272628784, 0.8172233700752258, 0.8152506351470947, 0.8149400353431702, 0.8123805522918701, 0.8138607740402222, 0.8159630298614502, 0.8111711144447327, 0.8115947842597961, 0.8107759952545166, 0.8112969398498535, 0.8117753267288208, 0.8105306029319763, 0.810464084148407, 0.8066169023513794, 0.8129891753196716, 0.8096577525138855, 0.8111698627471924, 0.8088192343711853, 0.8107143640518188, 0.8094668388366699, 0.8067267537117004, 0.8089525699615479, 0.8120071887969971, 0.8132289052009583, 0.807177722454071, 0.807274341583252, 0.8083325028419495, 0.8109279274940491, 0.8117467164993286, 0.8059161305427551, 0.8082736134529114, 0.8107515573501587, 0.8106763362884521, 0.8108617663383484, 0.8080608248710632]}\n",
            "Trial: Neurons = 100, Dropout Rate = 0.5, Batch Size = 256\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.3395 - loss: 1.4284 - val_accuracy: 0.5467 - val_loss: 1.0566\n",
            "Epoch 2/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5135 - loss: 1.0730 - val_accuracy: 0.5898 - val_loss: 0.9301\n",
            "Epoch 3/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5556 - loss: 0.9837 - val_accuracy: 0.5995 - val_loss: 0.8909\n",
            "Epoch 4/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5717 - loss: 0.9321 - val_accuracy: 0.6046 - val_loss: 0.8709\n",
            "Epoch 5/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5843 - loss: 0.9029 - val_accuracy: 0.6092 - val_loss: 0.8614\n",
            "Epoch 6/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5882 - loss: 0.8927 - val_accuracy: 0.6084 - val_loss: 0.8538\n",
            "Epoch 7/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6017 - loss: 0.8721 - val_accuracy: 0.6087 - val_loss: 0.8480\n",
            "Epoch 8/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6013 - loss: 0.8705 - val_accuracy: 0.6141 - val_loss: 0.8436\n",
            "Epoch 9/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6010 - loss: 0.8704 - val_accuracy: 0.6173 - val_loss: 0.8396\n",
            "Epoch 10/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6185 - loss: 0.8424 - val_accuracy: 0.6206 - val_loss: 0.8362\n",
            "Epoch 11/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6182 - loss: 0.8428 - val_accuracy: 0.6187 - val_loss: 0.8353\n",
            "Epoch 12/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6166 - loss: 0.8356 - val_accuracy: 0.6214 - val_loss: 0.8316\n",
            "Epoch 13/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6165 - loss: 0.8466 - val_accuracy: 0.6206 - val_loss: 0.8299\n",
            "Epoch 14/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6224 - loss: 0.8318 - val_accuracy: 0.6276 - val_loss: 0.8288\n",
            "Epoch 15/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6296 - loss: 0.8243 - val_accuracy: 0.6286 - val_loss: 0.8266\n",
            "Epoch 16/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6232 - loss: 0.8281 - val_accuracy: 0.6311 - val_loss: 0.8250\n",
            "Epoch 17/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6302 - loss: 0.8144 - val_accuracy: 0.6292 - val_loss: 0.8244\n",
            "Epoch 18/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6358 - loss: 0.8088 - val_accuracy: 0.6292 - val_loss: 0.8235\n",
            "Epoch 19/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6363 - loss: 0.8094 - val_accuracy: 0.6286 - val_loss: 0.8231\n",
            "Epoch 20/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6199 - loss: 0.8231 - val_accuracy: 0.6343 - val_loss: 0.8203\n",
            "Epoch 21/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6396 - loss: 0.7988 - val_accuracy: 0.6273 - val_loss: 0.8206\n",
            "Epoch 22/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6351 - loss: 0.8211 - val_accuracy: 0.6300 - val_loss: 0.8187\n",
            "Epoch 23/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6387 - loss: 0.7948 - val_accuracy: 0.6365 - val_loss: 0.8180\n",
            "Epoch 24/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6391 - loss: 0.8047 - val_accuracy: 0.6367 - val_loss: 0.8181\n",
            "Epoch 25/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6355 - loss: 0.7979 - val_accuracy: 0.6335 - val_loss: 0.8169\n",
            "Epoch 26/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6479 - loss: 0.8007 - val_accuracy: 0.6338 - val_loss: 0.8165\n",
            "Epoch 27/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6422 - loss: 0.7958 - val_accuracy: 0.6373 - val_loss: 0.8166\n",
            "Epoch 28/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6414 - loss: 0.7878 - val_accuracy: 0.6343 - val_loss: 0.8151\n",
            "Epoch 29/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6417 - loss: 0.7831 - val_accuracy: 0.6386 - val_loss: 0.8147\n",
            "Epoch 30/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6432 - loss: 0.7844 - val_accuracy: 0.6370 - val_loss: 0.8131\n",
            "Epoch 31/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6576 - loss: 0.7786 - val_accuracy: 0.6357 - val_loss: 0.8147\n",
            "Epoch 32/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6455 - loss: 0.7982 - val_accuracy: 0.6346 - val_loss: 0.8133\n",
            "Epoch 33/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6570 - loss: 0.7843 - val_accuracy: 0.6335 - val_loss: 0.8135\n",
            "Epoch 34/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6496 - loss: 0.7852 - val_accuracy: 0.6365 - val_loss: 0.8112\n",
            "Epoch 35/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6537 - loss: 0.7771 - val_accuracy: 0.6354 - val_loss: 0.8122\n",
            "Epoch 36/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6544 - loss: 0.7814 - val_accuracy: 0.6394 - val_loss: 0.8112\n",
            "Epoch 37/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6435 - loss: 0.7777 - val_accuracy: 0.6413 - val_loss: 0.8101\n",
            "Epoch 38/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6542 - loss: 0.7680 - val_accuracy: 0.6348 - val_loss: 0.8110\n",
            "Epoch 39/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6548 - loss: 0.7740 - val_accuracy: 0.6373 - val_loss: 0.8109\n",
            "Epoch 40/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6493 - loss: 0.7766 - val_accuracy: 0.6346 - val_loss: 0.8115\n",
            "Epoch 41/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6609 - loss: 0.7590 - val_accuracy: 0.6370 - val_loss: 0.8108\n",
            "Epoch 42/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6497 - loss: 0.7750 - val_accuracy: 0.6386 - val_loss: 0.8091\n",
            "Epoch 43/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6609 - loss: 0.7559 - val_accuracy: 0.6378 - val_loss: 0.8103\n",
            "Epoch 44/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6578 - loss: 0.7684 - val_accuracy: 0.6335 - val_loss: 0.8096\n",
            "Epoch 45/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6607 - loss: 0.7639 - val_accuracy: 0.6375 - val_loss: 0.8097\n",
            "Epoch 46/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6610 - loss: 0.7591 - val_accuracy: 0.6330 - val_loss: 0.8092\n",
            "Epoch 47/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6561 - loss: 0.7591 - val_accuracy: 0.6373 - val_loss: 0.8065\n",
            "Epoch 48/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6618 - loss: 0.7555 - val_accuracy: 0.6370 - val_loss: 0.8092\n",
            "Epoch 49/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6601 - loss: 0.7618 - val_accuracy: 0.6373 - val_loss: 0.8080\n",
            "Epoch 50/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6709 - loss: 0.7481 - val_accuracy: 0.6394 - val_loss: 0.8086\n",
            "History for model: {'accuracy': [0.4047105312347412, 0.5284969210624695, 0.5649946331977844, 0.5774900913238525, 0.5809061527252197, 0.5923228859901428, 0.5995145440101624, 0.6049982309341431, 0.6063466668128967, 0.611920177936554, 0.6158756017684937, 0.6150665283203125, 0.6180331110954285, 0.6239662170410156, 0.6229773759841919, 0.6191118359565735, 0.6290003657341003, 0.631787121295929, 0.6305285692214966, 0.6322365999221802, 0.6337648034095764, 0.6349334716796875, 0.6371808648109436, 0.6405069828033447, 0.6372707486152649, 0.6449118852615356, 0.6443725228309631, 0.6390686631202698, 0.6399676203727722, 0.6431139707565308, 0.6457209587097168, 0.6482380628585815, 0.6496763825416565, 0.6492269039154053, 0.6467097997665405, 0.6484178304672241, 0.6499460339546204, 0.6571376919746399, 0.6522833704948425, 0.6480582356452942, 0.652822732925415, 0.653182327747345, 0.6563286781311035, 0.6550701260566711, 0.6622617840766907, 0.6574972867965698, 0.6574074029922485, 0.6580366492271423, 0.6620819568634033, 0.6656777858734131], 'loss': [1.2974954843521118, 1.042230486869812, 0.9624608755111694, 0.9240227341651917, 0.9059610962867737, 0.8914106488227844, 0.8771902322769165, 0.8676242232322693, 0.86026930809021, 0.8546183109283447, 0.8474347591400146, 0.8422975540161133, 0.8385531902313232, 0.8352807760238647, 0.8311686515808105, 0.8279720544815063, 0.820997953414917, 0.8198711276054382, 0.8155988454818726, 0.809568464756012, 0.8105776906013489, 0.8114100098609924, 0.8020205497741699, 0.8021008968353271, 0.7995901107788086, 0.8002879023551941, 0.7961022853851318, 0.7924818992614746, 0.7905336022377014, 0.7897089123725891, 0.7887076735496521, 0.7863714098930359, 0.782768189907074, 0.7837203145027161, 0.7849044799804688, 0.7780733704566956, 0.776810348033905, 0.7740269303321838, 0.7737826108932495, 0.7762162089347839, 0.7695130705833435, 0.7731371521949768, 0.764750599861145, 0.7690708041191101, 0.7623677849769592, 0.7644566297531128, 0.7641696333885193, 0.7578966617584229, 0.7630813121795654, 0.7553302049636841], 'val_accuracy': [0.5466558933258057, 0.5898058414459229, 0.5995145440101624, 0.6046386361122131, 0.6092233061790466, 0.6084142327308655, 0.6086839437484741, 0.6140776872634888, 0.6173139214515686, 0.6205501556396484, 0.6186623573303223, 0.6213592290878296, 0.6205501556396484, 0.6275620460510254, 0.6286407709121704, 0.6310679316520691, 0.6291801333427429, 0.6291801333427429, 0.6286407709121704, 0.6343042254447937, 0.6272923350334167, 0.6299892067909241, 0.6364616751670837, 0.6367313861846924, 0.6334951519966125, 0.6337648034095764, 0.6372707486152649, 0.6343042254447937, 0.6386191844940186, 0.637001097202301, 0.6356526613235474, 0.6345738768577576, 0.6334951519966125, 0.6364616751670837, 0.6353829503059387, 0.6394282579421997, 0.6413160562515259, 0.6348435878753662, 0.6372707486152649, 0.6345738768577576, 0.637001097202301, 0.6386191844940186, 0.6378101110458374, 0.6334951519966125, 0.6375404596328735, 0.63295578956604, 0.6372707486152649, 0.637001097202301, 0.6372707486152649, 0.6394282579421997], 'val_loss': [1.0566025972366333, 0.9301002025604248, 0.8908874988555908, 0.8709360361099243, 0.8613780736923218, 0.8537737131118774, 0.8480321168899536, 0.8436239361763, 0.8395872116088867, 0.8362207412719727, 0.8352717757225037, 0.8315916061401367, 0.8299002051353455, 0.8288308382034302, 0.82662034034729, 0.8250017166137695, 0.8244135975837708, 0.8235052824020386, 0.8230962753295898, 0.8203001618385315, 0.8205984234809875, 0.8187129497528076, 0.8179665803909302, 0.8180602192878723, 0.816949188709259, 0.8165186643600464, 0.8165520429611206, 0.8150630593299866, 0.8146762847900391, 0.8130509853363037, 0.8146505951881409, 0.8132839202880859, 0.8135350942611694, 0.8112466931343079, 0.8121816515922546, 0.8111532330513, 0.8100571632385254, 0.810994029045105, 0.8108894228935242, 0.8114786744117737, 0.8108139038085938, 0.8091400861740112, 0.8103182315826416, 0.8095946311950684, 0.8097208142280579, 0.8091745972633362, 0.8064825534820557, 0.8091599941253662, 0.8079627156257629, 0.808632493019104]}\n",
            "Trial: Neurons = 150, Dropout Rate = 0, Batch Size = 64\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5031 - loss: 1.1126 - val_accuracy: 0.6057 - val_loss: 0.8754\n",
            "Epoch 2/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6123 - loss: 0.8455 - val_accuracy: 0.6087 - val_loss: 0.8579\n",
            "Epoch 3/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6190 - loss: 0.8218 - val_accuracy: 0.6162 - val_loss: 0.8442\n",
            "Epoch 4/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6254 - loss: 0.8145 - val_accuracy: 0.6135 - val_loss: 0.8411\n",
            "Epoch 5/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6290 - loss: 0.7993 - val_accuracy: 0.6254 - val_loss: 0.8343\n",
            "Epoch 6/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6443 - loss: 0.7899 - val_accuracy: 0.6211 - val_loss: 0.8344\n",
            "Epoch 7/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6389 - loss: 0.7894 - val_accuracy: 0.6284 - val_loss: 0.8259\n",
            "Epoch 8/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6354 - loss: 0.7913 - val_accuracy: 0.6313 - val_loss: 0.8273\n",
            "Epoch 9/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6474 - loss: 0.7779 - val_accuracy: 0.6222 - val_loss: 0.8300\n",
            "Epoch 10/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6580 - loss: 0.7593 - val_accuracy: 0.6316 - val_loss: 0.8250\n",
            "Epoch 11/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6657 - loss: 0.7449 - val_accuracy: 0.6243 - val_loss: 0.8290\n",
            "Epoch 12/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6571 - loss: 0.7570 - val_accuracy: 0.6211 - val_loss: 0.8250\n",
            "Epoch 13/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6656 - loss: 0.7453 - val_accuracy: 0.6230 - val_loss: 0.8212\n",
            "Epoch 14/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6618 - loss: 0.7419 - val_accuracy: 0.6292 - val_loss: 0.8236\n",
            "Epoch 15/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6614 - loss: 0.7529 - val_accuracy: 0.6154 - val_loss: 0.8247\n",
            "Epoch 16/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6625 - loss: 0.7440 - val_accuracy: 0.6303 - val_loss: 0.8287\n",
            "Epoch 17/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6721 - loss: 0.7321 - val_accuracy: 0.6297 - val_loss: 0.8266\n",
            "Epoch 18/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6636 - loss: 0.7425 - val_accuracy: 0.6154 - val_loss: 0.8355\n",
            "Epoch 19/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6825 - loss: 0.7203 - val_accuracy: 0.6281 - val_loss: 0.8240\n",
            "Epoch 20/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6814 - loss: 0.7217 - val_accuracy: 0.6370 - val_loss: 0.8249\n",
            "Epoch 21/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6826 - loss: 0.7125 - val_accuracy: 0.6241 - val_loss: 0.8265\n",
            "Epoch 22/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6809 - loss: 0.7234 - val_accuracy: 0.6246 - val_loss: 0.8255\n",
            "Epoch 23/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6860 - loss: 0.7016 - val_accuracy: 0.6184 - val_loss: 0.8267\n",
            "Epoch 24/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6864 - loss: 0.7071 - val_accuracy: 0.6222 - val_loss: 0.8411\n",
            "Epoch 25/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6847 - loss: 0.7091 - val_accuracy: 0.6257 - val_loss: 0.8286\n",
            "Epoch 26/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6842 - loss: 0.7093 - val_accuracy: 0.6294 - val_loss: 0.8253\n",
            "Epoch 27/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6880 - loss: 0.7021 - val_accuracy: 0.6251 - val_loss: 0.8277\n",
            "Epoch 28/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6940 - loss: 0.6936 - val_accuracy: 0.6294 - val_loss: 0.8362\n",
            "Epoch 29/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6941 - loss: 0.6872 - val_accuracy: 0.6340 - val_loss: 0.8296\n",
            "Epoch 30/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6974 - loss: 0.6904 - val_accuracy: 0.6276 - val_loss: 0.8347\n",
            "Epoch 31/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6944 - loss: 0.6862 - val_accuracy: 0.6273 - val_loss: 0.8352\n",
            "Epoch 32/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6972 - loss: 0.6800 - val_accuracy: 0.6208 - val_loss: 0.8374\n",
            "Epoch 33/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6945 - loss: 0.6798 - val_accuracy: 0.6208 - val_loss: 0.8478\n",
            "Epoch 34/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6973 - loss: 0.6782 - val_accuracy: 0.6300 - val_loss: 0.8360\n",
            "Epoch 35/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6931 - loss: 0.6818 - val_accuracy: 0.6268 - val_loss: 0.8394\n",
            "Epoch 36/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6981 - loss: 0.6711 - val_accuracy: 0.6232 - val_loss: 0.8479\n",
            "Epoch 37/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7044 - loss: 0.6625 - val_accuracy: 0.6246 - val_loss: 0.8472\n",
            "Epoch 38/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7029 - loss: 0.6629 - val_accuracy: 0.6313 - val_loss: 0.8479\n",
            "Epoch 39/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7047 - loss: 0.6569 - val_accuracy: 0.6273 - val_loss: 0.8616\n",
            "Epoch 40/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6962 - loss: 0.6688 - val_accuracy: 0.6243 - val_loss: 0.8645\n",
            "Epoch 41/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7045 - loss: 0.6585 - val_accuracy: 0.6170 - val_loss: 0.8699\n",
            "Epoch 42/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7135 - loss: 0.6543 - val_accuracy: 0.6294 - val_loss: 0.8547\n",
            "Epoch 43/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7078 - loss: 0.6551 - val_accuracy: 0.6297 - val_loss: 0.8541\n",
            "Epoch 44/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7070 - loss: 0.6508 - val_accuracy: 0.6170 - val_loss: 0.8641\n",
            "Epoch 45/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7096 - loss: 0.6612 - val_accuracy: 0.6381 - val_loss: 0.8594\n",
            "Epoch 46/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7098 - loss: 0.6455 - val_accuracy: 0.6313 - val_loss: 0.8595\n",
            "Epoch 47/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7182 - loss: 0.6332 - val_accuracy: 0.6316 - val_loss: 0.8636\n",
            "Epoch 48/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7174 - loss: 0.6352 - val_accuracy: 0.6259 - val_loss: 0.8830\n",
            "Epoch 49/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7074 - loss: 0.6436 - val_accuracy: 0.6284 - val_loss: 0.8795\n",
            "Epoch 50/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7220 - loss: 0.6310 - val_accuracy: 0.6297 - val_loss: 0.8728\n",
            "History for model: {'accuracy': [0.5615785717964172, 0.6080546379089355, 0.6176735162734985, 0.6267529726028442, 0.6307982802391052, 0.6368212699890137, 0.6415857672691345, 0.6429342031478882, 0.6495864987373352, 0.6501258611679077, 0.6536318063735962, 0.6545307636260986, 0.6608234643936157, 0.6613628268241882, 0.6666666865348816, 0.6669363379478455, 0.670082688331604, 0.6717008352279663, 0.6720604300498962, 0.6761956214904785, 0.6787126660346985, 0.6816792488098145, 0.6791621446609497, 0.683746874332428, 0.6855447888374329, 0.6858144402503967, 0.6893203854560852, 0.6893203854560852, 0.6893203854560852, 0.6948938965797424, 0.694084882736206, 0.6919273734092712, 0.69399493932724, 0.6949838399887085, 0.6983998417854309, 0.6975008845329285, 0.6957029700279236, 0.7050521373748779, 0.6990291476249695, 0.7017260193824768, 0.7010967135429382, 0.7080187201499939, 0.7064904570579529, 0.7086479663848877, 0.7090974450111389, 0.7081984877586365, 0.7119740843772888, 0.71278315782547, 0.7114347219467163, 0.715390145778656], 'loss': [0.9764688014984131, 0.844551682472229, 0.8246050477027893, 0.8113504648208618, 0.8006354570388794, 0.7928553223609924, 0.7861418724060059, 0.7787770628929138, 0.7757888436317444, 0.7684754729270935, 0.7607631683349609, 0.759860634803772, 0.7548700571060181, 0.7494151592254639, 0.7442935705184937, 0.7407715320587158, 0.7374746203422546, 0.73258376121521, 0.7291210889816284, 0.725306510925293, 0.7209609150886536, 0.7182772755622864, 0.7167018055915833, 0.7110676765441895, 0.7067024111747742, 0.7060612440109253, 0.7005928754806519, 0.6977757811546326, 0.6968569159507751, 0.6908745169639587, 0.6879575848579407, 0.6879127025604248, 0.6841288208961487, 0.6811311841011047, 0.67649906873703, 0.6758492588996887, 0.6744665503501892, 0.6680206060409546, 0.6681618690490723, 0.666023850440979, 0.6632806658744812, 0.6609147191047668, 0.6563932299613953, 0.6540611386299133, 0.6520340442657471, 0.650722086429596, 0.6456198692321777, 0.6464186906814575, 0.6432310342788696, 0.639743447303772], 'val_accuracy': [0.6057173609733582, 0.6086839437484741, 0.6162351965904236, 0.6135383248329163, 0.6254045367240906, 0.621089518070221, 0.6283710598945618, 0.6313376426696777, 0.6221683025360107, 0.6316073536872864, 0.6243258118629456, 0.621089518070221, 0.6229773759841919, 0.6291801333427429, 0.6154261231422424, 0.6302589178085327, 0.6297194957733154, 0.6154261231422424, 0.6281014084815979, 0.637001097202301, 0.6240561008453369, 0.6245954632759094, 0.6183926463127136, 0.6221683025360107, 0.6256741881370544, 0.6294498443603516, 0.6251348257064819, 0.6294498443603516, 0.6340345144271851, 0.6275620460510254, 0.6272923350334167, 0.6208198666572571, 0.6208198666572571, 0.6299892067909241, 0.6267529726028442, 0.6232470273971558, 0.6245954632759094, 0.6313376426696777, 0.6272923350334167, 0.6243258118629456, 0.61704421043396, 0.6294498443603516, 0.6297194957733154, 0.61704421043396, 0.638079822063446, 0.6313376426696777, 0.6316073536872864, 0.6259438991546631, 0.6283710598945618, 0.6297194957733154], 'val_loss': [0.8753942847251892, 0.8579334616661072, 0.8441705107688904, 0.8411416411399841, 0.8342798948287964, 0.83436518907547, 0.8259493112564087, 0.8272927403450012, 0.8299812078475952, 0.824951708316803, 0.8290261030197144, 0.8249631524085999, 0.8211954236030579, 0.8236120343208313, 0.824722945690155, 0.8287427425384521, 0.8265870213508606, 0.8355253338813782, 0.8239829540252686, 0.8249092102050781, 0.8265072703361511, 0.8254998326301575, 0.8267087340354919, 0.8411173224449158, 0.8286490440368652, 0.8253216743469238, 0.8277277946472168, 0.8361799716949463, 0.8295629024505615, 0.8347487449645996, 0.835241973400116, 0.837392270565033, 0.8477588891983032, 0.8359687328338623, 0.8393622636795044, 0.847943127155304, 0.8471826910972595, 0.8478689789772034, 0.861579179763794, 0.86447674036026, 0.8698527812957764, 0.8546569347381592, 0.8541221022605896, 0.864118754863739, 0.8594127297401428, 0.8595208525657654, 0.8636088967323303, 0.8829888701438904, 0.8794817924499512, 0.872814953327179]}\n",
            "Trial: Neurons = 150, Dropout Rate = 0, Batch Size = 128\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4484 - loss: 1.2095 - val_accuracy: 0.5860 - val_loss: 0.9034\n",
            "Epoch 2/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5991 - loss: 0.8784 - val_accuracy: 0.5895 - val_loss: 0.8714\n",
            "Epoch 3/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6130 - loss: 0.8397 - val_accuracy: 0.6152 - val_loss: 0.8481\n",
            "Epoch 4/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6269 - loss: 0.8212 - val_accuracy: 0.6095 - val_loss: 0.8413\n",
            "Epoch 5/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6260 - loss: 0.8225 - val_accuracy: 0.6249 - val_loss: 0.8332\n",
            "Epoch 6/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6336 - loss: 0.7940 - val_accuracy: 0.6268 - val_loss: 0.8308\n",
            "Epoch 7/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6482 - loss: 0.7889 - val_accuracy: 0.6241 - val_loss: 0.8310\n",
            "Epoch 8/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6376 - loss: 0.7967 - val_accuracy: 0.6332 - val_loss: 0.8272\n",
            "Epoch 9/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6420 - loss: 0.7814 - val_accuracy: 0.6284 - val_loss: 0.8287\n",
            "Epoch 10/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6475 - loss: 0.7844 - val_accuracy: 0.6303 - val_loss: 0.8269\n",
            "Epoch 11/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6521 - loss: 0.7688 - val_accuracy: 0.6354 - val_loss: 0.8209\n",
            "Epoch 12/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6556 - loss: 0.7639 - val_accuracy: 0.6359 - val_loss: 0.8190\n",
            "Epoch 13/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6541 - loss: 0.7626 - val_accuracy: 0.6270 - val_loss: 0.8187\n",
            "Epoch 14/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6558 - loss: 0.7661 - val_accuracy: 0.6276 - val_loss: 0.8237\n",
            "Epoch 15/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6595 - loss: 0.7468 - val_accuracy: 0.6351 - val_loss: 0.8178\n",
            "Epoch 16/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6662 - loss: 0.7516 - val_accuracy: 0.6273 - val_loss: 0.8183\n",
            "Epoch 17/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6676 - loss: 0.7474 - val_accuracy: 0.6343 - val_loss: 0.8180\n",
            "Epoch 18/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6738 - loss: 0.7403 - val_accuracy: 0.6227 - val_loss: 0.8270\n",
            "Epoch 19/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6619 - loss: 0.7437 - val_accuracy: 0.6338 - val_loss: 0.8219\n",
            "Epoch 20/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6658 - loss: 0.7359 - val_accuracy: 0.6235 - val_loss: 0.8175\n",
            "Epoch 21/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6783 - loss: 0.7293 - val_accuracy: 0.6241 - val_loss: 0.8219\n",
            "Epoch 22/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6746 - loss: 0.7345 - val_accuracy: 0.6327 - val_loss: 0.8171\n",
            "Epoch 23/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6722 - loss: 0.7230 - val_accuracy: 0.6367 - val_loss: 0.8184\n",
            "Epoch 24/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6785 - loss: 0.7215 - val_accuracy: 0.6276 - val_loss: 0.8210\n",
            "Epoch 25/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6764 - loss: 0.7225 - val_accuracy: 0.6305 - val_loss: 0.8281\n",
            "Epoch 26/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6822 - loss: 0.7140 - val_accuracy: 0.6324 - val_loss: 0.8189\n",
            "Epoch 27/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6894 - loss: 0.7109 - val_accuracy: 0.6359 - val_loss: 0.8231\n",
            "Epoch 28/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6857 - loss: 0.7121 - val_accuracy: 0.6195 - val_loss: 0.8321\n",
            "Epoch 29/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6848 - loss: 0.7104 - val_accuracy: 0.6327 - val_loss: 0.8233\n",
            "Epoch 30/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6829 - loss: 0.7079 - val_accuracy: 0.6332 - val_loss: 0.8234\n",
            "Epoch 31/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6831 - loss: 0.6939 - val_accuracy: 0.6346 - val_loss: 0.8287\n",
            "Epoch 32/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6835 - loss: 0.6976 - val_accuracy: 0.6257 - val_loss: 0.8396\n",
            "Epoch 33/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6886 - loss: 0.7036 - val_accuracy: 0.6211 - val_loss: 0.8366\n",
            "Epoch 34/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6846 - loss: 0.6998 - val_accuracy: 0.6270 - val_loss: 0.8286\n",
            "Epoch 35/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6803 - loss: 0.6942 - val_accuracy: 0.6246 - val_loss: 0.8302\n",
            "Epoch 36/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6917 - loss: 0.6869 - val_accuracy: 0.6251 - val_loss: 0.8284\n",
            "Epoch 37/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6977 - loss: 0.6789 - val_accuracy: 0.6281 - val_loss: 0.8315\n",
            "Epoch 38/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6886 - loss: 0.6963 - val_accuracy: 0.6268 - val_loss: 0.8344\n",
            "Epoch 39/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6951 - loss: 0.6834 - val_accuracy: 0.6297 - val_loss: 0.8332\n",
            "Epoch 40/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7032 - loss: 0.6784 - val_accuracy: 0.6300 - val_loss: 0.8321\n",
            "Epoch 41/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6934 - loss: 0.6903 - val_accuracy: 0.6238 - val_loss: 0.8374\n",
            "Epoch 42/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7033 - loss: 0.6724 - val_accuracy: 0.6216 - val_loss: 0.8400\n",
            "Epoch 43/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6973 - loss: 0.6764 - val_accuracy: 0.6251 - val_loss: 0.8381\n",
            "Epoch 44/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6992 - loss: 0.6778 - val_accuracy: 0.6319 - val_loss: 0.8436\n",
            "Epoch 45/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6994 - loss: 0.6728 - val_accuracy: 0.6268 - val_loss: 0.8454\n",
            "Epoch 46/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7029 - loss: 0.6595 - val_accuracy: 0.6224 - val_loss: 0.8420\n",
            "Epoch 47/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6950 - loss: 0.6755 - val_accuracy: 0.6214 - val_loss: 0.8521\n",
            "Epoch 48/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7076 - loss: 0.6610 - val_accuracy: 0.6187 - val_loss: 0.8509\n",
            "Epoch 49/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7159 - loss: 0.6563 - val_accuracy: 0.6189 - val_loss: 0.8547\n",
            "Epoch 50/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7037 - loss: 0.6658 - val_accuracy: 0.6286 - val_loss: 0.8513\n",
            "History for model: {'accuracy': [0.5324523448944092, 0.6005933284759521, 0.6160553693771362, 0.6208198666572571, 0.6263933777809143, 0.6305285692214966, 0.6393383741378784, 0.6412261724472046, 0.6448220014572144, 0.6521035432815552, 0.6523732542991638, 0.6514742970466614, 0.6512045860290527, 0.6585760712623596, 0.6578568816184998, 0.6612729430198669, 0.6669363379478455, 0.666037380695343, 0.6616324782371521, 0.667565643787384, 0.6725099086761475, 0.6725997924804688, 0.667565643787384, 0.6745774745941162, 0.67268967628479, 0.6775440573692322, 0.6790722608566284, 0.6812297701835632, 0.6816792488098145, 0.6785328984260559, 0.6785328984260559, 0.6834771633148193, 0.6872527599334717, 0.6861740350723267, 0.686263918876648, 0.6886911392211914, 0.6933656930923462, 0.6913880109786987, 0.6921970248222351, 0.6960625648498535, 0.6932758092880249, 0.6992089152336121, 0.6983099579811096, 0.6954333186149597, 0.6975008845329285, 0.69399493932724, 0.6975008845329285, 0.703254222869873, 0.7056813836097717, 0.7029845118522644], 'loss': [1.0622265338897705, 0.8676499724388123, 0.8388257622718811, 0.8231892585754395, 0.8129523992538452, 0.8038214445114136, 0.7975372672080994, 0.7915941476821899, 0.7842702269554138, 0.7792305946350098, 0.7759864330291748, 0.771380603313446, 0.765425443649292, 0.7604982256889343, 0.7573058605194092, 0.7541006207466125, 0.7494073510169983, 0.7470611929893494, 0.7435601949691772, 0.740655779838562, 0.7348706722259521, 0.731442391872406, 0.7305976748466492, 0.7275976538658142, 0.7243639826774597, 0.7212784290313721, 0.7164492607116699, 0.7153615951538086, 0.711615264415741, 0.711131751537323, 0.7068958282470703, 0.7053025364875793, 0.7011708617210388, 0.6992684602737427, 0.697773277759552, 0.694697380065918, 0.6918293237686157, 0.6906618475914001, 0.6874521970748901, 0.6881430149078369, 0.6851319074630737, 0.6805426478385925, 0.6775162816047668, 0.6780645251274109, 0.6738507747650146, 0.6746190190315247, 0.6716153025627136, 0.6674773693084717, 0.6657945513725281, 0.6658353209495544], 'val_accuracy': [0.5860301852226257, 0.5895361304283142, 0.6151564121246338, 0.6094930171966553, 0.6248651742935181, 0.6267529726028442, 0.6240561008453369, 0.6332254409790039, 0.6283710598945618, 0.6302589178085327, 0.6353829503059387, 0.6359223127365112, 0.6270226240158081, 0.6275620460510254, 0.6351132392883301, 0.6272923350334167, 0.6343042254447937, 0.6227076649665833, 0.6337648034095764, 0.6235167384147644, 0.6240561008453369, 0.6326860785484314, 0.6367313861846924, 0.6275620460510254, 0.6305285692214966, 0.6324163675308228, 0.6359223127365112, 0.6194714307785034, 0.6326860785484314, 0.6332254409790039, 0.6345738768577576, 0.6256741881370544, 0.621089518070221, 0.6270226240158081, 0.6245954632759094, 0.6251348257064819, 0.6281014084815979, 0.6267529726028442, 0.6297194957733154, 0.6299892067909241, 0.6237863898277283, 0.6216289401054382, 0.6251348257064819, 0.6318770051002502, 0.6267529726028442, 0.6224379539489746, 0.6213592290878296, 0.6186623573303223, 0.6189320683479309, 0.6286407709121704], 'val_loss': [0.9034029245376587, 0.8714386224746704, 0.8480738997459412, 0.8412871360778809, 0.8331871032714844, 0.8308400511741638, 0.8309503793716431, 0.8271756172180176, 0.828728437423706, 0.8269232511520386, 0.820857584476471, 0.8190174102783203, 0.8186814188957214, 0.8236638307571411, 0.8177863955497742, 0.8183175921440125, 0.8179550170898438, 0.8269745707511902, 0.8219326734542847, 0.8175017833709717, 0.8219140768051147, 0.8170965909957886, 0.8183673620223999, 0.8209652304649353, 0.8280865550041199, 0.8189024925231934, 0.8231405019760132, 0.832135021686554, 0.8232839703559875, 0.8234378695487976, 0.828685998916626, 0.8396109342575073, 0.8365983963012695, 0.8286265730857849, 0.8302217125892639, 0.8283751606941223, 0.8314982652664185, 0.8344039916992188, 0.8331670165061951, 0.8321343064308167, 0.8374250531196594, 0.8400406837463379, 0.8381471037864685, 0.8435742259025574, 0.845364511013031, 0.84195476770401, 0.8521193265914917, 0.8509450554847717, 0.854711651802063, 0.8513144850730896]}\n",
            "Trial: Neurons = 150, Dropout Rate = 0, Batch Size = 256\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3933 - loss: 1.2885 - val_accuracy: 0.5812 - val_loss: 0.9717\n",
            "Epoch 2/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5870 - loss: 0.9367 - val_accuracy: 0.5952 - val_loss: 0.8797\n",
            "Epoch 3/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6029 - loss: 0.8670 - val_accuracy: 0.5965 - val_loss: 0.8624\n",
            "Epoch 4/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6030 - loss: 0.8535 - val_accuracy: 0.6114 - val_loss: 0.8528\n",
            "Epoch 5/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6163 - loss: 0.8279 - val_accuracy: 0.6081 - val_loss: 0.8473\n",
            "Epoch 6/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6322 - loss: 0.8117 - val_accuracy: 0.6122 - val_loss: 0.8417\n",
            "Epoch 7/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6287 - loss: 0.8148 - val_accuracy: 0.6187 - val_loss: 0.8386\n",
            "Epoch 8/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6351 - loss: 0.8000 - val_accuracy: 0.6173 - val_loss: 0.8355\n",
            "Epoch 9/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6402 - loss: 0.7862 - val_accuracy: 0.6195 - val_loss: 0.8333\n",
            "Epoch 10/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6372 - loss: 0.7969 - val_accuracy: 0.6276 - val_loss: 0.8316\n",
            "Epoch 11/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6399 - loss: 0.7940 - val_accuracy: 0.6303 - val_loss: 0.8284\n",
            "Epoch 12/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6472 - loss: 0.7819 - val_accuracy: 0.6289 - val_loss: 0.8281\n",
            "Epoch 13/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6547 - loss: 0.7787 - val_accuracy: 0.6308 - val_loss: 0.8258\n",
            "Epoch 14/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6549 - loss: 0.7733 - val_accuracy: 0.6294 - val_loss: 0.8272\n",
            "Epoch 15/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6531 - loss: 0.7663 - val_accuracy: 0.6278 - val_loss: 0.8259\n",
            "Epoch 16/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6549 - loss: 0.7686 - val_accuracy: 0.6273 - val_loss: 0.8283\n",
            "Epoch 17/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6522 - loss: 0.7666 - val_accuracy: 0.6286 - val_loss: 0.8241\n",
            "Epoch 18/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6631 - loss: 0.7642 - val_accuracy: 0.6303 - val_loss: 0.8284\n",
            "Epoch 19/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6661 - loss: 0.7524 - val_accuracy: 0.6332 - val_loss: 0.8216\n",
            "Epoch 20/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6641 - loss: 0.7488 - val_accuracy: 0.6316 - val_loss: 0.8222\n",
            "Epoch 21/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6653 - loss: 0.7449 - val_accuracy: 0.6289 - val_loss: 0.8236\n",
            "Epoch 22/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6732 - loss: 0.7399 - val_accuracy: 0.6340 - val_loss: 0.8212\n",
            "Epoch 23/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6671 - loss: 0.7446 - val_accuracy: 0.6308 - val_loss: 0.8213\n",
            "Epoch 24/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6721 - loss: 0.7362 - val_accuracy: 0.6300 - val_loss: 0.8230\n",
            "Epoch 25/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6667 - loss: 0.7370 - val_accuracy: 0.6316 - val_loss: 0.8184\n",
            "Epoch 26/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6786 - loss: 0.7330 - val_accuracy: 0.6321 - val_loss: 0.8219\n",
            "Epoch 27/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6695 - loss: 0.7339 - val_accuracy: 0.6294 - val_loss: 0.8219\n",
            "Epoch 28/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6796 - loss: 0.7336 - val_accuracy: 0.6300 - val_loss: 0.8201\n",
            "Epoch 29/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6703 - loss: 0.7330 - val_accuracy: 0.6297 - val_loss: 0.8207\n",
            "Epoch 30/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6799 - loss: 0.7194 - val_accuracy: 0.6332 - val_loss: 0.8211\n",
            "Epoch 31/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6781 - loss: 0.7324 - val_accuracy: 0.6313 - val_loss: 0.8236\n",
            "Epoch 32/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6860 - loss: 0.7170 - val_accuracy: 0.6284 - val_loss: 0.8222\n",
            "Epoch 33/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6801 - loss: 0.7115 - val_accuracy: 0.6276 - val_loss: 0.8221\n",
            "Epoch 34/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6805 - loss: 0.7199 - val_accuracy: 0.6292 - val_loss: 0.8212\n",
            "Epoch 35/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6834 - loss: 0.7127 - val_accuracy: 0.6305 - val_loss: 0.8240\n",
            "Epoch 36/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6824 - loss: 0.7202 - val_accuracy: 0.6354 - val_loss: 0.8230\n",
            "Epoch 37/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6825 - loss: 0.7128 - val_accuracy: 0.6300 - val_loss: 0.8228\n",
            "Epoch 38/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6875 - loss: 0.7016 - val_accuracy: 0.6319 - val_loss: 0.8274\n",
            "Epoch 39/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6882 - loss: 0.7029 - val_accuracy: 0.6330 - val_loss: 0.8237\n",
            "Epoch 40/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6898 - loss: 0.7005 - val_accuracy: 0.6251 - val_loss: 0.8232\n",
            "Epoch 41/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6931 - loss: 0.6923 - val_accuracy: 0.6346 - val_loss: 0.8237\n",
            "Epoch 42/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6844 - loss: 0.7036 - val_accuracy: 0.6297 - val_loss: 0.8263\n",
            "Epoch 43/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6875 - loss: 0.6921 - val_accuracy: 0.6292 - val_loss: 0.8294\n",
            "Epoch 44/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6933 - loss: 0.6916 - val_accuracy: 0.6270 - val_loss: 0.8286\n",
            "Epoch 45/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6874 - loss: 0.6994 - val_accuracy: 0.6335 - val_loss: 0.8324\n",
            "Epoch 46/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6832 - loss: 0.6929 - val_accuracy: 0.6327 - val_loss: 0.8232\n",
            "Epoch 47/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6950 - loss: 0.6930 - val_accuracy: 0.6262 - val_loss: 0.8296\n",
            "Epoch 48/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6939 - loss: 0.6838 - val_accuracy: 0.6375 - val_loss: 0.8257\n",
            "Epoch 49/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6958 - loss: 0.6912 - val_accuracy: 0.6316 - val_loss: 0.8277\n",
            "Epoch 50/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6911 - loss: 0.6829 - val_accuracy: 0.6289 - val_loss: 0.8327\n",
            "History for model: {'accuracy': [0.4916397035121918, 0.5934915542602539, 0.6073355078697205, 0.6125494241714478, 0.6179431676864624, 0.6243258118629456, 0.6274721026420593, 0.6321467161178589, 0.6356526613235474, 0.6352930665016174, 0.6411362886428833, 0.6467097997665405, 0.6484178304672241, 0.6509349346160889, 0.6505753397941589, 0.6551600098609924, 0.6555196046829224, 0.6592952013015747, 0.658306360244751, 0.6625314354896545, 0.6619021892547607, 0.666037380695343, 0.6642394661903381, 0.667475700378418, 0.6684646010398865, 0.6718806028366089, 0.6706220507621765, 0.674127995967865, 0.671610951423645, 0.6737684011459351, 0.678173303604126, 0.6766451001167297, 0.6775440573692322, 0.6804206967353821, 0.6806005239486694, 0.6826680898666382, 0.6815893650054932, 0.6808701753616333, 0.6830276846885681, 0.6831175684928894, 0.6814095377922058, 0.6892305016517639, 0.6881517171859741, 0.6883315443992615, 0.685904324054718, 0.6890506744384766, 0.6908485889434814, 0.6937252879142761, 0.6944444179534912, 0.6938151717185974], 'loss': [1.1688427925109863, 0.9106409549713135, 0.85887610912323, 0.8412275910377502, 0.8291648626327515, 0.820037305355072, 0.8121659755706787, 0.8061038255691528, 0.7976574897766113, 0.7973775863647461, 0.7880271077156067, 0.7839658260345459, 0.7818916440010071, 0.7748724222183228, 0.7720786929130554, 0.7679024338722229, 0.7655560970306396, 0.7619561553001404, 0.7595823407173157, 0.7555580139160156, 0.7510582208633423, 0.7486298680305481, 0.7474645972251892, 0.7425975799560547, 0.7418306469917297, 0.7370497584342957, 0.7360206842422485, 0.7340847849845886, 0.7322479486465454, 0.7281263470649719, 0.7263379096984863, 0.7242141962051392, 0.7220239639282227, 0.7192959785461426, 0.7162503004074097, 0.7150664925575256, 0.7139894366264343, 0.7100270390510559, 0.7102121710777283, 0.706722617149353, 0.7059823274612427, 0.7036575078964233, 0.7006810903549194, 0.7003309726715088, 0.6993028521537781, 0.695541501045227, 0.6928833723068237, 0.6896551251411438, 0.6913248300552368, 0.6874954104423523], 'val_accuracy': [0.5811758637428284, 0.5951995849609375, 0.5965480208396912, 0.6113808155059814, 0.6081445813179016, 0.6121898889541626, 0.6186623573303223, 0.6173139214515686, 0.6194714307785034, 0.6275620460510254, 0.6302589178085327, 0.628910481929779, 0.6307982802391052, 0.6294498443603516, 0.6278316974639893, 0.6272923350334167, 0.6286407709121704, 0.6302589178085327, 0.6332254409790039, 0.6316073536872864, 0.628910481929779, 0.6340345144271851, 0.6307982802391052, 0.6299892067909241, 0.6316073536872864, 0.6321467161178589, 0.6294498443603516, 0.6299892067909241, 0.6297194957733154, 0.6332254409790039, 0.6313376426696777, 0.6283710598945618, 0.6275620460510254, 0.6291801333427429, 0.6305285692214966, 0.6353829503059387, 0.6299892067909241, 0.6318770051002502, 0.63295578956604, 0.6251348257064819, 0.6345738768577576, 0.6297194957733154, 0.6291801333427429, 0.6270226240158081, 0.6334951519966125, 0.6326860785484314, 0.6262136101722717, 0.6375404596328735, 0.6316073536872864, 0.628910481929779], 'val_loss': [0.9717147946357727, 0.8797135353088379, 0.8623619079589844, 0.8528270721435547, 0.847281813621521, 0.8417319655418396, 0.8385909199714661, 0.8354749083518982, 0.833300769329071, 0.8316391110420227, 0.8283789753913879, 0.8280642032623291, 0.8258103132247925, 0.8272367715835571, 0.8258988261222839, 0.8282943367958069, 0.8241326808929443, 0.8283531665802002, 0.8216251730918884, 0.8222012519836426, 0.8235989809036255, 0.8211902976036072, 0.8212518692016602, 0.8229523301124573, 0.8183731436729431, 0.8219418525695801, 0.8218884468078613, 0.8201213479042053, 0.8206979632377625, 0.8210788369178772, 0.8236369490623474, 0.8222222328186035, 0.822059690952301, 0.821233332157135, 0.8239814043045044, 0.8230180740356445, 0.8227888941764832, 0.8274097442626953, 0.8236538171768188, 0.8231722116470337, 0.8237283825874329, 0.8263302445411682, 0.8293942809104919, 0.8286152482032776, 0.8324211835861206, 0.8232411742210388, 0.8296141624450684, 0.8257207274436951, 0.8276554346084595, 0.8326796293258667]}\n",
            "Trial: Neurons = 150, Dropout Rate = 0.1, Batch Size = 64\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.4864 - loss: 1.1179 - val_accuracy: 0.6041 - val_loss: 0.8728\n",
            "Epoch 2/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6028 - loss: 0.8540 - val_accuracy: 0.6052 - val_loss: 0.8556\n",
            "Epoch 3/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6098 - loss: 0.8416 - val_accuracy: 0.6170 - val_loss: 0.8460\n",
            "Epoch 4/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6210 - loss: 0.8328 - val_accuracy: 0.6170 - val_loss: 0.8537\n",
            "Epoch 5/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6280 - loss: 0.8128 - val_accuracy: 0.6251 - val_loss: 0.8312\n",
            "Epoch 6/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6353 - loss: 0.8029 - val_accuracy: 0.6241 - val_loss: 0.8364\n",
            "Epoch 7/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6396 - loss: 0.7925 - val_accuracy: 0.6251 - val_loss: 0.8283\n",
            "Epoch 8/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6340 - loss: 0.7946 - val_accuracy: 0.6316 - val_loss: 0.8249\n",
            "Epoch 9/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6312 - loss: 0.7932 - val_accuracy: 0.6319 - val_loss: 0.8269\n",
            "Epoch 10/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6536 - loss: 0.7751 - val_accuracy: 0.6346 - val_loss: 0.8217\n",
            "Epoch 11/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6524 - loss: 0.7718 - val_accuracy: 0.6246 - val_loss: 0.8200\n",
            "Epoch 12/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6646 - loss: 0.7530 - val_accuracy: 0.6289 - val_loss: 0.8208\n",
            "Epoch 13/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6636 - loss: 0.7422 - val_accuracy: 0.6117 - val_loss: 0.8323\n",
            "Epoch 14/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6672 - loss: 0.7586 - val_accuracy: 0.6232 - val_loss: 0.8208\n",
            "Epoch 15/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6660 - loss: 0.7479 - val_accuracy: 0.6227 - val_loss: 0.8270\n",
            "Epoch 16/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6636 - loss: 0.7431 - val_accuracy: 0.6286 - val_loss: 0.8207\n",
            "Epoch 17/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6695 - loss: 0.7345 - val_accuracy: 0.6308 - val_loss: 0.8185\n",
            "Epoch 18/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6749 - loss: 0.7371 - val_accuracy: 0.6276 - val_loss: 0.8224\n",
            "Epoch 19/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6657 - loss: 0.7410 - val_accuracy: 0.6216 - val_loss: 0.8299\n",
            "Epoch 20/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6692 - loss: 0.7259 - val_accuracy: 0.6335 - val_loss: 0.8217\n",
            "Epoch 21/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6724 - loss: 0.7314 - val_accuracy: 0.6284 - val_loss: 0.8245\n",
            "Epoch 22/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6781 - loss: 0.7178 - val_accuracy: 0.6303 - val_loss: 0.8236\n",
            "Epoch 23/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6749 - loss: 0.7292 - val_accuracy: 0.6246 - val_loss: 0.8320\n",
            "Epoch 24/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6837 - loss: 0.7102 - val_accuracy: 0.6357 - val_loss: 0.8198\n",
            "Epoch 25/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6722 - loss: 0.7224 - val_accuracy: 0.6303 - val_loss: 0.8300\n",
            "Epoch 26/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6701 - loss: 0.7222 - val_accuracy: 0.6289 - val_loss: 0.8204\n",
            "Epoch 27/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6818 - loss: 0.7129 - val_accuracy: 0.6270 - val_loss: 0.8273\n",
            "Epoch 28/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6832 - loss: 0.7127 - val_accuracy: 0.6294 - val_loss: 0.8248\n",
            "Epoch 29/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6934 - loss: 0.7005 - val_accuracy: 0.6273 - val_loss: 0.8272\n",
            "Epoch 30/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6849 - loss: 0.7075 - val_accuracy: 0.6179 - val_loss: 0.8375\n",
            "Epoch 31/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6860 - loss: 0.6992 - val_accuracy: 0.6313 - val_loss: 0.8289\n",
            "Epoch 32/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6944 - loss: 0.6951 - val_accuracy: 0.6338 - val_loss: 0.8349\n",
            "Epoch 33/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6951 - loss: 0.6787 - val_accuracy: 0.6195 - val_loss: 0.8389\n",
            "Epoch 34/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6984 - loss: 0.6906 - val_accuracy: 0.6246 - val_loss: 0.8368\n",
            "Epoch 35/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6963 - loss: 0.6916 - val_accuracy: 0.6316 - val_loss: 0.8424\n",
            "Epoch 36/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6969 - loss: 0.6824 - val_accuracy: 0.6297 - val_loss: 0.8348\n",
            "Epoch 37/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7012 - loss: 0.6690 - val_accuracy: 0.6278 - val_loss: 0.8368\n",
            "Epoch 38/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6930 - loss: 0.6831 - val_accuracy: 0.6173 - val_loss: 0.8459\n",
            "Epoch 39/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6954 - loss: 0.6845 - val_accuracy: 0.6286 - val_loss: 0.8486\n",
            "Epoch 40/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7004 - loss: 0.6736 - val_accuracy: 0.6238 - val_loss: 0.8433\n",
            "Epoch 41/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7057 - loss: 0.6674 - val_accuracy: 0.6265 - val_loss: 0.8462\n",
            "Epoch 42/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7048 - loss: 0.6691 - val_accuracy: 0.6300 - val_loss: 0.8438\n",
            "Epoch 43/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7170 - loss: 0.6527 - val_accuracy: 0.6211 - val_loss: 0.8469\n",
            "Epoch 44/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7019 - loss: 0.6667 - val_accuracy: 0.6243 - val_loss: 0.8476\n",
            "Epoch 45/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7091 - loss: 0.6629 - val_accuracy: 0.6257 - val_loss: 0.8504\n",
            "Epoch 46/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7037 - loss: 0.6671 - val_accuracy: 0.6294 - val_loss: 0.8471\n",
            "Epoch 47/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7070 - loss: 0.6576 - val_accuracy: 0.6273 - val_loss: 0.8570\n",
            "Epoch 48/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7088 - loss: 0.6552 - val_accuracy: 0.6273 - val_loss: 0.8585\n",
            "Epoch 49/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7089 - loss: 0.6566 - val_accuracy: 0.6284 - val_loss: 0.8591\n",
            "Epoch 50/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7110 - loss: 0.6423 - val_accuracy: 0.6276 - val_loss: 0.8552\n",
            "History for model: {'accuracy': [0.5471053719520569, 0.602750837802887, 0.6135383248329163, 0.626303493976593, 0.6255843043327332, 0.6316073536872864, 0.638079822063446, 0.641046404838562, 0.6379000544548035, 0.649137020111084, 0.6494066715240479, 0.6535418629646301, 0.6551600098609924, 0.6608234643936157, 0.6597446799278259, 0.6620819568634033, 0.6638798713684082, 0.6659474968910217, 0.6647788286209106, 0.6671161651611328, 0.667475700378418, 0.6734987497329712, 0.6777238249778748, 0.6722401976585388, 0.6777238249778748, 0.6762855052947998, 0.6830276846885681, 0.683387279510498, 0.6844660043716431, 0.685904324054718, 0.686263918876648, 0.6875224709510803, 0.6861740350723267, 0.6917475461959839, 0.6915677785873413, 0.6934555768966675, 0.6938151717185974, 0.6922869682312012, 0.6953433752059937, 0.6962423324584961, 0.7001977562904358, 0.7001078724861145, 0.7040632963180542, 0.700647234916687, 0.7047824263572693, 0.7019057869911194, 0.7054117321968079, 0.707299530506134, 0.7046026587486267, 0.7072096467018127], 'loss': [0.9898638129234314, 0.8564650416374207, 0.8403148055076599, 0.8205675482749939, 0.814095139503479, 0.8067896962165833, 0.7957443594932556, 0.7886956334114075, 0.7875733971595764, 0.7760034203529358, 0.7770092487335205, 0.7695716619491577, 0.7633013129234314, 0.7585293650627136, 0.7535511255264282, 0.7486250400543213, 0.7457031607627869, 0.743224561214447, 0.7398224472999573, 0.736443281173706, 0.7324399352073669, 0.727588951587677, 0.7225791215896606, 0.7220404744148254, 0.7183555960655212, 0.7183563709259033, 0.7095872759819031, 0.7089410424232483, 0.7076892852783203, 0.7042449712753296, 0.7013132572174072, 0.6991167664527893, 0.6953862309455872, 0.6943165063858032, 0.6901337504386902, 0.6904687881469727, 0.6832822561264038, 0.6885694265365601, 0.68043452501297, 0.67915940284729, 0.6754205226898193, 0.6757826805114746, 0.6714620590209961, 0.6719530820846558, 0.6655228734016418, 0.6686916947364807, 0.6635566353797913, 0.66204833984375, 0.6602880358695984, 0.6566057801246643], 'val_accuracy': [0.6040992736816406, 0.6051779985427856, 0.61704421043396, 0.61704421043396, 0.6251348257064819, 0.6240561008453369, 0.6251348257064819, 0.6316073536872864, 0.6318770051002502, 0.6345738768577576, 0.6245954632759094, 0.628910481929779, 0.6116504669189453, 0.6232470273971558, 0.6227076649665833, 0.6286407709121704, 0.6307982802391052, 0.6275620460510254, 0.6216289401054382, 0.6334951519966125, 0.6283710598945618, 0.6302589178085327, 0.6245954632759094, 0.6356526613235474, 0.6302589178085327, 0.628910481929779, 0.6270226240158081, 0.6294498443603516, 0.6272923350334167, 0.6178532838821411, 0.6313376426696777, 0.6337648034095764, 0.6194714307785034, 0.6245954632759094, 0.6316073536872864, 0.6297194957733154, 0.6278316974639893, 0.6173139214515686, 0.6286407709121704, 0.6237863898277283, 0.6264832615852356, 0.6299892067909241, 0.621089518070221, 0.6243258118629456, 0.6256741881370544, 0.6294498443603516, 0.6272923350334167, 0.6272923350334167, 0.6283710598945618, 0.6275620460510254], 'val_loss': [0.8727924227714539, 0.8555917143821716, 0.8459749817848206, 0.8537372946739197, 0.831211268901825, 0.8364318013191223, 0.8283224105834961, 0.8249385356903076, 0.8268621563911438, 0.8216530084609985, 0.819998025894165, 0.8207890391349792, 0.8323376178741455, 0.8207682967185974, 0.8270018100738525, 0.8206825256347656, 0.8185338973999023, 0.8223973512649536, 0.8299028277397156, 0.8216628432273865, 0.8245395421981812, 0.8235859274864197, 0.8319922089576721, 0.8197762370109558, 0.8300159573554993, 0.8203778266906738, 0.8272810578346252, 0.8248385787010193, 0.827166736125946, 0.8374954462051392, 0.8288846015930176, 0.8348577618598938, 0.838870108127594, 0.8368250727653503, 0.842399537563324, 0.8347549438476562, 0.8367538452148438, 0.8458659052848816, 0.8485966920852661, 0.8432671427726746, 0.846229076385498, 0.8437789082527161, 0.8468946814537048, 0.8475844860076904, 0.850446343421936, 0.8470690250396729, 0.8570278286933899, 0.8585159182548523, 0.8590980172157288, 0.8551720380783081]}\n",
            "Trial: Neurons = 150, Dropout Rate = 0.1, Batch Size = 128\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.4594 - loss: 1.1804 - val_accuracy: 0.5957 - val_loss: 0.9041\n",
            "Epoch 2/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6014 - loss: 0.8883 - val_accuracy: 0.6068 - val_loss: 0.8648\n",
            "Epoch 3/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6136 - loss: 0.8479 - val_accuracy: 0.6119 - val_loss: 0.8481\n",
            "Epoch 4/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6204 - loss: 0.8259 - val_accuracy: 0.6063 - val_loss: 0.8425\n",
            "Epoch 5/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6261 - loss: 0.8274 - val_accuracy: 0.6187 - val_loss: 0.8354\n",
            "Epoch 6/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6257 - loss: 0.8181 - val_accuracy: 0.6168 - val_loss: 0.8372\n",
            "Epoch 7/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6333 - loss: 0.8045 - val_accuracy: 0.6232 - val_loss: 0.8313\n",
            "Epoch 8/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6409 - loss: 0.7997 - val_accuracy: 0.6203 - val_loss: 0.8276\n",
            "Epoch 9/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6397 - loss: 0.7879 - val_accuracy: 0.6259 - val_loss: 0.8195\n",
            "Epoch 10/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6429 - loss: 0.7848 - val_accuracy: 0.6257 - val_loss: 0.8264\n",
            "Epoch 11/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6444 - loss: 0.7844 - val_accuracy: 0.6276 - val_loss: 0.8185\n",
            "Epoch 12/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6519 - loss: 0.7721 - val_accuracy: 0.6286 - val_loss: 0.8180\n",
            "Epoch 13/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6432 - loss: 0.7792 - val_accuracy: 0.6208 - val_loss: 0.8204\n",
            "Epoch 14/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6492 - loss: 0.7660 - val_accuracy: 0.6321 - val_loss: 0.8139\n",
            "Epoch 15/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6522 - loss: 0.7709 - val_accuracy: 0.6297 - val_loss: 0.8140\n",
            "Epoch 16/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6502 - loss: 0.7639 - val_accuracy: 0.6316 - val_loss: 0.8139\n",
            "Epoch 17/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6585 - loss: 0.7583 - val_accuracy: 0.6330 - val_loss: 0.8119\n",
            "Epoch 18/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6571 - loss: 0.7598 - val_accuracy: 0.6313 - val_loss: 0.8123\n",
            "Epoch 19/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6596 - loss: 0.7550 - val_accuracy: 0.6324 - val_loss: 0.8095\n",
            "Epoch 20/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6576 - loss: 0.7501 - val_accuracy: 0.6276 - val_loss: 0.8161\n",
            "Epoch 21/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6649 - loss: 0.7513 - val_accuracy: 0.6327 - val_loss: 0.8100\n",
            "Epoch 22/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6712 - loss: 0.7430 - val_accuracy: 0.6211 - val_loss: 0.8175\n",
            "Epoch 23/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6770 - loss: 0.7300 - val_accuracy: 0.6251 - val_loss: 0.8136\n",
            "Epoch 24/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6698 - loss: 0.7330 - val_accuracy: 0.6367 - val_loss: 0.8091\n",
            "Epoch 25/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6613 - loss: 0.7425 - val_accuracy: 0.6316 - val_loss: 0.8135\n",
            "Epoch 26/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6720 - loss: 0.7283 - val_accuracy: 0.6365 - val_loss: 0.8119\n",
            "Epoch 27/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6667 - loss: 0.7306 - val_accuracy: 0.6362 - val_loss: 0.8141\n",
            "Epoch 28/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6786 - loss: 0.7202 - val_accuracy: 0.6338 - val_loss: 0.8093\n",
            "Epoch 29/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6739 - loss: 0.7286 - val_accuracy: 0.6343 - val_loss: 0.8173\n",
            "Epoch 30/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6769 - loss: 0.7211 - val_accuracy: 0.6340 - val_loss: 0.8121\n",
            "Epoch 31/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6748 - loss: 0.7230 - val_accuracy: 0.6367 - val_loss: 0.8137\n",
            "Epoch 32/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6766 - loss: 0.7116 - val_accuracy: 0.6324 - val_loss: 0.8128\n",
            "Epoch 33/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6805 - loss: 0.7123 - val_accuracy: 0.6348 - val_loss: 0.8141\n",
            "Epoch 34/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6881 - loss: 0.6968 - val_accuracy: 0.6268 - val_loss: 0.8237\n",
            "Epoch 35/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6844 - loss: 0.7132 - val_accuracy: 0.6373 - val_loss: 0.8159\n",
            "Epoch 36/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6910 - loss: 0.6979 - val_accuracy: 0.6305 - val_loss: 0.8173\n",
            "Epoch 37/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6801 - loss: 0.7014 - val_accuracy: 0.6338 - val_loss: 0.8132\n",
            "Epoch 38/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6907 - loss: 0.6977 - val_accuracy: 0.6276 - val_loss: 0.8200\n",
            "Epoch 39/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6875 - loss: 0.6885 - val_accuracy: 0.6308 - val_loss: 0.8168\n",
            "Epoch 40/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6844 - loss: 0.6985 - val_accuracy: 0.6397 - val_loss: 0.8135\n",
            "Epoch 41/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6857 - loss: 0.6930 - val_accuracy: 0.6313 - val_loss: 0.8167\n",
            "Epoch 42/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6895 - loss: 0.6953 - val_accuracy: 0.6300 - val_loss: 0.8204\n",
            "Epoch 43/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6894 - loss: 0.6878 - val_accuracy: 0.6348 - val_loss: 0.8203\n",
            "Epoch 44/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6815 - loss: 0.6969 - val_accuracy: 0.6270 - val_loss: 0.8251\n",
            "Epoch 45/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6888 - loss: 0.6918 - val_accuracy: 0.6346 - val_loss: 0.8210\n",
            "Epoch 46/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6958 - loss: 0.6805 - val_accuracy: 0.6303 - val_loss: 0.8225\n",
            "Epoch 47/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6923 - loss: 0.6858 - val_accuracy: 0.6262 - val_loss: 0.8271\n",
            "Epoch 48/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6940 - loss: 0.6824 - val_accuracy: 0.6257 - val_loss: 0.8248\n",
            "Epoch 49/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6962 - loss: 0.6833 - val_accuracy: 0.6332 - val_loss: 0.8218\n",
            "Epoch 50/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6845 - loss: 0.6818 - val_accuracy: 0.6346 - val_loss: 0.8257\n",
            "History for model: {'accuracy': [0.5308342576026917, 0.6003236174583435, 0.6085940599441528, 0.6177634000778198, 0.6248651742935181, 0.6261236667633057, 0.6298993229866028, 0.6366415023803711, 0.6358324289321899, 0.6401474475860596, 0.6432937979698181, 0.646170437335968, 0.6446422338485718, 0.6468896269798279, 0.6526429057121277, 0.651654064655304, 0.6556993722915649, 0.6567781567573547, 0.6590255498886108, 0.6618123054504395, 0.6625314354896545, 0.663430392742157, 0.669003963470459, 0.6676555275917053, 0.6670262217521667, 0.6702625155448914, 0.670082688331604, 0.67268967628479, 0.6748471856117249, 0.6753865480422974, 0.6743977069854736, 0.6780834197998047, 0.6794318556785583, 0.6763753890991211, 0.6806904077529907, 0.6831175684928894, 0.6768248677253723, 0.6819489598274231, 0.6847357153892517, 0.6819489598274231, 0.6841064095497131, 0.6939050555229187, 0.6865336298942566, 0.6858144402503967, 0.6869831085205078, 0.6886911392211914, 0.6905789375305176, 0.69147789478302, 0.69399493932724, 0.69183748960495], 'loss': [1.0565123558044434, 0.8819898962974548, 0.8499034643173218, 0.8322839140892029, 0.8235499858856201, 0.8163151144981384, 0.8069582581520081, 0.7988461852073669, 0.7939646244049072, 0.7892369627952576, 0.7851521372795105, 0.7812132239341736, 0.7764420509338379, 0.7721365690231323, 0.7683707475662231, 0.7636874318122864, 0.7598354816436768, 0.759209156036377, 0.7531181573867798, 0.7505720257759094, 0.7478582859039307, 0.7450913190841675, 0.740105152130127, 0.7365431785583496, 0.7377362847328186, 0.7312756776809692, 0.7302068471908569, 0.7291820049285889, 0.7259974479675293, 0.7214754819869995, 0.7213680148124695, 0.7178037166595459, 0.7157156467437744, 0.7155129909515381, 0.7107862234115601, 0.7080017328262329, 0.707770586013794, 0.705178439617157, 0.7018859386444092, 0.7027815580368042, 0.6951627135276794, 0.6924723386764526, 0.6940588355064392, 0.6922190189361572, 0.6916155219078064, 0.6869190335273743, 0.6866493821144104, 0.6838792562484741, 0.6830898523330688, 0.6811813116073608], 'val_accuracy': [0.59573894739151, 0.606796145439148, 0.611920177936554, 0.6062567234039307, 0.6186623573303223, 0.6167745590209961, 0.6232470273971558, 0.6202805042266846, 0.6259438991546631, 0.6256741881370544, 0.6275620460510254, 0.6286407709121704, 0.6208198666572571, 0.6321467161178589, 0.6297194957733154, 0.6316073536872864, 0.63295578956604, 0.6313376426696777, 0.6324163675308228, 0.6275620460510254, 0.6326860785484314, 0.621089518070221, 0.6251348257064819, 0.6367313861846924, 0.6316073536872864, 0.6364616751670837, 0.6361920237541199, 0.6337648034095764, 0.6343042254447937, 0.6340345144271851, 0.6367313861846924, 0.6324163675308228, 0.6348435878753662, 0.6267529726028442, 0.6372707486152649, 0.6305285692214966, 0.6337648034095764, 0.6275620460510254, 0.6307982802391052, 0.6396979689598083, 0.6313376426696777, 0.6299892067909241, 0.6348435878753662, 0.6270226240158081, 0.6345738768577576, 0.6302589178085327, 0.6262136101722717, 0.6256741881370544, 0.6332254409790039, 0.6345738768577576], 'val_loss': [0.9040604829788208, 0.8647952675819397, 0.848091721534729, 0.8424609899520874, 0.8353995084762573, 0.8371954560279846, 0.8312591314315796, 0.8276469707489014, 0.8195304274559021, 0.8264490962028503, 0.8185109496116638, 0.8179826140403748, 0.8204220533370972, 0.8139172196388245, 0.8140068650245667, 0.8139215707778931, 0.8118862509727478, 0.8123043775558472, 0.8094671964645386, 0.8160558938980103, 0.8099995851516724, 0.8174803853034973, 0.8135778307914734, 0.8090680837631226, 0.81351637840271, 0.811858057975769, 0.8140525817871094, 0.8092681169509888, 0.8173066973686218, 0.812070906162262, 0.813674807548523, 0.8127986192703247, 0.8140563368797302, 0.8237003684043884, 0.815911591053009, 0.8172957897186279, 0.8132073283195496, 0.8200292587280273, 0.8167664408683777, 0.8135432004928589, 0.8166751861572266, 0.8204165101051331, 0.8202511072158813, 0.8251322507858276, 0.8210418820381165, 0.8224604725837708, 0.8270691633224487, 0.8248423337936401, 0.8218253254890442, 0.8256517052650452]}\n",
            "Trial: Neurons = 150, Dropout Rate = 0.1, Batch Size = 256\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.3704 - loss: 1.2887 - val_accuracy: 0.5607 - val_loss: 0.9781\n",
            "Epoch 2/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5664 - loss: 0.9534 - val_accuracy: 0.5868 - val_loss: 0.8942\n",
            "Epoch 3/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6036 - loss: 0.8770 - val_accuracy: 0.5987 - val_loss: 0.8702\n",
            "Epoch 4/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6069 - loss: 0.8494 - val_accuracy: 0.6130 - val_loss: 0.8537\n",
            "Epoch 5/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6078 - loss: 0.8458 - val_accuracy: 0.6141 - val_loss: 0.8495\n",
            "Epoch 6/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6246 - loss: 0.8314 - val_accuracy: 0.6200 - val_loss: 0.8413\n",
            "Epoch 7/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6189 - loss: 0.8261 - val_accuracy: 0.6203 - val_loss: 0.8381\n",
            "Epoch 8/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6214 - loss: 0.8219 - val_accuracy: 0.6238 - val_loss: 0.8353\n",
            "Epoch 9/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6358 - loss: 0.8049 - val_accuracy: 0.6257 - val_loss: 0.8316\n",
            "Epoch 10/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6329 - loss: 0.7961 - val_accuracy: 0.6313 - val_loss: 0.8296\n",
            "Epoch 11/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6239 - loss: 0.8064 - val_accuracy: 0.6211 - val_loss: 0.8282\n",
            "Epoch 12/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6363 - loss: 0.7881 - val_accuracy: 0.6270 - val_loss: 0.8274\n",
            "Epoch 13/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6431 - loss: 0.7866 - val_accuracy: 0.6270 - val_loss: 0.8238\n",
            "Epoch 14/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6392 - loss: 0.7926 - val_accuracy: 0.6241 - val_loss: 0.8252\n",
            "Epoch 15/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6467 - loss: 0.7776 - val_accuracy: 0.6340 - val_loss: 0.8199\n",
            "Epoch 16/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6440 - loss: 0.7777 - val_accuracy: 0.6321 - val_loss: 0.8246\n",
            "Epoch 17/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6532 - loss: 0.7734 - val_accuracy: 0.6286 - val_loss: 0.8245\n",
            "Epoch 18/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6567 - loss: 0.7679 - val_accuracy: 0.6249 - val_loss: 0.8208\n",
            "Epoch 19/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6500 - loss: 0.7759 - val_accuracy: 0.6316 - val_loss: 0.8192\n",
            "Epoch 20/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6559 - loss: 0.7576 - val_accuracy: 0.6305 - val_loss: 0.8194\n",
            "Epoch 21/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6597 - loss: 0.7591 - val_accuracy: 0.6324 - val_loss: 0.8174\n",
            "Epoch 22/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6654 - loss: 0.7507 - val_accuracy: 0.6313 - val_loss: 0.8193\n",
            "Epoch 23/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6628 - loss: 0.7526 - val_accuracy: 0.6335 - val_loss: 0.8188\n",
            "Epoch 24/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6638 - loss: 0.7524 - val_accuracy: 0.6378 - val_loss: 0.8133\n",
            "Epoch 25/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6704 - loss: 0.7390 - val_accuracy: 0.6254 - val_loss: 0.8161\n",
            "Epoch 26/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6633 - loss: 0.7464 - val_accuracy: 0.6354 - val_loss: 0.8163\n",
            "Epoch 27/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6683 - loss: 0.7384 - val_accuracy: 0.6348 - val_loss: 0.8128\n",
            "Epoch 28/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6688 - loss: 0.7420 - val_accuracy: 0.6357 - val_loss: 0.8144\n",
            "Epoch 29/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6695 - loss: 0.7415 - val_accuracy: 0.6321 - val_loss: 0.8143\n",
            "Epoch 30/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6743 - loss: 0.7282 - val_accuracy: 0.6300 - val_loss: 0.8153\n",
            "Epoch 31/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6669 - loss: 0.7400 - val_accuracy: 0.6276 - val_loss: 0.8161\n",
            "Epoch 32/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6672 - loss: 0.7389 - val_accuracy: 0.6383 - val_loss: 0.8132\n",
            "Epoch 33/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6719 - loss: 0.7278 - val_accuracy: 0.6348 - val_loss: 0.8168\n",
            "Epoch 34/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6750 - loss: 0.7236 - val_accuracy: 0.6313 - val_loss: 0.8138\n",
            "Epoch 35/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6784 - loss: 0.7222 - val_accuracy: 0.6370 - val_loss: 0.8134\n",
            "Epoch 36/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6804 - loss: 0.7231 - val_accuracy: 0.6405 - val_loss: 0.8131\n",
            "Epoch 37/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6749 - loss: 0.7225 - val_accuracy: 0.6330 - val_loss: 0.8142\n",
            "Epoch 38/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6807 - loss: 0.7100 - val_accuracy: 0.6359 - val_loss: 0.8140\n",
            "Epoch 39/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6761 - loss: 0.7178 - val_accuracy: 0.6389 - val_loss: 0.8130\n",
            "Epoch 40/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6807 - loss: 0.7087 - val_accuracy: 0.6381 - val_loss: 0.8139\n",
            "Epoch 41/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6801 - loss: 0.7170 - val_accuracy: 0.6367 - val_loss: 0.8151\n",
            "Epoch 42/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6804 - loss: 0.7133 - val_accuracy: 0.6373 - val_loss: 0.8128\n",
            "Epoch 43/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6752 - loss: 0.7154 - val_accuracy: 0.6389 - val_loss: 0.8150\n",
            "Epoch 44/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6873 - loss: 0.7022 - val_accuracy: 0.6303 - val_loss: 0.8191\n",
            "Epoch 45/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6892 - loss: 0.7006 - val_accuracy: 0.6321 - val_loss: 0.8189\n",
            "Epoch 46/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6841 - loss: 0.7002 - val_accuracy: 0.6289 - val_loss: 0.8170\n",
            "Epoch 47/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6924 - loss: 0.6929 - val_accuracy: 0.6335 - val_loss: 0.8148\n",
            "Epoch 48/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6836 - loss: 0.7078 - val_accuracy: 0.6305 - val_loss: 0.8176\n",
            "Epoch 49/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6955 - loss: 0.6936 - val_accuracy: 0.6340 - val_loss: 0.8180\n",
            "Epoch 50/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6865 - loss: 0.7039 - val_accuracy: 0.6330 - val_loss: 0.8202\n",
            "History for model: {'accuracy': [0.4653002619743347, 0.5735347270965576, 0.5954692363739014, 0.6009529232978821, 0.611560583114624, 0.6230672597885132, 0.6218985915184021, 0.627741813659668, 0.6297194957733154, 0.631427526473999, 0.6342142820358276, 0.6342142820358276, 0.6413160562515259, 0.6429342031478882, 0.6441028118133545, 0.646170437335968, 0.6508449912071228, 0.6521934270858765, 0.6489571928977966, 0.6527328491210938, 0.6563286781311035, 0.6577669978141785, 0.6578568816184998, 0.660913348197937, 0.6631607413291931, 0.6629809141159058, 0.6629809141159058, 0.6621719002723694, 0.6653182506561279, 0.6679251790046692, 0.6692736148834229, 0.6682847738265991, 0.6713412404060364, 0.6718806028366089, 0.670082688331604, 0.6733189225196838, 0.6742178797721863, 0.6744875907897949, 0.6720604300498962, 0.6752966642379761, 0.6804206967353821, 0.6757461428642273, 0.678173303604126, 0.6794318556785583, 0.683387279510498, 0.6768248677253723, 0.6861740350723267, 0.6850953102111816, 0.6873427033424377, 0.6855447888374329], 'loss': [1.163155436515808, 0.931059718132019, 0.8734865188598633, 0.8573983311653137, 0.8407222628593445, 0.8267300128936768, 0.8226771950721741, 0.8128230571746826, 0.8071632385253906, 0.8020532131195068, 0.7988653182983398, 0.795482873916626, 0.7914171814918518, 0.7875572443008423, 0.7830467820167542, 0.776817798614502, 0.7765437960624695, 0.7713070511817932, 0.7692395448684692, 0.7661476135253906, 0.7635913491249084, 0.7596167325973511, 0.759556233882904, 0.7549813985824585, 0.7502326369285583, 0.7489963173866272, 0.7495490908622742, 0.7475160956382751, 0.7433040142059326, 0.7404528856277466, 0.7370928525924683, 0.7353599667549133, 0.7314013838768005, 0.7303280830383301, 0.7302811145782471, 0.7258486747741699, 0.7280070185661316, 0.7203444838523865, 0.7243123650550842, 0.7175615429878235, 0.7144399881362915, 0.716189444065094, 0.7135525941848755, 0.7126314043998718, 0.7102364301681519, 0.7114520072937012, 0.7056809663772583, 0.7028573751449585, 0.7024405598640442, 0.7019928097724915], 'val_accuracy': [0.5606796145439148, 0.5868392586708069, 0.598705530166626, 0.612998902797699, 0.6140776872634888, 0.6200107932090759, 0.6202805042266846, 0.6237863898277283, 0.6256741881370544, 0.6313376426696777, 0.621089518070221, 0.6270226240158081, 0.6270226240158081, 0.6240561008453369, 0.6340345144271851, 0.6321467161178589, 0.6286407709121704, 0.6248651742935181, 0.6316073536872864, 0.6305285692214966, 0.6324163675308228, 0.6313376426696777, 0.6334951519966125, 0.6378101110458374, 0.6254045367240906, 0.6353829503059387, 0.6348435878753662, 0.6356526613235474, 0.6321467161178589, 0.6299892067909241, 0.6275620460510254, 0.6383495330810547, 0.6348435878753662, 0.6313376426696777, 0.637001097202301, 0.6405069828033447, 0.63295578956604, 0.6359223127365112, 0.6388888955116272, 0.638079822063446, 0.6367313861846924, 0.6372707486152649, 0.6388888955116272, 0.6302589178085327, 0.6321467161178589, 0.628910481929779, 0.6334951519966125, 0.6305285692214966, 0.6340345144271851, 0.63295578956604], 'val_loss': [0.978094756603241, 0.8942149877548218, 0.8702079653739929, 0.8537479639053345, 0.8495125770568848, 0.841301679611206, 0.838078498840332, 0.8352829217910767, 0.831569254398346, 0.829575777053833, 0.8282364010810852, 0.8274280428886414, 0.8238346576690674, 0.8252040147781372, 0.8199093341827393, 0.8245611190795898, 0.8245409727096558, 0.8208329677581787, 0.8192372918128967, 0.8194032311439514, 0.817389190196991, 0.8193253874778748, 0.8188363313674927, 0.8133022785186768, 0.8160802125930786, 0.8162546157836914, 0.8127675652503967, 0.81440269947052, 0.8143031001091003, 0.8153296709060669, 0.816050112247467, 0.8132138252258301, 0.8168317675590515, 0.8138033747673035, 0.8134070634841919, 0.8131141662597656, 0.8141611814498901, 0.8139694929122925, 0.8130456805229187, 0.8138880133628845, 0.8150845170021057, 0.8128488063812256, 0.8149600625038147, 0.8190741539001465, 0.8189136385917664, 0.817044734954834, 0.814802348613739, 0.8176091313362122, 0.8179734349250793, 0.8202409148216248]}\n",
            "Trial: Neurons = 150, Dropout Rate = 0.5, Batch Size = 64\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4080 - loss: 1.2537 - val_accuracy: 0.5925 - val_loss: 0.8943\n",
            "Epoch 2/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5714 - loss: 0.9149 - val_accuracy: 0.5963 - val_loss: 0.8651\n",
            "Epoch 3/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5886 - loss: 0.8761 - val_accuracy: 0.6036 - val_loss: 0.8525\n",
            "Epoch 4/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6119 - loss: 0.8664 - val_accuracy: 0.6117 - val_loss: 0.8479\n",
            "Epoch 5/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6079 - loss: 0.8380 - val_accuracy: 0.6146 - val_loss: 0.8362\n",
            "Epoch 6/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6244 - loss: 0.8377 - val_accuracy: 0.6143 - val_loss: 0.8399\n",
            "Epoch 7/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6213 - loss: 0.8217 - val_accuracy: 0.6176 - val_loss: 0.8333\n",
            "Epoch 8/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6340 - loss: 0.8080 - val_accuracy: 0.6192 - val_loss: 0.8259\n",
            "Epoch 9/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6244 - loss: 0.8169 - val_accuracy: 0.6216 - val_loss: 0.8255\n",
            "Epoch 10/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6352 - loss: 0.7991 - val_accuracy: 0.6257 - val_loss: 0.8215\n",
            "Epoch 11/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6384 - loss: 0.8017 - val_accuracy: 0.6300 - val_loss: 0.8201\n",
            "Epoch 12/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6394 - loss: 0.8008 - val_accuracy: 0.6278 - val_loss: 0.8209\n",
            "Epoch 13/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6470 - loss: 0.7919 - val_accuracy: 0.6259 - val_loss: 0.8208\n",
            "Epoch 14/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6402 - loss: 0.7890 - val_accuracy: 0.6268 - val_loss: 0.8194\n",
            "Epoch 15/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6407 - loss: 0.7959 - val_accuracy: 0.6327 - val_loss: 0.8142\n",
            "Epoch 16/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6465 - loss: 0.7861 - val_accuracy: 0.6305 - val_loss: 0.8151\n",
            "Epoch 17/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6526 - loss: 0.7696 - val_accuracy: 0.6294 - val_loss: 0.8188\n",
            "Epoch 18/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6526 - loss: 0.7779 - val_accuracy: 0.6300 - val_loss: 0.8161\n",
            "Epoch 19/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6547 - loss: 0.7688 - val_accuracy: 0.6373 - val_loss: 0.8134\n",
            "Epoch 20/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6582 - loss: 0.7745 - val_accuracy: 0.6305 - val_loss: 0.8137\n",
            "Epoch 21/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6527 - loss: 0.7747 - val_accuracy: 0.6268 - val_loss: 0.8181\n",
            "Epoch 22/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6608 - loss: 0.7548 - val_accuracy: 0.6335 - val_loss: 0.8127\n",
            "Epoch 23/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6601 - loss: 0.7646 - val_accuracy: 0.6340 - val_loss: 0.8157\n",
            "Epoch 24/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6674 - loss: 0.7403 - val_accuracy: 0.6276 - val_loss: 0.8120\n",
            "Epoch 25/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6588 - loss: 0.7565 - val_accuracy: 0.6292 - val_loss: 0.8130\n",
            "Epoch 26/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6718 - loss: 0.7383 - val_accuracy: 0.6311 - val_loss: 0.8116\n",
            "Epoch 27/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6634 - loss: 0.7533 - val_accuracy: 0.6276 - val_loss: 0.8183\n",
            "Epoch 28/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6642 - loss: 0.7497 - val_accuracy: 0.6281 - val_loss: 0.8203\n",
            "Epoch 29/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6648 - loss: 0.7539 - val_accuracy: 0.6327 - val_loss: 0.8170\n",
            "Epoch 30/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6694 - loss: 0.7362 - val_accuracy: 0.6311 - val_loss: 0.8166\n",
            "Epoch 31/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6628 - loss: 0.7420 - val_accuracy: 0.6321 - val_loss: 0.8144\n",
            "Epoch 32/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6709 - loss: 0.7426 - val_accuracy: 0.6338 - val_loss: 0.8167\n",
            "Epoch 33/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6771 - loss: 0.7292 - val_accuracy: 0.6319 - val_loss: 0.8223\n",
            "Epoch 34/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6657 - loss: 0.7430 - val_accuracy: 0.6367 - val_loss: 0.8145\n",
            "Epoch 35/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6682 - loss: 0.7448 - val_accuracy: 0.6343 - val_loss: 0.8161\n",
            "Epoch 36/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6711 - loss: 0.7260 - val_accuracy: 0.6338 - val_loss: 0.8181\n",
            "Epoch 37/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6779 - loss: 0.7201 - val_accuracy: 0.6303 - val_loss: 0.8233\n",
            "Epoch 38/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6767 - loss: 0.7309 - val_accuracy: 0.6319 - val_loss: 0.8249\n",
            "Epoch 39/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6856 - loss: 0.7026 - val_accuracy: 0.6381 - val_loss: 0.8181\n",
            "Epoch 40/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6781 - loss: 0.7225 - val_accuracy: 0.6378 - val_loss: 0.8207\n",
            "Epoch 41/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6752 - loss: 0.7277 - val_accuracy: 0.6408 - val_loss: 0.8213\n",
            "Epoch 42/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6783 - loss: 0.7166 - val_accuracy: 0.6321 - val_loss: 0.8229\n",
            "Epoch 43/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6834 - loss: 0.7108 - val_accuracy: 0.6343 - val_loss: 0.8236\n",
            "Epoch 44/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6842 - loss: 0.7006 - val_accuracy: 0.6319 - val_loss: 0.8252\n",
            "Epoch 45/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6804 - loss: 0.7086 - val_accuracy: 0.6346 - val_loss: 0.8252\n",
            "Epoch 46/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6792 - loss: 0.7039 - val_accuracy: 0.6386 - val_loss: 0.8292\n",
            "Epoch 47/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6860 - loss: 0.7094 - val_accuracy: 0.6343 - val_loss: 0.8306\n",
            "Epoch 48/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6804 - loss: 0.7094 - val_accuracy: 0.6284 - val_loss: 0.8315\n",
            "Epoch 49/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6913 - loss: 0.6914 - val_accuracy: 0.6410 - val_loss: 0.8251\n",
            "Epoch 50/50\n",
            "\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6903 - loss: 0.6929 - val_accuracy: 0.6362 - val_loss: 0.8312\n",
            "History for model: {'accuracy': [0.4943365752696991, 0.5740740895271301, 0.592772364616394, 0.607515275478363, 0.6122797727584839, 0.6204602718353271, 0.6245954632759094, 0.6270226240158081, 0.6283710598945618, 0.6353829503059387, 0.6360121965408325, 0.6366415023803711, 0.6416756510734558, 0.6399676203727722, 0.6444624066352844, 0.6431139707565308, 0.6432937979698181, 0.6499460339546204, 0.6479683518409729, 0.650215744972229, 0.6503955125808716, 0.6576771140098572, 0.658306360244751, 0.6561488509178162, 0.6574972867965698, 0.6628910303115845, 0.6601042747497559, 0.6637001037597656, 0.6633405089378357, 0.6650485396385193, 0.6618123054504395, 0.6712513566017151, 0.670082688331604, 0.6687342524528503, 0.6671161651611328, 0.6685544848442078, 0.6711614727973938, 0.6752966642379761, 0.6746673583984375, 0.6765552163124084, 0.6765552163124084, 0.6759259104728699, 0.676734983921051, 0.6791621446609497, 0.6810500025749207, 0.6811398863792419, 0.6841064095497131, 0.6828479170799255, 0.6811398863792419, 0.6819489598274231], 'loss': [1.0942182540893555, 0.9059522151947021, 0.8723944425582886, 0.8590431809425354, 0.8427975177764893, 0.8383144736289978, 0.8267603516578674, 0.8179416656494141, 0.8147392272949219, 0.8066320419311523, 0.8046450018882751, 0.7998969554901123, 0.7940800786018372, 0.7918147444725037, 0.7898600697517395, 0.7858855724334717, 0.7806603908538818, 0.7770123481750488, 0.774438202381134, 0.7764160633087158, 0.7676305174827576, 0.767141580581665, 0.761812150478363, 0.7604922652244568, 0.7551233768463135, 0.7526654601097107, 0.7554856538772583, 0.749011218547821, 0.7498141527175903, 0.7443411946296692, 0.7447453737258911, 0.7376251816749573, 0.7378630042076111, 0.7382565140724182, 0.7384984493255615, 0.7319656014442444, 0.728505551815033, 0.7251562476158142, 0.7243871092796326, 0.7227104306221008, 0.7226448655128479, 0.7221765518188477, 0.717186450958252, 0.7124192714691162, 0.7111784815788269, 0.7095829844474792, 0.7109941840171814, 0.7079035639762878, 0.7027924656867981, 0.7032151222229004], 'val_accuracy': [0.5925027132034302, 0.5962783098220825, 0.6035598516464233, 0.6116504669189453, 0.6146170496940613, 0.6143473386764526, 0.6175836324691772, 0.6192017197608948, 0.6216289401054382, 0.6256741881370544, 0.6299892067909241, 0.6278316974639893, 0.6259438991546631, 0.6267529726028442, 0.6326860785484314, 0.6305285692214966, 0.6294498443603516, 0.6299892067909241, 0.6372707486152649, 0.6305285692214966, 0.6267529726028442, 0.6334951519966125, 0.6340345144271851, 0.6275620460510254, 0.6291801333427429, 0.6310679316520691, 0.6275620460510254, 0.6281014084815979, 0.6326860785484314, 0.6310679316520691, 0.6321467161178589, 0.6337648034095764, 0.6318770051002502, 0.6367313861846924, 0.6343042254447937, 0.6337648034095764, 0.6302589178085327, 0.6318770051002502, 0.638079822063446, 0.6378101110458374, 0.6407766938209534, 0.6321467161178589, 0.6343042254447937, 0.6318770051002502, 0.6345738768577576, 0.6386191844940186, 0.6343042254447937, 0.6283710598945618, 0.641046404838562, 0.6361920237541199], 'val_loss': [0.8942516446113586, 0.8650987148284912, 0.8525478839874268, 0.8479040861129761, 0.8362010717391968, 0.8398572206497192, 0.8332986831665039, 0.8259128332138062, 0.825535774230957, 0.8215077519416809, 0.820083737373352, 0.820874035358429, 0.8208371996879578, 0.8193920254707336, 0.8142024874687195, 0.8151286840438843, 0.8187992572784424, 0.8161451816558838, 0.8134192824363708, 0.8136560916900635, 0.8180705308914185, 0.8127025961875916, 0.8157466053962708, 0.8120133876800537, 0.813029944896698, 0.8116254806518555, 0.818348228931427, 0.8203322887420654, 0.8169905543327332, 0.816603422164917, 0.8144445419311523, 0.8166623711585999, 0.8222790360450745, 0.8144518733024597, 0.8160587549209595, 0.8181025981903076, 0.8232887387275696, 0.8248886466026306, 0.8181408047676086, 0.8206637501716614, 0.8212682008743286, 0.8229212760925293, 0.8236061334609985, 0.8252367973327637, 0.8251807689666748, 0.8291910290718079, 0.8306328654289246, 0.8315280079841614, 0.8250951766967773, 0.8312152624130249]}\n",
            "Trial: Neurons = 150, Dropout Rate = 0.5, Batch Size = 128\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.3915 - loss: 1.2958 - val_accuracy: 0.5820 - val_loss: 0.9298\n",
            "Epoch 2/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5560 - loss: 0.9644 - val_accuracy: 0.6001 - val_loss: 0.8769\n",
            "Epoch 3/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5817 - loss: 0.8927 - val_accuracy: 0.6065 - val_loss: 0.8625\n",
            "Epoch 4/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5874 - loss: 0.8905 - val_accuracy: 0.6146 - val_loss: 0.8516\n",
            "Epoch 5/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6084 - loss: 0.8459 - val_accuracy: 0.6197 - val_loss: 0.8447\n",
            "Epoch 6/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6092 - loss: 0.8542 - val_accuracy: 0.6203 - val_loss: 0.8411\n",
            "Epoch 7/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6193 - loss: 0.8377 - val_accuracy: 0.6230 - val_loss: 0.8353\n",
            "Epoch 8/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6160 - loss: 0.8426 - val_accuracy: 0.6232 - val_loss: 0.8342\n",
            "Epoch 9/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6201 - loss: 0.8340 - val_accuracy: 0.6246 - val_loss: 0.8299\n",
            "Epoch 10/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6199 - loss: 0.8360 - val_accuracy: 0.6292 - val_loss: 0.8283\n",
            "Epoch 11/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6182 - loss: 0.8334 - val_accuracy: 0.6319 - val_loss: 0.8249\n",
            "Epoch 12/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6290 - loss: 0.8102 - val_accuracy: 0.6300 - val_loss: 0.8228\n",
            "Epoch 13/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6385 - loss: 0.8090 - val_accuracy: 0.6251 - val_loss: 0.8240\n",
            "Epoch 14/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6349 - loss: 0.8028 - val_accuracy: 0.6327 - val_loss: 0.8195\n",
            "Epoch 15/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6404 - loss: 0.8113 - val_accuracy: 0.6305 - val_loss: 0.8195\n",
            "Epoch 16/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6328 - loss: 0.8010 - val_accuracy: 0.6335 - val_loss: 0.8189\n",
            "Epoch 17/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6398 - loss: 0.7955 - val_accuracy: 0.6348 - val_loss: 0.8185\n",
            "Epoch 18/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6461 - loss: 0.7864 - val_accuracy: 0.6319 - val_loss: 0.8186\n",
            "Epoch 19/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6395 - loss: 0.7840 - val_accuracy: 0.6321 - val_loss: 0.8157\n",
            "Epoch 20/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6534 - loss: 0.7753 - val_accuracy: 0.6335 - val_loss: 0.8172\n",
            "Epoch 21/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6354 - loss: 0.8022 - val_accuracy: 0.6348 - val_loss: 0.8159\n",
            "Epoch 22/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6537 - loss: 0.7776 - val_accuracy: 0.6324 - val_loss: 0.8157\n",
            "Epoch 23/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6539 - loss: 0.7867 - val_accuracy: 0.6340 - val_loss: 0.8150\n",
            "Epoch 24/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6575 - loss: 0.7643 - val_accuracy: 0.6338 - val_loss: 0.8163\n",
            "Epoch 25/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6507 - loss: 0.7765 - val_accuracy: 0.6351 - val_loss: 0.8150\n",
            "Epoch 26/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6592 - loss: 0.7663 - val_accuracy: 0.6297 - val_loss: 0.8141\n",
            "Epoch 27/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6677 - loss: 0.7623 - val_accuracy: 0.6289 - val_loss: 0.8180\n",
            "Epoch 28/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6608 - loss: 0.7706 - val_accuracy: 0.6346 - val_loss: 0.8142\n",
            "Epoch 29/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6532 - loss: 0.7573 - val_accuracy: 0.6389 - val_loss: 0.8132\n",
            "Epoch 30/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6566 - loss: 0.7603 - val_accuracy: 0.6348 - val_loss: 0.8147\n",
            "Epoch 31/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6598 - loss: 0.7568 - val_accuracy: 0.6343 - val_loss: 0.8114\n",
            "Epoch 32/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6611 - loss: 0.7572 - val_accuracy: 0.6346 - val_loss: 0.8121\n",
            "Epoch 33/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6607 - loss: 0.7529 - val_accuracy: 0.6316 - val_loss: 0.8128\n",
            "Epoch 34/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6584 - loss: 0.7559 - val_accuracy: 0.6357 - val_loss: 0.8124\n",
            "Epoch 35/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6576 - loss: 0.7478 - val_accuracy: 0.6313 - val_loss: 0.8148\n",
            "Epoch 36/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6559 - loss: 0.7567 - val_accuracy: 0.6359 - val_loss: 0.8143\n",
            "Epoch 37/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6630 - loss: 0.7467 - val_accuracy: 0.6408 - val_loss: 0.8131\n",
            "Epoch 38/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6628 - loss: 0.7371 - val_accuracy: 0.6330 - val_loss: 0.8148\n",
            "Epoch 39/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6658 - loss: 0.7347 - val_accuracy: 0.6351 - val_loss: 0.8121\n",
            "Epoch 40/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6663 - loss: 0.7460 - val_accuracy: 0.6362 - val_loss: 0.8129\n",
            "Epoch 41/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6786 - loss: 0.7335 - val_accuracy: 0.6343 - val_loss: 0.8157\n",
            "Epoch 42/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6698 - loss: 0.7361 - val_accuracy: 0.6340 - val_loss: 0.8192\n",
            "Epoch 43/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6635 - loss: 0.7446 - val_accuracy: 0.6357 - val_loss: 0.8161\n",
            "Epoch 44/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6761 - loss: 0.7304 - val_accuracy: 0.6373 - val_loss: 0.8166\n",
            "Epoch 45/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6660 - loss: 0.7333 - val_accuracy: 0.6335 - val_loss: 0.8213\n",
            "Epoch 46/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6745 - loss: 0.7352 - val_accuracy: 0.6335 - val_loss: 0.8243\n",
            "Epoch 47/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6693 - loss: 0.7272 - val_accuracy: 0.6343 - val_loss: 0.8167\n",
            "Epoch 48/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6771 - loss: 0.7212 - val_accuracy: 0.6367 - val_loss: 0.8175\n",
            "Epoch 49/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6729 - loss: 0.7295 - val_accuracy: 0.6362 - val_loss: 0.8213\n",
            "Epoch 50/50\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6717 - loss: 0.7203 - val_accuracy: 0.6402 - val_loss: 0.8149\n",
            "History for model: {'accuracy': [0.4620639979839325, 0.5671520829200745, 0.5791082382202148, 0.5910643935203552, 0.6002337336540222, 0.6118302941322327, 0.6116504669189453, 0.6209097504615784, 0.6257641315460205, 0.6241459846496582, 0.6267529726028442, 0.6305285692214966, 0.6341243982315063, 0.6347537040710449, 0.6447321176528931, 0.636911153793335, 0.6405969262123108, 0.6459906697273254, 0.6444624066352844, 0.645091712474823, 0.6467097997665405, 0.6521934270858765, 0.6521934270858765, 0.6515641808509827, 0.6510248184204102, 0.6532722115516663, 0.6566882133483887, 0.6556993722915649, 0.651654064655304, 0.655339777469635, 0.658306360244751, 0.6616324782371521, 0.6570478081703186, 0.6589356064796448, 0.6588457226753235, 0.663430392742157, 0.6622617840766907, 0.6602840423583984, 0.6637899875640869, 0.6687342524528503, 0.6720604300498962, 0.6665767431259155, 0.6681050062179565, 0.6692736148834229, 0.670082688331604, 0.6708917617797852, 0.6710715293884277, 0.6736785173416138, 0.6743078231811523, 0.6733189225196838], 'loss': [1.1637954711914062, 0.9443827271461487, 0.8987302780151367, 0.8791512846946716, 0.8586640954017639, 0.8528490662574768, 0.8438599109649658, 0.8340333700180054, 0.8284346461296082, 0.8256668448448181, 0.819263219833374, 0.8151675462722778, 0.8094741106033325, 0.8059754371643066, 0.8002006411552429, 0.8012441396713257, 0.7967541217803955, 0.7916566729545593, 0.7877216339111328, 0.7861500382423401, 0.7839832305908203, 0.7800628542900085, 0.7814469933509827, 0.7750020623207092, 0.7749996781349182, 0.7726485133171082, 0.7658162117004395, 0.7706124782562256, 0.7650771141052246, 0.7625083327293396, 0.7611441612243652, 0.7559358477592468, 0.7550982236862183, 0.7542786598205566, 0.7505391836166382, 0.7512029409408569, 0.7472968101501465, 0.7439830303192139, 0.74083012342453, 0.7395266890525818, 0.741482138633728, 0.7380333542823792, 0.7373830676078796, 0.7342181205749512, 0.7340031266212463, 0.7333650588989258, 0.7318597435951233, 0.7259754538536072, 0.7252486348152161, 0.7210895419120789], 'val_accuracy': [0.5819848775863647, 0.6000539660453796, 0.6065264344215393, 0.6146170496940613, 0.6197410821914673, 0.6202805042266846, 0.6229773759841919, 0.6232470273971558, 0.6245954632759094, 0.6291801333427429, 0.6318770051002502, 0.6299892067909241, 0.6251348257064819, 0.6326860785484314, 0.6305285692214966, 0.6334951519966125, 0.6348435878753662, 0.6318770051002502, 0.6321467161178589, 0.6334951519966125, 0.6348435878753662, 0.6324163675308228, 0.6340345144271851, 0.6337648034095764, 0.6351132392883301, 0.6297194957733154, 0.628910481929779, 0.6345738768577576, 0.6388888955116272, 0.6348435878753662, 0.6343042254447937, 0.6345738768577576, 0.6316073536872864, 0.6356526613235474, 0.6313376426696777, 0.6359223127365112, 0.6407766938209534, 0.63295578956604, 0.6351132392883301, 0.6361920237541199, 0.6343042254447937, 0.6340345144271851, 0.6356526613235474, 0.6372707486152649, 0.6334951519966125, 0.6334951519966125, 0.6343042254447937, 0.6367313861846924, 0.6361920237541199, 0.6402373313903809], 'val_loss': [0.9298313856124878, 0.8768948912620544, 0.8625476956367493, 0.8516003489494324, 0.8446758389472961, 0.8410586714744568, 0.8353074193000793, 0.8341839909553528, 0.829868733882904, 0.8283008337020874, 0.8249493837356567, 0.8228018283843994, 0.8239586353302002, 0.8194558620452881, 0.8195396065711975, 0.8188796639442444, 0.8184700608253479, 0.8185625672340393, 0.8156575560569763, 0.8171521425247192, 0.8159165382385254, 0.8156819343566895, 0.8149872422218323, 0.8163437247276306, 0.8149803876876831, 0.8140568733215332, 0.8180197477340698, 0.8142196536064148, 0.8132193088531494, 0.8147211074829102, 0.8114230036735535, 0.812106728553772, 0.8128122091293335, 0.8124313354492188, 0.8147750496864319, 0.814283013343811, 0.8130518198013306, 0.8147815465927124, 0.8120691180229187, 0.8129362463951111, 0.8157010674476624, 0.8191541433334351, 0.8160512447357178, 0.8166404962539673, 0.8213469982147217, 0.8243098855018616, 0.8167011737823486, 0.8174946308135986, 0.8213406801223755, 0.8148655295372009]}\n",
            "Trial: Neurons = 150, Dropout Rate = 0.5, Batch Size = 256\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.3808 - loss: 1.3153 - val_accuracy: 0.5647 - val_loss: 0.9911\n",
            "Epoch 2/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5342 - loss: 1.0158 - val_accuracy: 0.5979 - val_loss: 0.9058\n",
            "Epoch 3/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5729 - loss: 0.9367 - val_accuracy: 0.6044 - val_loss: 0.8784\n",
            "Epoch 4/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5734 - loss: 0.9175 - val_accuracy: 0.6098 - val_loss: 0.8646\n",
            "Epoch 5/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5877 - loss: 0.8982 - val_accuracy: 0.6092 - val_loss: 0.8565\n",
            "Epoch 6/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6000 - loss: 0.8716 - val_accuracy: 0.6160 - val_loss: 0.8496\n",
            "Epoch 7/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6087 - loss: 0.8633 - val_accuracy: 0.6184 - val_loss: 0.8438\n",
            "Epoch 8/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6039 - loss: 0.8566 - val_accuracy: 0.6241 - val_loss: 0.8409\n",
            "Epoch 9/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6155 - loss: 0.8431 - val_accuracy: 0.6224 - val_loss: 0.8380\n",
            "Epoch 10/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6142 - loss: 0.8447 - val_accuracy: 0.6238 - val_loss: 0.8352\n",
            "Epoch 11/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6163 - loss: 0.8366 - val_accuracy: 0.6238 - val_loss: 0.8323\n",
            "Epoch 12/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6322 - loss: 0.8174 - val_accuracy: 0.6265 - val_loss: 0.8307\n",
            "Epoch 13/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6276 - loss: 0.8180 - val_accuracy: 0.6281 - val_loss: 0.8286\n",
            "Epoch 14/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6127 - loss: 0.8369 - val_accuracy: 0.6321 - val_loss: 0.8267\n",
            "Epoch 15/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6317 - loss: 0.8048 - val_accuracy: 0.6270 - val_loss: 0.8262\n",
            "Epoch 16/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6297 - loss: 0.8117 - val_accuracy: 0.6303 - val_loss: 0.8253\n",
            "Epoch 17/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6309 - loss: 0.8123 - val_accuracy: 0.6294 - val_loss: 0.8245\n",
            "Epoch 18/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6427 - loss: 0.7970 - val_accuracy: 0.6276 - val_loss: 0.8255\n",
            "Epoch 19/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6371 - loss: 0.8027 - val_accuracy: 0.6332 - val_loss: 0.8236\n",
            "Epoch 20/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6424 - loss: 0.7966 - val_accuracy: 0.6351 - val_loss: 0.8202\n",
            "Epoch 21/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6364 - loss: 0.8001 - val_accuracy: 0.6340 - val_loss: 0.8205\n",
            "Epoch 22/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6454 - loss: 0.7923 - val_accuracy: 0.6321 - val_loss: 0.8192\n",
            "Epoch 23/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6420 - loss: 0.7960 - val_accuracy: 0.6378 - val_loss: 0.8192\n",
            "Epoch 24/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6461 - loss: 0.7869 - val_accuracy: 0.6394 - val_loss: 0.8163\n",
            "Epoch 25/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6448 - loss: 0.7839 - val_accuracy: 0.6367 - val_loss: 0.8173\n",
            "Epoch 26/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6469 - loss: 0.7801 - val_accuracy: 0.6362 - val_loss: 0.8162\n",
            "Epoch 27/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6565 - loss: 0.7748 - val_accuracy: 0.6332 - val_loss: 0.8176\n",
            "Epoch 28/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6505 - loss: 0.7743 - val_accuracy: 0.6346 - val_loss: 0.8171\n",
            "Epoch 29/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6469 - loss: 0.7708 - val_accuracy: 0.6362 - val_loss: 0.8161\n",
            "Epoch 30/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6534 - loss: 0.7734 - val_accuracy: 0.6335 - val_loss: 0.8179\n",
            "Epoch 31/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6548 - loss: 0.7729 - val_accuracy: 0.6378 - val_loss: 0.8138\n",
            "Epoch 32/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6395 - loss: 0.7806 - val_accuracy: 0.6378 - val_loss: 0.8141\n",
            "Epoch 33/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6572 - loss: 0.7576 - val_accuracy: 0.6346 - val_loss: 0.8160\n",
            "Epoch 34/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6594 - loss: 0.7657 - val_accuracy: 0.6370 - val_loss: 0.8137\n",
            "Epoch 35/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6643 - loss: 0.7614 - val_accuracy: 0.6357 - val_loss: 0.8154\n",
            "Epoch 36/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6587 - loss: 0.7610 - val_accuracy: 0.6392 - val_loss: 0.8137\n",
            "Epoch 37/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6522 - loss: 0.7652 - val_accuracy: 0.6346 - val_loss: 0.8148\n",
            "Epoch 38/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6608 - loss: 0.7622 - val_accuracy: 0.6400 - val_loss: 0.8147\n",
            "Epoch 39/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6672 - loss: 0.7495 - val_accuracy: 0.6354 - val_loss: 0.8124\n",
            "Epoch 40/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6592 - loss: 0.7465 - val_accuracy: 0.6346 - val_loss: 0.8136\n",
            "Epoch 41/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6643 - loss: 0.7446 - val_accuracy: 0.6389 - val_loss: 0.8134\n",
            "Epoch 42/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6623 - loss: 0.7515 - val_accuracy: 0.6394 - val_loss: 0.8120\n",
            "Epoch 43/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6672 - loss: 0.7445 - val_accuracy: 0.6357 - val_loss: 0.8120\n",
            "Epoch 44/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6688 - loss: 0.7378 - val_accuracy: 0.6392 - val_loss: 0.8136\n",
            "Epoch 45/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6686 - loss: 0.7504 - val_accuracy: 0.6375 - val_loss: 0.8130\n",
            "Epoch 46/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6602 - loss: 0.7503 - val_accuracy: 0.6397 - val_loss: 0.8113\n",
            "Epoch 47/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6669 - loss: 0.7424 - val_accuracy: 0.6362 - val_loss: 0.8149\n",
            "Epoch 48/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6683 - loss: 0.7499 - val_accuracy: 0.6419 - val_loss: 0.8134\n",
            "Epoch 49/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6686 - loss: 0.7421 - val_accuracy: 0.6408 - val_loss: 0.8141\n",
            "Epoch 50/50\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6672 - loss: 0.7536 - val_accuracy: 0.6427 - val_loss: 0.8132\n",
            "History for model: {'accuracy': [0.44066882133483887, 0.5433297157287598, 0.5690399408340454, 0.5845019817352295, 0.5880978107452393, 0.5979863405227661, 0.6059870719909668, 0.608953595161438, 0.6120999455451965, 0.6186623573303223, 0.6221683025360107, 0.6250449419021606, 0.6241459846496582, 0.6237863898277283, 0.6272024512290955, 0.626303493976593, 0.6331355571746826, 0.6352031826972961, 0.6339446306228638, 0.6400575041770935, 0.6361920237541199, 0.646170437335968, 0.6425746083259583, 0.6430240869522095, 0.6469795107841492, 0.6451815962791443, 0.6460805535316467, 0.6489571928977966, 0.6515641808509827, 0.6526429057121277, 0.6554297208786011, 0.6490470767021179, 0.6539014577865601, 0.6571376919746399, 0.6583962440490723, 0.6575871706008911, 0.6573175191879272, 0.6618123054504395, 0.6585760712623596, 0.658306360244751, 0.657227635383606, 0.6636102199554443, 0.6639698147773743, 0.6654980182647705, 0.663430392742157, 0.6607335209846497, 0.6662172079086304, 0.6656777858734131, 0.6642394661903381, 0.6699029207229614], 'loss': [1.2129731178283691, 0.9965664744377136, 0.9343709349632263, 0.9043475985527039, 0.8901793956756592, 0.8729806542396545, 0.85886549949646, 0.8517379760742188, 0.8444725871086121, 0.8366925716400146, 0.8322618007659912, 0.8268603086471558, 0.8201918005943298, 0.8223432302474976, 0.8129491209983826, 0.8118458986282349, 0.8093759417533875, 0.8025107979774475, 0.8026484251022339, 0.797469437122345, 0.7982828617095947, 0.792818546295166, 0.7918633818626404, 0.7891369462013245, 0.7872291207313538, 0.7846289873123169, 0.7813710570335388, 0.7805141806602478, 0.7725958824157715, 0.7718529105186462, 0.7741273641586304, 0.7697353363037109, 0.7674363255500793, 0.7687200903892517, 0.7652451395988464, 0.765978991985321, 0.7642975449562073, 0.7625859379768372, 0.7568551898002625, 0.7525606155395508, 0.7517193555831909, 0.7566003799438477, 0.7521297335624695, 0.7484553456306458, 0.7485418319702148, 0.7521764636039734, 0.7473419308662415, 0.7469897270202637, 0.7469398379325867, 0.742596447467804], 'val_accuracy': [0.5647249221801758, 0.5978964567184448, 0.6043689250946045, 0.6097626686096191, 0.6092233061790466, 0.6159654855728149, 0.6183926463127136, 0.6240561008453369, 0.6224379539489746, 0.6237863898277283, 0.6237863898277283, 0.6264832615852356, 0.6281014084815979, 0.6321467161178589, 0.6270226240158081, 0.6302589178085327, 0.6294498443603516, 0.6275620460510254, 0.6332254409790039, 0.6351132392883301, 0.6340345144271851, 0.6321467161178589, 0.6378101110458374, 0.6394282579421997, 0.6367313861846924, 0.6361920237541199, 0.6332254409790039, 0.6345738768577576, 0.6361920237541199, 0.6334951519966125, 0.6378101110458374, 0.6378101110458374, 0.6345738768577576, 0.637001097202301, 0.6356526613235474, 0.6391585469245911, 0.6345738768577576, 0.6399676203727722, 0.6353829503059387, 0.6345738768577576, 0.6388888955116272, 0.6394282579421997, 0.6356526613235474, 0.6391585469245911, 0.6375404596328735, 0.6396979689598083, 0.6361920237541199, 0.6418554186820984, 0.6407766938209534, 0.6426644921302795], 'val_loss': [0.9910810589790344, 0.9058153033256531, 0.8784191012382507, 0.8645541071891785, 0.8565439581871033, 0.8496062755584717, 0.8437992930412292, 0.8409258723258972, 0.8380314111709595, 0.8352483510971069, 0.8322595953941345, 0.8307346701622009, 0.8285598158836365, 0.8266847729682922, 0.8262346982955933, 0.8253257274627686, 0.8245164752006531, 0.8254557847976685, 0.8235781788825989, 0.8202462792396545, 0.8205429911613464, 0.8192342519760132, 0.8192414045333862, 0.8163301348686218, 0.8173033595085144, 0.816224217414856, 0.8175708055496216, 0.8170741200447083, 0.8160902261734009, 0.8178925514221191, 0.8138165473937988, 0.8141342997550964, 0.8159953951835632, 0.8137404322624207, 0.8153632283210754, 0.8137198686599731, 0.81475830078125, 0.8147059679031372, 0.8124089241027832, 0.813552975654602, 0.8133816719055176, 0.8120315074920654, 0.8119859099388123, 0.8136131167411804, 0.8130477070808411, 0.811341404914856, 0.8149017691612244, 0.8133867383003235, 0.8141267895698547, 0.8132049441337585]}\n",
            "Results saved to 'model_results.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_grid = pd.read_csv('model_results.csv')"
      ],
      "metadata": {
        "id": "vAaCPlHaBiDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_grid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "UkwwKhMhBn7t",
        "outputId": "32c26ab8-8488-4226-9cfb-3db4ffbc0416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Neurons  Dropout Rate  Batch Size  Validation Loss  Validation Accuracy\n",
              "0        50           0.0          64         0.847567             0.632956\n",
              "1        50           0.0         128         0.832985             0.630529\n",
              "2        50           0.0         256         0.827030             0.627023\n",
              "3        50           0.1          64         0.844133             0.628910\n",
              "4        50           0.1         128         0.825108             0.633495\n",
              "5        50           0.1         256         0.813165             0.634574\n",
              "6        50           0.5          64         0.819451             0.635922\n",
              "7        50           0.5         128         0.813928             0.641855\n",
              "8        50           0.5         256         0.811948             0.634304\n",
              "9       100           0.0          64         0.854179             0.627023\n",
              "10      100           0.0         128         0.846697             0.627832\n",
              "11      100           0.0         256         0.824959             0.631607\n",
              "12      100           0.1          64         0.845309             0.633495\n",
              "13      100           0.1         128         0.837646             0.630798\n",
              "14      100           0.1         256         0.819994             0.630798\n",
              "15      100           0.5          64         0.818810             0.637810\n",
              "16      100           0.5         128         0.808061             0.639968\n",
              "17      100           0.5         256         0.808633             0.639428\n",
              "18      150           0.0          64         0.872815             0.629719\n",
              "19      150           0.0         128         0.851315             0.628641\n",
              "20      150           0.0         256         0.832680             0.628910\n",
              "21      150           0.1          64         0.855172             0.627562\n",
              "22      150           0.1         128         0.825652             0.634574\n",
              "23      150           0.1         256         0.820241             0.632956\n",
              "24      150           0.5          64         0.831215             0.636192\n",
              "25      150           0.5         128         0.814866             0.640237\n",
              "26      150           0.5         256         0.813205             0.642664"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c5e2281a-f4ff-49c4-b5c2-23d817aba4e9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Neurons</th>\n",
              "      <th>Dropout Rate</th>\n",
              "      <th>Batch Size</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Validation Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64</td>\n",
              "      <td>0.847567</td>\n",
              "      <td>0.632956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>128</td>\n",
              "      <td>0.832985</td>\n",
              "      <td>0.630529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>256</td>\n",
              "      <td>0.827030</td>\n",
              "      <td>0.627023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>0.844133</td>\n",
              "      <td>0.628910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50</td>\n",
              "      <td>0.1</td>\n",
              "      <td>128</td>\n",
              "      <td>0.825108</td>\n",
              "      <td>0.633495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>50</td>\n",
              "      <td>0.1</td>\n",
              "      <td>256</td>\n",
              "      <td>0.813165</td>\n",
              "      <td>0.634574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>64</td>\n",
              "      <td>0.819451</td>\n",
              "      <td>0.635922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>128</td>\n",
              "      <td>0.813928</td>\n",
              "      <td>0.641855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>256</td>\n",
              "      <td>0.811948</td>\n",
              "      <td>0.634304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64</td>\n",
              "      <td>0.854179</td>\n",
              "      <td>0.627023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>128</td>\n",
              "      <td>0.846697</td>\n",
              "      <td>0.627832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>256</td>\n",
              "      <td>0.824959</td>\n",
              "      <td>0.631607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>100</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>0.845309</td>\n",
              "      <td>0.633495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>100</td>\n",
              "      <td>0.1</td>\n",
              "      <td>128</td>\n",
              "      <td>0.837646</td>\n",
              "      <td>0.630798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>100</td>\n",
              "      <td>0.1</td>\n",
              "      <td>256</td>\n",
              "      <td>0.819994</td>\n",
              "      <td>0.630798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>100</td>\n",
              "      <td>0.5</td>\n",
              "      <td>64</td>\n",
              "      <td>0.818810</td>\n",
              "      <td>0.637810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>100</td>\n",
              "      <td>0.5</td>\n",
              "      <td>128</td>\n",
              "      <td>0.808061</td>\n",
              "      <td>0.639968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>100</td>\n",
              "      <td>0.5</td>\n",
              "      <td>256</td>\n",
              "      <td>0.808633</td>\n",
              "      <td>0.639428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>150</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64</td>\n",
              "      <td>0.872815</td>\n",
              "      <td>0.629719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>150</td>\n",
              "      <td>0.0</td>\n",
              "      <td>128</td>\n",
              "      <td>0.851315</td>\n",
              "      <td>0.628641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>150</td>\n",
              "      <td>0.0</td>\n",
              "      <td>256</td>\n",
              "      <td>0.832680</td>\n",
              "      <td>0.628910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>150</td>\n",
              "      <td>0.1</td>\n",
              "      <td>64</td>\n",
              "      <td>0.855172</td>\n",
              "      <td>0.627562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>150</td>\n",
              "      <td>0.1</td>\n",
              "      <td>128</td>\n",
              "      <td>0.825652</td>\n",
              "      <td>0.634574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>150</td>\n",
              "      <td>0.1</td>\n",
              "      <td>256</td>\n",
              "      <td>0.820241</td>\n",
              "      <td>0.632956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>150</td>\n",
              "      <td>0.5</td>\n",
              "      <td>64</td>\n",
              "      <td>0.831215</td>\n",
              "      <td>0.636192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>150</td>\n",
              "      <td>0.5</td>\n",
              "      <td>128</td>\n",
              "      <td>0.814866</td>\n",
              "      <td>0.640237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>150</td>\n",
              "      <td>0.5</td>\n",
              "      <td>256</td>\n",
              "      <td>0.813205</td>\n",
              "      <td>0.642664</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5e2281a-f4ff-49c4-b5c2-23d817aba4e9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c5e2281a-f4ff-49c4-b5c2-23d817aba4e9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c5e2281a-f4ff-49c4-b5c2-23d817aba4e9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-eba02316-15a9-4e8c-ba11-0e73dc5c532d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eba02316-15a9-4e8c-ba11-0e73dc5c532d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-eba02316-15a9-4e8c-ba11-0e73dc5c532d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_6dd30036-c33c-4849-bdc0-7d3a443ceaa4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_grid')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6dd30036-c33c-4849-bdc0-7d3a443ceaa4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_grid');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_grid",
              "summary": "{\n  \"name\": \"df_grid\",\n  \"rows\": 27,\n  \"fields\": [\n    {\n      \"column\": \"Neurons\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 41,\n        \"min\": 50,\n        \"max\": 150,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          50,\n          100,\n          150\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dropout Rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.22013981571160288,\n        \"min\": 0.0,\n        \"max\": 0.5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          0.1,\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Batch Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 81,\n        \"min\": 64,\n        \"max\": 256,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          64,\n          128,\n          256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Validation Loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.016901388931003417,\n        \"min\": 0.808060884475708,\n        \"max\": 0.8728151917457581,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          0.8119481801986694,\n          0.8376463651657104,\n          0.8541789650917053\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Validation Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004677787056249192,\n        \"min\": 0.6270226240158081,\n        \"max\": 0.6426644921302795,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          0.63295578956604,\n          0.6275620460510254,\n          0.6297194957733154\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "heatmap_data = df_grid.pivot_table(values='Validation Accuracy', index='Batch Size', columns='Dropout Rate')"
      ],
      "metadata": {
        "id": "NuDDmSETFuzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the size of the figure\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Create the heatmap with annotations\n",
        "sns.heatmap(heatmap_data, annot=True, fmt=\".4f\", cmap='viridis', linewidths=0.5)\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Validation Accuracy Heatmap')\n",
        "plt.xlabel('Dropout Rate')\n",
        "plt.ylabel('Batch Size')\n",
        "\n",
        "plt.savefig('validation_accuracy_heatmap.png')\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "-eLadIP9F3La",
        "outputId": "b295be38-337d-42fe-d9e2-fbe66f5dfa77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('validation_accuracy_heatmap.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "bjKaQBhkGYbI",
        "outputId": "714bddbb-fab8-4856-85a5-8315bfe1431e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e6b3f2a8-385b-47e6-ad7d-c042c1658b22\", \"validation_accuracy_heatmap.png\", 36000)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your CSV file into a DataFrame (replace 'model_results.csv' with your file name)\n",
        "df = pd.read_csv('model_results.csv')\n",
        "\n",
        "# Display the first few rows of the data to check\n",
        "df.head()\n",
        "\n",
        "# Set the size of the figure\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Create a scatter plot of Validation Accuracy vs Neurons\n",
        "sns.scatterplot(data=df, x='Neurons', y='Validation Accuracy')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Validation Accuracy vs Neurons')\n",
        "plt.xlabel('Neurons')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "\n",
        "# Save the scatter plot as an image (e.g., PNG)\n",
        "plt.savefig('validation_accuracy_vs_neurons_scatter.png')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "yfLwgjRLHGtn",
        "outputId": "67216d15-01b0-4a1b-e0ee-f917647b5e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('validation_accuracy_vs_neurons_scatter.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ttFiCC51HQYJ",
        "outputId": "96012bc2-e339-4cc7-a877-b3bfe4a2767d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_436d46ed-7666-4c03-bb7a-8c1316baa9cd\", \"validation_accuracy_vs_neurons_scatter.png\", 29894)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # Set the size of the figure\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Create the line plot with different lines for different Dropout Rates\n",
        "sns.lineplot(data=df, x='Batch Size', y='Validation Accuracy', hue='Dropout Rate', marker=\"o\")\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Validation Accuracy vs Batch Size for Different Dropout Rates')\n",
        "plt.xlabel('Batch Size')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "\n",
        "# Save the plot as a PNG file\n",
        "plt.savefig('validation_accuracy_vs_batch_size_line_chart.png')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "DOqGYOd_HrdM",
        "outputId": "ffb6e88c-de12-45b8-ab1d-50328f876e4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('validation_accuracy_vs_batch_size_line_chart.png')"
      ],
      "metadata": {
        "id": "iFslMrJ_IEsL",
        "outputId": "0172c68b-0bbd-4b3e-de9b-b0d96e925bcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ec3920d2-0a25-4c9a-b9dd-58bf11ceac8f\", \"validation_accuracy_vs_batch_size_line_chart.png\", 71245)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}